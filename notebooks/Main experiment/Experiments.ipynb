{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Baseline code provided by uni"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import logging\n",
    "import json\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.dummy import DummyRegressor\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Baseline function to create [predictions](https://github.com/larshanen/MLChallenge/tree/main/notebooks/predicted.json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndef main():\\n    # Set the logging level to INFO and set loading message\\n    logging.getLogger().setLevel(logging.INFO)\\n    \\n    # Load train and test sets and change all NA values to empty values\\n    logging.info(\"Loading training/test data\")\\n    train = pd.DataFrame.from_records(json.load(open(\\'../data/train.json\\'))).fillna(\"\")\\n    test = pd.DataFrame.from_records(json.load(open(\\'../data/test.json\\'))).fillna(\"\")\\n    \\n    # Split the train set into train (75%) and validation (25%) sets\\n    logging.info(\"Splitting validation\")\\n    train, val = train_test_split(train, stratify=train[\\'year\\'], random_state=123)\\n    \\n    # Store a featurizer to transform the \\'title\\' column into a bag-of-words format\\n    featurizer = ColumnTransformer(\\n        transformers=[(\"title\", CountVectorizer(), \"title\")], remainder=\\'drop\\')\\n    \\n    # Make a pipeline for the featurizer combined with a dummy regressor, that simply predicts the overall trained mean of the target variable\\n    dummy = make_pipeline(featurizer, DummyRegressor(strategy=\\'mean\\'))\\n\\n    # Make a pipeline for the featurizer and a ridge model, that aims to minimize the sum of squares\\n    ridge = make_pipeline(featurizer, Ridge())\\n    \\n    # Drop target variable column and fit both models\\n    logging.info(\"Fitting models\")\\n    dummy.fit(train.drop(\\'year\\', axis=1), train[\\'year\\'].values)\\n    ridge.fit(train.drop(\\'year\\', axis=1), train[\\'year\\'].values)\\n    \\n    # Calculate and report both MAE\\'s\\n    logging.info(\"Evaluating on validation data\")\\n    err = mean_absolute_error(val[\\'year\\'].values, dummy.predict(val.drop(\\'year\\', axis=1)))\\n    logging.info(f\"Mean baseline MAE: {err}\")\\n    err = mean_absolute_error(val[\\'year\\'].values, ridge.predict(val.drop(\\'year\\', axis=1)))\\n    logging.info(f\"Ridge regress MAE: {err}\")\\n    \\n    # Let the ridge model predict on test set\\n    logging.info(f\"Predicting on test\")\\n    pred = ridge.predict(test)\\n    test[\\'year\\'] = pred\\n    \\n    # Write JSON prediction file\\n    logging.info(\"Writing prediction file\")\\n    test.to_json(\"predicted.json\", orient=\\'records\\', indent=2)\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "def main():\n",
    "    # Set the logging level to INFO and set loading message\n",
    "    logging.getLogger().setLevel(logging.INFO)\n",
    "    \n",
    "    # Load train and test sets and change all NA values to empty values\n",
    "    logging.info(\"Loading training/test data\")\n",
    "    train = pd.DataFrame.from_records(json.load(open('../data/train.json'))).fillna(\"\")\n",
    "    test = pd.DataFrame.from_records(json.load(open('../data/test.json'))).fillna(\"\")\n",
    "    \n",
    "    # Split the train set into train (75%) and validation (25%) sets\n",
    "    logging.info(\"Splitting validation\")\n",
    "    train, val = train_test_split(train, stratify=train['year'], random_state=123)\n",
    "    \n",
    "    # Store a featurizer to transform the 'title' column into a bag-of-words format\n",
    "    featurizer = ColumnTransformer(\n",
    "        transformers=[(\"title\", CountVectorizer(), \"title\")], remainder='drop')\n",
    "    \n",
    "    # Make a pipeline for the featurizer combined with a dummy regressor, that simply predicts the overall trained mean of the target variable\n",
    "    dummy = make_pipeline(featurizer, DummyRegressor(strategy='mean'))\n",
    "\n",
    "    # Make a pipeline for the featurizer and a ridge model, that aims to minimize the sum of squares\n",
    "    ridge = make_pipeline(featurizer, Ridge())\n",
    "    \n",
    "    # Drop target variable column and fit both models\n",
    "    logging.info(\"Fitting models\")\n",
    "    dummy.fit(train.drop('year', axis=1), train['year'].values)\n",
    "    ridge.fit(train.drop('year', axis=1), train['year'].values)\n",
    "    \n",
    "    # Calculate and report both MAE's\n",
    "    logging.info(\"Evaluating on validation data\")\n",
    "    err = mean_absolute_error(val['year'].values, dummy.predict(val.drop('year', axis=1)))\n",
    "    logging.info(f\"Mean baseline MAE: {err}\")\n",
    "    err = mean_absolute_error(val['year'].values, ridge.predict(val.drop('year', axis=1)))\n",
    "    logging.info(f\"Ridge regress MAE: {err}\")\n",
    "    \n",
    "    # Let the ridge model predict on test set\n",
    "    logging.info(f\"Predicting on test\")\n",
    "    pred = ridge.predict(test)\n",
    "    test['year'] = pred\n",
    "    \n",
    "    # Write JSON prediction file\n",
    "    logging.info(\"Writing prediction file\")\n",
    "    test.to_json(\"predicted.json\", orient='records', indent=2)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Team code\n",
    "\n",
    "Please follow the instructions beneath when writing or adjusting code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Describe every piece of code with comments\n",
    "# Include your name in every header so we can report our individual contributions (this is mandatory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Explore baseline performance (Lars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Loading training/test data\n"
     ]
    }
   ],
   "source": [
    "# Set the logging level to INFO and set loading message\n",
    "logging.getLogger().setLevel(logging.INFO)\n",
    "    \n",
    "# Load train and test sets and change all NA values to empty values\n",
    "logging.info(\"Loading training/test data\")\n",
    "train = pd.DataFrame.from_records(json.load(open('../../data/train.json')))\n",
    "test = pd.DataFrame.from_records(json.load(open('../../data/test.json')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# Split the train set into train (75%) and validation (25%) sets\\nlogging.info(\"Splitting validation\")\\ntrain, val = train_test_split(train, stratify=train[\\'year\\'], random_state=123)\\n    \\n# Store a featurizer to transform the \\'title\\' column into a bag-of-words format\\nfeaturizer = ColumnTransformer(\\ntransformers=[(\"title\", CountVectorizer(), \"title\")], remainder=\\'drop\\')\\n    \\n# Make a pipeline for the featurizer combined with a dummy regressor, that simply predicts the overall trained mean of the target variable\\ndummy = make_pipeline(featurizer, DummyRegressor(strategy=\\'mean\\'))\\n\\n# Make a pipeline for the featurizer and a ridge model, that aims to minimize the sum of squares\\nridge = make_pipeline(featurizer, Ridge())\\n    \\n# Drop target variable column and fit both models\\nlogging.info(\"Fitting models\")\\ndummy.fit(train.drop(\\'year\\', axis=1), train[\\'year\\'].values)\\nridge.fit(train.drop(\\'year\\', axis=1), train[\\'year\\'].values)\\n    \\n# Calculate and report both MAE\\'s\\nlogging.info(\"Evaluating on validation data\")\\nerr = mean_absolute_error(val[\\'year\\'].values, dummy.predict(val.drop(\\'year\\', axis=1)))\\nlogging.info(f\"Mean baseline MAE: {err}\")\\nerr = mean_absolute_error(val[\\'year\\'].values, ridge.predict(val.drop(\\'year\\', axis=1)))\\nlogging.info(f\"Ridge regress MAE: {err}\")\\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "# Split the train set into train (75%) and validation (25%) sets\n",
    "logging.info(\"Splitting validation\")\n",
    "train, val = train_test_split(train, stratify=train['year'], random_state=123)\n",
    "    \n",
    "# Store a featurizer to transform the 'title' column into a bag-of-words format\n",
    "featurizer = ColumnTransformer(\n",
    "transformers=[(\"title\", CountVectorizer(), \"title\")], remainder='drop')\n",
    "    \n",
    "# Make a pipeline for the featurizer combined with a dummy regressor, that simply predicts the overall trained mean of the target variable\n",
    "dummy = make_pipeline(featurizer, DummyRegressor(strategy='mean'))\n",
    "\n",
    "# Make a pipeline for the featurizer and a ridge model, that aims to minimize the sum of squares\n",
    "ridge = make_pipeline(featurizer, Ridge())\n",
    "    \n",
    "# Drop target variable column and fit both models\n",
    "logging.info(\"Fitting models\")\n",
    "dummy.fit(train.drop('year', axis=1), train['year'].values)\n",
    "ridge.fit(train.drop('year', axis=1), train['year'].values)\n",
    "    \n",
    "# Calculate and report both MAE's\n",
    "logging.info(\"Evaluating on validation data\")\n",
    "err = mean_absolute_error(val['year'].values, dummy.predict(val.drop('year', axis=1)))\n",
    "logging.info(f\"Mean baseline MAE: {err}\")\n",
    "err = mean_absolute_error(val['year'].values, ridge.predict(val.drop('year', axis=1)))\n",
    "logging.info(f\"Ridge regress MAE: {err}\")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Preprocessing (Lars)\n",
    "\n",
    "This paragraph build upon the previous baseline code. It entails the following adjustments/additions chronologically:\n",
    "\n",
    "- [x] Removal of dummy regressor, since ridge works better from the very start;\n",
    "- [x] 5-fold cross validation to reduce variability (Ridge regress MAE (5.773));\n",
    "- [x] Entrytype: Dummy-encoded;\n",
    "- [x] Title: TF-IDF vectorized, limited to top X (hyperparameter) occuring (non-stopword) words;\n",
    "- [x] Editor: Removed, >75% missing values;\n",
    "- [x] Publisher: Count vectorized, limited to top X (hyperparameter) occuring publishers;\n",
    "- [x] Author: Count vectorized, limited to top X (hyperparameter) occuring authors;\n",
    "- [x] Abstract: TF-IDF vectorized, limited to top X (hyperparameter) occuring (non-stopword) words;\n",
    "- [ ] Try tasks other than regression, like lazy learning (kNN)(?);\n",
    "- [ ] Try BERTopic modelling;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import extra modules\n",
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For experimental purposes we start working with a 10% subset of the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ENTRYTYPE</th>\n",
       "      <th>title</th>\n",
       "      <th>editor</th>\n",
       "      <th>year</th>\n",
       "      <th>publisher</th>\n",
       "      <th>author</th>\n",
       "      <th>abstract</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>12680</th>\n",
       "      <td>inproceedings</td>\n",
       "      <td>Question-Answering Based on Virtually Integrat...</td>\n",
       "      <td>None</td>\n",
       "      <td>2003</td>\n",
       "      <td>Association for Computational Linguistics</td>\n",
       "      <td>[Choi, Key-Sun, Kim, Jae-Ho, Miyazaki, Masaru,...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17292</th>\n",
       "      <td>inproceedings</td>\n",
       "      <td>AMI&amp;ERIC: How to Learn with Naive Bayes and Pr...</td>\n",
       "      <td>None</td>\n",
       "      <td>2013</td>\n",
       "      <td>Association for Computational Linguistics</td>\n",
       "      <td>[Dermouche, Mohamed, Khouas, Leila, Velcin, Ju...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33265</th>\n",
       "      <td>inproceedings</td>\n",
       "      <td>Inducing Gazetteers for Named Entity Recogniti...</td>\n",
       "      <td>None</td>\n",
       "      <td>2008</td>\n",
       "      <td>Association for Computational Linguistics</td>\n",
       "      <td>[Kazama, Jun'ichi, Torisawa, Kentaro]</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52850</th>\n",
       "      <td>inproceedings</td>\n",
       "      <td>Leveraging Explicit Lexico-logical Alignments ...</td>\n",
       "      <td>None</td>\n",
       "      <td>2022</td>\n",
       "      <td>Association for Computational Linguistics</td>\n",
       "      <td>[Sun, Runxin, He, Shizhu, Zhu, Chong, He, Yaoh...</td>\n",
       "      <td>Text-to-SQL aims to parse natural language que...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2298</th>\n",
       "      <td>inproceedings</td>\n",
       "      <td>CLAM: Quickly deploy NLP command-line tools on...</td>\n",
       "      <td>None</td>\n",
       "      <td>2014</td>\n",
       "      <td>Dublin City University and Association for Com...</td>\n",
       "      <td>[van Gompel, Maarten, Reynaert, Martin]</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           ENTRYTYPE                                              title  \\\n",
       "12680  inproceedings  Question-Answering Based on Virtually Integrat...   \n",
       "17292  inproceedings  AMI&ERIC: How to Learn with Naive Bayes and Pr...   \n",
       "33265  inproceedings  Inducing Gazetteers for Named Entity Recogniti...   \n",
       "52850  inproceedings  Leveraging Explicit Lexico-logical Alignments ...   \n",
       "2298   inproceedings  CLAM: Quickly deploy NLP command-line tools on...   \n",
       "\n",
       "      editor  year                                          publisher  \\\n",
       "12680   None  2003          Association for Computational Linguistics   \n",
       "17292   None  2013          Association for Computational Linguistics   \n",
       "33265   None  2008          Association for Computational Linguistics   \n",
       "52850   None  2022          Association for Computational Linguistics   \n",
       "2298    None  2014  Dublin City University and Association for Com...   \n",
       "\n",
       "                                                  author  \\\n",
       "12680  [Choi, Key-Sun, Kim, Jae-Ho, Miyazaki, Masaru,...   \n",
       "17292  [Dermouche, Mohamed, Khouas, Leila, Velcin, Ju...   \n",
       "33265              [Kazama, Jun'ichi, Torisawa, Kentaro]   \n",
       "52850  [Sun, Runxin, He, Shizhu, Zhu, Chong, He, Yaoh...   \n",
       "2298             [van Gompel, Maarten, Reynaert, Martin]   \n",
       "\n",
       "                                                abstract  \n",
       "12680                                               None  \n",
       "17292                                               None  \n",
       "33265                                               None  \n",
       "52850  Text-to-SQL aims to parse natural language que...  \n",
       "2298                                                None  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Randomly save 10% of the train set for velocity purposes\n",
    "percentage_to_save = 30\n",
    "\n",
    "# Calculate the number of rows to save\n",
    "num_rows_to_save = int(len(train) * (percentage_to_save / 100))\n",
    "\n",
    "# Use the sample method to randomly select rows\n",
    "train_sample = train.sample(n=num_rows_to_save, random_state=42)  # Set a random_state for reproducibility\n",
    "\n",
    "train_sample.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.1 Drop all columns with over 75% of missing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19774\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ENTRYTYPE</th>\n",
       "      <th>title</th>\n",
       "      <th>year</th>\n",
       "      <th>publisher</th>\n",
       "      <th>author</th>\n",
       "      <th>abstract</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>12680</th>\n",
       "      <td>inproceedings</td>\n",
       "      <td>Question-Answering Based on Virtually Integrat...</td>\n",
       "      <td>2003</td>\n",
       "      <td>Association for Computational Linguistics</td>\n",
       "      <td>[Choi, Key-Sun, Kim, Jae-Ho, Miyazaki, Masaru,...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17292</th>\n",
       "      <td>inproceedings</td>\n",
       "      <td>AMI&amp;ERIC: How to Learn with Naive Bayes and Pr...</td>\n",
       "      <td>2013</td>\n",
       "      <td>Association for Computational Linguistics</td>\n",
       "      <td>[Dermouche, Mohamed, Khouas, Leila, Velcin, Ju...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33265</th>\n",
       "      <td>inproceedings</td>\n",
       "      <td>Inducing Gazetteers for Named Entity Recogniti...</td>\n",
       "      <td>2008</td>\n",
       "      <td>Association for Computational Linguistics</td>\n",
       "      <td>[Kazama, Jun'ichi, Torisawa, Kentaro]</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52850</th>\n",
       "      <td>inproceedings</td>\n",
       "      <td>Leveraging Explicit Lexico-logical Alignments ...</td>\n",
       "      <td>2022</td>\n",
       "      <td>Association for Computational Linguistics</td>\n",
       "      <td>[Sun, Runxin, He, Shizhu, Zhu, Chong, He, Yaoh...</td>\n",
       "      <td>Text-to-SQL aims to parse natural language que...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2298</th>\n",
       "      <td>inproceedings</td>\n",
       "      <td>CLAM: Quickly deploy NLP command-line tools on...</td>\n",
       "      <td>2014</td>\n",
       "      <td>Dublin City University and Association for Com...</td>\n",
       "      <td>[van Gompel, Maarten, Reynaert, Martin]</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           ENTRYTYPE                                              title  year  \\\n",
       "12680  inproceedings  Question-Answering Based on Virtually Integrat...  2003   \n",
       "17292  inproceedings  AMI&ERIC: How to Learn with Naive Bayes and Pr...  2013   \n",
       "33265  inproceedings  Inducing Gazetteers for Named Entity Recogniti...  2008   \n",
       "52850  inproceedings  Leveraging Explicit Lexico-logical Alignments ...  2022   \n",
       "2298   inproceedings  CLAM: Quickly deploy NLP command-line tools on...  2014   \n",
       "\n",
       "                                               publisher  \\\n",
       "12680          Association for Computational Linguistics   \n",
       "17292          Association for Computational Linguistics   \n",
       "33265          Association for Computational Linguistics   \n",
       "52850          Association for Computational Linguistics   \n",
       "2298   Dublin City University and Association for Com...   \n",
       "\n",
       "                                                  author  \\\n",
       "12680  [Choi, Key-Sun, Kim, Jae-Ho, Miyazaki, Masaru,...   \n",
       "17292  [Dermouche, Mohamed, Khouas, Leila, Velcin, Ju...   \n",
       "33265              [Kazama, Jun'ichi, Torisawa, Kentaro]   \n",
       "52850  [Sun, Runxin, He, Shizhu, Zhu, Chong, He, Yaoh...   \n",
       "2298             [van Gompel, Maarten, Reynaert, Martin]   \n",
       "\n",
       "                                                abstract  \n",
       "12680                                               None  \n",
       "17292                                               None  \n",
       "33265                                               None  \n",
       "52850  Text-to-SQL aims to parse natural language que...  \n",
       "2298                                                None  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set threshold on 75%\n",
    "threshold = 0.25\n",
    "\n",
    "# Calculate the threshold for each column\n",
    "missing_threshold = int(threshold * len(train_sample))\n",
    "\n",
    "# Drop columns with more than the specified percentage of missing data\n",
    "train_filtered = train_sample.dropna(axis=1, thresh=missing_threshold)\n",
    "\n",
    "print(len(train_filtered))\n",
    "train_filtered.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.2 Featurize 'author' column (count-vectors, reduced to top X most frequent authors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19774\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Gebruiker\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>baldwin, timothy</th>\n",
       "      <th>bandyopadhyay, sivaji</th>\n",
       "      <th>bansal, mohit</th>\n",
       "      <th>baroni, marco</th>\n",
       "      <th>bhattacharyya, pushpak</th>\n",
       "      <th>biemann, chris</th>\n",
       "      <th>bojar, ondřej</th>\n",
       "      <th>bond, francis</th>\n",
       "      <th>callison-burch, chris</th>\n",
       "      <th>cardie, claire</th>\n",
       "      <th>...</th>\n",
       "      <th>zhang, min</th>\n",
       "      <th>zhang, qi</th>\n",
       "      <th>zhang, yue</th>\n",
       "      <th>zhao, hai</th>\n",
       "      <th>zhao, jun</th>\n",
       "      <th>zhou, guodong</th>\n",
       "      <th>zhou, jie</th>\n",
       "      <th>zhou, ming</th>\n",
       "      <th>zong, chengqing</th>\n",
       "      <th>øvrelid, lilja</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 119 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   baldwin, timothy  bandyopadhyay, sivaji  bansal, mohit  baroni, marco  \\\n",
       "0                 0                      0              0              0   \n",
       "1                 0                      0              0              0   \n",
       "2                 0                      0              0              0   \n",
       "3                 0                      0              0              0   \n",
       "4                 0                      0              0              0   \n",
       "\n",
       "   bhattacharyya, pushpak  biemann, chris  bojar, ondřej  bond, francis  \\\n",
       "0                       0               0              0              0   \n",
       "1                       0               0              0              0   \n",
       "2                       0               0              0              0   \n",
       "3                       0               0              0              0   \n",
       "4                       0               0              0              0   \n",
       "\n",
       "   callison-burch, chris  cardie, claire  ...  zhang, min  zhang, qi  \\\n",
       "0                      0               0  ...           0          0   \n",
       "1                      0               0  ...           0          0   \n",
       "2                      0               0  ...           0          0   \n",
       "3                      0               0  ...           0          0   \n",
       "4                      0               0  ...           0          0   \n",
       "\n",
       "   zhang, yue  zhao, hai  zhao, jun  zhou, guodong  zhou, jie  zhou, ming  \\\n",
       "0           0          0          0              0          0           0   \n",
       "1           0          0          0              0          0           0   \n",
       "2           0          0          0              0          0           0   \n",
       "3           0          0          1              0          0           0   \n",
       "4           0          0          0              0          0           0   \n",
       "\n",
       "   zong, chengqing  øvrelid, lilja  \n",
       "0                0               0  \n",
       "1                0               0  \n",
       "2                0               0  \n",
       "3                0               0  \n",
       "4                0               0  \n",
       "\n",
       "[5 rows x 119 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert lists of strings, accounting for None values\n",
    "train_filtered['author_str'] = train_filtered['author'].apply(lambda x: ';'.join(map(str, x)) if x is not None else 'unknown')\n",
    "\n",
    "# Add a column to store the original row numbers\n",
    "# train_filtered['original_index'] = train_filtered.index\n",
    "\n",
    "# Count the number of papers for each author\n",
    "author_paper_counts = train_filtered['author_str'].str.split(';').explode().value_counts()\n",
    "\n",
    "# Set the number of most frequent authors you want to include\n",
    "n_mostfreq_authors = 120  # Adjust this value to the desired number of most frequent authors\n",
    "\n",
    "# Filter authors based on the X most frequent authors\n",
    "top_authors = author_paper_counts.head(n_mostfreq_authors).index.tolist()\n",
    "\n",
    "# Filter only the top authors in 'author_str'\n",
    "train_filtered['author_str_filtered'] = train_filtered['author_str'].apply(lambda x: ';'.join([author for author in x.split(';') if author in top_authors]))\n",
    "\n",
    "# Count-vectorize 'author_str_filtered'\n",
    "count_vectorizer = CountVectorizer(tokenizer=lambda x: x.split(';'))\n",
    "count_matrix = count_vectorizer.fit_transform(train_filtered['author_str_filtered'])\n",
    "\n",
    "# Extract and create columns\n",
    "feature_names = count_vectorizer.get_feature_names_out()\n",
    "author_count_df = pd.DataFrame(count_matrix.toarray(), columns=feature_names)\n",
    "\n",
    "# Set the original_index column as the index\n",
    "# author_count_df.set_index(train_filtered['original_index'], inplace=True)\n",
    "\n",
    "author_count_df = author_count_df.drop(['unknown', ''], axis=1) # See if this approach always works out\n",
    "\n",
    "print(len(author_count_df))\n",
    "author_count_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merge with train_filtered, meaning we drop the author column and then add author_count_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop all redundant columns\n",
    "# train_filtered_buffer = train_filtered.drop(['author', 'author_str', 'author_str_filtered'], axis=1)\n",
    "\n",
    "# Concatenate the original with dropped redundants and the extracted features for author\n",
    "# train_2 = pd.concat([train_filtered_buffer, author_count_df], axis=1).reindex(train_filtered_buffer.index)\n",
    "\n",
    "# print(len(train_2))\n",
    "# train_2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.3 Vectorize 'ENTRYTYPE' column (3-categorical variable one-hot encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category_article</th>\n",
       "      <th>category_inproceedings</th>\n",
       "      <th>category_proceedings</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>False</th>\n",
       "      <td>19139</td>\n",
       "      <td>1177</td>\n",
       "      <td>19232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True</th>\n",
       "      <td>635</td>\n",
       "      <td>18597</td>\n",
       "      <td>542</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       category_article  category_inproceedings  category_proceedings\n",
       "False             19139                    1177                 19232\n",
       "True                635                   18597                   542"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Perform one-hot encoding\n",
    "train_encoded_entrytype = pd.get_dummies(train_filtered['ENTRYTYPE'], columns=['category'], prefix='category')\n",
    "\n",
    "# Show count-values for each of the columns\n",
    "train_encoded_entrytype.apply(lambda x: x.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merge with train_filtered, meaning we drop the ENTRYTYPE column and then add train_encoded_entrytype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop all redundant columns\n",
    "# train_2 = train_2.drop(['ENTRYTYPE'], axis=1)\n",
    "\n",
    "# Concatenate the original with dropped redundants and the extracted features for ENTRYTYPE\n",
    "# train_3 = pd.concat([train_2, train_encoded_entrytype], axis=1).reindex(train_2.index)\n",
    "\n",
    "# print(len(train_3))\n",
    "# train_3.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.4 Vectorize 'Publisher' column (116-categorical variable count-vectorized, and reduced to X most frequent publishers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>publisher_ATALA</th>\n",
       "      <th>publisher_ATALA (Association pour le Traitement Automatique des Langues)</th>\n",
       "      <th>publisher_Asian Federation of Natural Language Processing</th>\n",
       "      <th>publisher_Aslib</th>\n",
       "      <th>publisher_Association for Computational Linguistics</th>\n",
       "      <th>publisher_Association for Machine Translation in the Americas</th>\n",
       "      <th>publisher_COLING</th>\n",
       "      <th>publisher_Coling 2008 Organizing Committee</th>\n",
       "      <th>publisher_Coling 2010 Organizing Committee</th>\n",
       "      <th>publisher_European Association for Machine Translation</th>\n",
       "      <th>publisher_European Language Resources Association</th>\n",
       "      <th>publisher_European Language Resources Association (ELRA)</th>\n",
       "      <th>publisher_INCOMA Ltd.</th>\n",
       "      <th>publisher_INCOMA Ltd. Shoumen, BULGARIA</th>\n",
       "      <th>publisher_International Committee on Computational Linguistics</th>\n",
       "      <th>publisher_MIT Press</th>\n",
       "      <th>publisher_NLP Association of India</th>\n",
       "      <th>publisher_The Association for Computational Linguistics and Chinese Language Processing (ACLCLP)</th>\n",
       "      <th>publisher_The COLING 2012 Organizing Committee</th>\n",
       "      <th>publisher_The COLING 2016 Organizing Committee</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>False</th>\n",
       "      <td>15526</td>\n",
       "      <td>15839</td>\n",
       "      <td>15710</td>\n",
       "      <td>15776</td>\n",
       "      <td>4748</td>\n",
       "      <td>15726</td>\n",
       "      <td>15804</td>\n",
       "      <td>15826</td>\n",
       "      <td>15812</td>\n",
       "      <td>15710</td>\n",
       "      <td>15357</td>\n",
       "      <td>14578</td>\n",
       "      <td>15778</td>\n",
       "      <td>15839</td>\n",
       "      <td>15603</td>\n",
       "      <td>15524</td>\n",
       "      <td>15844</td>\n",
       "      <td>15674</td>\n",
       "      <td>15787</td>\n",
       "      <td>15734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True</th>\n",
       "      <td>379</td>\n",
       "      <td>66</td>\n",
       "      <td>195</td>\n",
       "      <td>129</td>\n",
       "      <td>11157</td>\n",
       "      <td>179</td>\n",
       "      <td>101</td>\n",
       "      <td>79</td>\n",
       "      <td>93</td>\n",
       "      <td>195</td>\n",
       "      <td>548</td>\n",
       "      <td>1327</td>\n",
       "      <td>127</td>\n",
       "      <td>66</td>\n",
       "      <td>302</td>\n",
       "      <td>381</td>\n",
       "      <td>61</td>\n",
       "      <td>231</td>\n",
       "      <td>118</td>\n",
       "      <td>171</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       publisher_ATALA  \\\n",
       "False            15526   \n",
       "True               379   \n",
       "\n",
       "       publisher_ATALA (Association pour le Traitement Automatique des Langues)  \\\n",
       "False                                              15839                          \n",
       "True                                                  66                          \n",
       "\n",
       "       publisher_Asian Federation of Natural Language Processing  \\\n",
       "False                                              15710           \n",
       "True                                                 195           \n",
       "\n",
       "       publisher_Aslib  publisher_Association for Computational Linguistics  \\\n",
       "False            15776                                               4748     \n",
       "True               129                                              11157     \n",
       "\n",
       "       publisher_Association for Machine Translation in the Americas  \\\n",
       "False                                              15726               \n",
       "True                                                 179               \n",
       "\n",
       "       publisher_COLING  publisher_Coling 2008 Organizing Committee  \\\n",
       "False             15804                                       15826   \n",
       "True                101                                          79   \n",
       "\n",
       "       publisher_Coling 2010 Organizing Committee  \\\n",
       "False                                       15812   \n",
       "True                                           93   \n",
       "\n",
       "       publisher_European Association for Machine Translation  \\\n",
       "False                                              15710        \n",
       "True                                                 195        \n",
       "\n",
       "       publisher_European Language Resources Association  \\\n",
       "False                                              15357   \n",
       "True                                                 548   \n",
       "\n",
       "       publisher_European Language Resources Association (ELRA)  \\\n",
       "False                                              14578          \n",
       "True                                                1327          \n",
       "\n",
       "       publisher_INCOMA Ltd.  publisher_INCOMA Ltd. Shoumen, BULGARIA  \\\n",
       "False                  15778                                    15839   \n",
       "True                     127                                       66   \n",
       "\n",
       "       publisher_International Committee on Computational Linguistics  \\\n",
       "False                                              15603                \n",
       "True                                                 302                \n",
       "\n",
       "       publisher_MIT Press  publisher_NLP Association of India  \\\n",
       "False                15524                               15844   \n",
       "True                   381                                  61   \n",
       "\n",
       "       publisher_The Association for Computational Linguistics and Chinese Language Processing (ACLCLP)  \\\n",
       "False                                              15674                                                  \n",
       "True                                                 231                                                  \n",
       "\n",
       "       publisher_The COLING 2012 Organizing Committee  \\\n",
       "False                                           15787   \n",
       "True                                              118   \n",
       "\n",
       "       publisher_The COLING 2016 Organizing Committee  \n",
       "False                                           15734  \n",
       "True                                              171  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set the number of most frequent publishers to include\n",
    "n_mostfreq_publishers = 20  # Adjust this value as needed\n",
    "\n",
    "# Get the X most frequent publishers\n",
    "top_publishers = train_filtered['publisher'].value_counts().head(n_mostfreq_publishers).index.tolist()\n",
    "\n",
    "# Create a new DataFrame with one-hot encoding for the X most frequent publishers\n",
    "train_encoded_publisher = pd.get_dummies(train_filtered['publisher'][train_filtered['publisher'].isin(top_publishers)], prefix='publisher')\n",
    "\n",
    "# Show count-values for each of the columns\n",
    "train_encoded_publisher.apply(lambda x: x.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merge with train_filtered, meaning we drop the 'publisher' column and then add train_encoded_entrytype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop all redundant columns\n",
    "# train_3 = train_3.drop(['publisher'], axis=1)\n",
    "\n",
    "# Concatenate the original with dropped redundants and the extracted features for ENTRYTYPE\n",
    "# train_4 = pd.concat([train_3, train_encoded_publisher], axis=1).reindex(train_3.index).fillna(0)\n",
    "\n",
    "# print(len(train_4))\n",
    "# train_4.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.5.1 Vectorize 'title' and 'abstract' column (English-translated with stop-words removal and/or synonym replacement)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfrom googletrans import Translator\\nfrom langdetect import detect\\n'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "from googletrans import Translator\n",
    "from langdetect import detect\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\ndef det(x):\\n    try:\\n        lang = detect(x)\\n    except:\\n        lang = 'Other'\\n    return lang\\n\""
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "def det(x):\n",
    "    try:\n",
    "        lang = detect(x)\n",
    "    except:\n",
    "        lang = 'Other'\n",
    "    return lang\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\ntrain_filtered['language_title'] = train_filtered['title'].apply(det)\\ntrain_filtered['language_abstract'] = train_filtered['abstract'].apply(det)\\n\""
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "train_filtered['language_title'] = train_filtered['title'].apply(det)\n",
    "train_filtered['language_abstract'] = train_filtered['abstract'].apply(det)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\ntranslator = Translator(service_urls=['translate.googleapis.com'])\\n\\n# Function to translate non-English titles to English based on 'translated_title' column\\ndef translate_to_english(dataframe, column, translated_column):\\n    for i in dataframe[column].index:\\n        # Check if the value in 'translated_title' is not 'en' or 'Other' before translation\\n        if dataframe[translated_column][i] not in ['en', 'Other']:\\n            dataframe[column][i] = translator.translate(dataframe[column][i], dest='en').text\\n\""
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "translator = Translator(service_urls=['translate.googleapis.com'])\n",
    "\n",
    "# Function to translate non-English titles to English based on 'translated_title' column\n",
    "def translate_to_english(dataframe, column, translated_column):\n",
    "    for i in dataframe[column].index:\n",
    "        # Check if the value in 'translated_title' is not 'en' or 'Other' before translation\n",
    "        if dataframe[translated_column][i] not in ['en', 'Other']:\n",
    "            dataframe[column][i] = translator.translate(dataframe[column][i], dest='en').text\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nfrom deep_translator import GoogleTranslator\\n\\n# Function to translate non-English titles to English based on 'translated_title' column\\ndef translate_to_english(dataframe, column, translated_column):\\n    for i in dataframe[column].index:\\n        # Check if the value in 'translated_title' is not 'en' or 'Other' before translation\\n        if dataframe[translated_column][i] not in ['en', 'Other']:\\n            dataframe[column][i] = GoogleTranslator(source='auto', target='en').translate(text=dataframe[column][i])\\n\""
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "from deep_translator import GoogleTranslator\n",
    "\n",
    "# Function to translate non-English titles to English based on 'translated_title' column\n",
    "def translate_to_english(dataframe, column, translated_column):\n",
    "    for i in dataframe[column].index:\n",
    "        # Check if the value in 'translated_title' is not 'en' or 'Other' before translation\n",
    "        if dataframe[translated_column][i] not in ['en', 'Other']:\n",
    "            dataframe[column][i] = GoogleTranslator(source='auto', target='en').translate(text=dataframe[column][i])\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\ntranslate_to_english(train_filtered, 'title', 'language_title')\\ntranslate_to_english(train_filtered, 'abstract', 'language_abstract')\\n\""
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "translate_to_english(train_filtered, 'title', 'language_title')\n",
    "translate_to_english(train_filtered, 'abstract', 'language_abstract')\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_4['new_language_title'] = train_4['title'].apply(det)\n",
    "# train_4['new_language_abstract'] = train_4['abstract'].apply(det)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_4['new_language_title'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_4['new_language_abstract'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>abstract</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>12680</th>\n",
       "      <td>Question-Answering Based on Virtually Integrat...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17292</th>\n",
       "      <td>AMI&amp;ERIC: How to Learn with Naive Bayes and Pr...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33265</th>\n",
       "      <td>Inducing Gazetteers for Named Entity Recogniti...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52850</th>\n",
       "      <td>Leveraging Explicit Lexico-logical Alignments ...</td>\n",
       "      <td>Text-to-SQL aims to parse natural language que...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2298</th>\n",
       "      <td>CLAM: Quickly deploy NLP command-line tools on...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7809</th>\n",
       "      <td>Usability Recommendations for Annotation Tools</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6236</th>\n",
       "      <td>Deep Span Representations for Named Entity Rec...</td>\n",
       "      <td>Span-based models are one of the most straight...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63616</th>\n",
       "      <td>Comparing a Hand-crafted to an Automatically G...</td>\n",
       "      <td>The automatic evaluation of machine translatio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6396</th>\n",
       "      <td>Thai Stock News Sentiment Classification using...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28216</th>\n",
       "      <td>Effects of Empty Categories on Machine Transla...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>19774 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   title  \\\n",
       "12680  Question-Answering Based on Virtually Integrat...   \n",
       "17292  AMI&ERIC: How to Learn with Naive Bayes and Pr...   \n",
       "33265  Inducing Gazetteers for Named Entity Recogniti...   \n",
       "52850  Leveraging Explicit Lexico-logical Alignments ...   \n",
       "2298   CLAM: Quickly deploy NLP command-line tools on...   \n",
       "...                                                  ...   \n",
       "7809      Usability Recommendations for Annotation Tools   \n",
       "6236   Deep Span Representations for Named Entity Rec...   \n",
       "63616  Comparing a Hand-crafted to an Automatically G...   \n",
       "6396   Thai Stock News Sentiment Classification using...   \n",
       "28216  Effects of Empty Categories on Machine Transla...   \n",
       "\n",
       "                                                abstract  \n",
       "12680                                               None  \n",
       "17292                                               None  \n",
       "33265                                               None  \n",
       "52850  Text-to-SQL aims to parse natural language que...  \n",
       "2298                                                None  \n",
       "...                                                  ...  \n",
       "7809                                                None  \n",
       "6236   Span-based models are one of the most straight...  \n",
       "63616  The automatic evaluation of machine translatio...  \n",
       "6396                                                None  \n",
       "28216                                               None  \n",
       "\n",
       "[19774 rows x 2 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_textcolumns = train_filtered[['title', 'abstract']]\n",
    "train_textcolumns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We've transformed the 'title' column to a dataframe of 500 columns.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>19</th>\n",
       "      <th>2016</th>\n",
       "      <th>2017</th>\n",
       "      <th>2018</th>\n",
       "      <th>2019</th>\n",
       "      <th>2020</th>\n",
       "      <th>2021</th>\n",
       "      <th>2022</th>\n",
       "      <th>2023</th>\n",
       "      <th>abstract</th>\n",
       "      <th>...</th>\n",
       "      <th>web</th>\n",
       "      <th>wikipedia</th>\n",
       "      <th>wmt</th>\n",
       "      <th>word</th>\n",
       "      <th>wordnet</th>\n",
       "      <th>words</th>\n",
       "      <th>workshop</th>\n",
       "      <th>world</th>\n",
       "      <th>writing</th>\n",
       "      <th>zero</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.545583</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 500 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    19  2016  2017  2018  2019  2020  2021  2022  2023  abstract  ...  \\\n",
       "0  0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0       0.0  ...   \n",
       "1  0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0       0.0  ...   \n",
       "2  0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0       0.0  ...   \n",
       "3  0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0       0.0  ...   \n",
       "4  0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0       0.0  ...   \n",
       "\n",
       "        web  wikipedia  wmt  word  wordnet  words  workshop  world  writing  \\\n",
       "0  0.000000        0.0  0.0   0.0      0.0    0.0       0.0    0.0      0.0   \n",
       "1  0.000000        0.0  0.0   0.0      0.0    0.0       0.0    0.0      0.0   \n",
       "2  0.000000        0.0  0.0   0.0      0.0    0.0       0.0    0.0      0.0   \n",
       "3  0.000000        0.0  0.0   0.0      0.0    0.0       0.0    0.0      0.0   \n",
       "4  0.545583        0.0  0.0   0.0      0.0    0.0       0.0    0.0      0.0   \n",
       "\n",
       "   zero  \n",
       "0   0.0  \n",
       "1   0.0  \n",
       "2   0.0  \n",
       "3   0.0  \n",
       "4   0.0  \n",
       "\n",
       "[5 rows x 500 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Specify the number of most frequent tokens you want to keep (replace X with the desired value)\n",
    "max_features_title = 500\n",
    "\n",
    "# Create a list of English stopwords\n",
    "stop_words = 'english'\n",
    "\n",
    "# Apply the TF-IDF vectorizer to column 'title' with max_features parameter\n",
    "tfidf_vectorizer = TfidfVectorizer(stop_words=stop_words, max_features=max_features_title)\n",
    "\n",
    "# Apply the TF-IDF vectorizer to column 'title'\n",
    "tfidf_matrix_title = tfidf_vectorizer.fit_transform(train_textcolumns['title'])\n",
    "\n",
    "# Extract and create columns\n",
    "feature_names = tfidf_vectorizer.get_feature_names_out()\n",
    "tfidf_df_title = pd.DataFrame(tfidf_matrix_title.toarray(), columns=feature_names)\n",
    "\n",
    "print(f\"We've transformed the 'title' column to a dataframe of {len(tfidf_df_title.columns)} columns.\")\n",
    "tfidf_df_title.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Gebruiker\\AppData\\Local\\Temp\\ipykernel_22932\\261390455.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  train_textcolumns['abstract'].replace({None: '', '0': ''}, inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We've transformed the 'abstract' column to a dataframe of 500 columns.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ability</th>\n",
       "      <th>able</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>achieve</th>\n",
       "      <th>achieved</th>\n",
       "      <th>achieves</th>\n",
       "      <th>adaptation</th>\n",
       "      <th>addition</th>\n",
       "      <th>additional</th>\n",
       "      <th>address</th>\n",
       "      <th>...</th>\n",
       "      <th>web</th>\n",
       "      <th>wikipedia</th>\n",
       "      <th>word</th>\n",
       "      <th>words</th>\n",
       "      <th>work</th>\n",
       "      <th>works</th>\n",
       "      <th>world</th>\n",
       "      <th>written</th>\n",
       "      <th>years</th>\n",
       "      <th>zero</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.173568</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.115918</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 500 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   ability  able  accuracy  achieve  achieved  achieves  adaptation  addition  \\\n",
       "0      0.0   0.0       0.0      0.0       0.0       0.0         0.0       0.0   \n",
       "1      0.0   0.0       0.0      0.0       0.0       0.0         0.0       0.0   \n",
       "2      0.0   0.0       0.0      0.0       0.0       0.0         0.0       0.0   \n",
       "3      0.0   0.0       0.0      0.0       0.0       0.0         0.0       0.0   \n",
       "4      0.0   0.0       0.0      0.0       0.0       0.0         0.0       0.0   \n",
       "\n",
       "   additional  address  ...  web  wikipedia  word  words      work  works  \\\n",
       "0    0.000000      0.0  ...  0.0        0.0   0.0    0.0  0.000000    0.0   \n",
       "1    0.000000      0.0  ...  0.0        0.0   0.0    0.0  0.000000    0.0   \n",
       "2    0.000000      0.0  ...  0.0        0.0   0.0    0.0  0.000000    0.0   \n",
       "3    0.173568      0.0  ...  0.0        0.0   0.0    0.0  0.115918    0.0   \n",
       "4    0.000000      0.0  ...  0.0        0.0   0.0    0.0  0.000000    0.0   \n",
       "\n",
       "   world  written  years  zero  \n",
       "0    0.0      0.0    0.0   0.0  \n",
       "1    0.0      0.0    0.0   0.0  \n",
       "2    0.0      0.0    0.0   0.0  \n",
       "3    0.0      0.0    0.0   0.0  \n",
       "4    0.0      0.0    0.0   0.0  \n",
       "\n",
       "[5 rows x 500 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Specify the number of most frequent tokens you want to keep (replace X with the desired value)\n",
    "max_features_abstract = 500\n",
    "\n",
    "# Create a list of English stopwords\n",
    "stop_words = 'english'\n",
    "\n",
    "# Handle missing values and '0's in the 'abstract' column\n",
    "train_textcolumns['abstract'].replace({None: '', '0': ''}, inplace=True)\n",
    "\n",
    "# Apply the TF-IDF vectorizer to column 'title' with max_features parameter\n",
    "tfidf_vectorizer = TfidfVectorizer(stop_words=stop_words, max_features=max_features_abstract)\n",
    "\n",
    "# Apply the TF-IDF vectorizer to column 'title'\n",
    "tfidf_matrix_abstract = tfidf_vectorizer.fit_transform(train_textcolumns['abstract'])\n",
    "\n",
    "# Extract and create columns\n",
    "feature_names = tfidf_vectorizer.get_feature_names_out()\n",
    "tfidf_df_abstract = pd.DataFrame(tfidf_matrix_abstract.toarray(), columns=feature_names)\n",
    "\n",
    "print(f\"We've transformed the 'abstract' column to a dataframe of {len(tfidf_df_abstract.columns)} columns.\")\n",
    "tfidf_df_abstract.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Merge the following dataframes: author_count_df, train_encoded_entrytype, train_encoded_publisher, tfidf_df_title, tfidf_df_abstract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "year_df = train_sample['year']\n",
    "\n",
    "features_df = pd.concat([author_count_df, train_encoded_entrytype, train_encoded_publisher, tfidf_df_title, tfidf_df_abstract, year_df], axis=1).reindex(year_df.index).fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>baldwin, timothy</th>\n",
       "      <th>bandyopadhyay, sivaji</th>\n",
       "      <th>bansal, mohit</th>\n",
       "      <th>baroni, marco</th>\n",
       "      <th>bhattacharyya, pushpak</th>\n",
       "      <th>biemann, chris</th>\n",
       "      <th>bojar, ondřej</th>\n",
       "      <th>bond, francis</th>\n",
       "      <th>callison-burch, chris</th>\n",
       "      <th>cardie, claire</th>\n",
       "      <th>...</th>\n",
       "      <th>wikipedia</th>\n",
       "      <th>word</th>\n",
       "      <th>words</th>\n",
       "      <th>work</th>\n",
       "      <th>works</th>\n",
       "      <th>world</th>\n",
       "      <th>written</th>\n",
       "      <th>years</th>\n",
       "      <th>zero</th>\n",
       "      <th>year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>12680</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17292</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33265</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52850</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2298</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.062171</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.100883</td>\n",
       "      <td>2014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7809</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6236</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63616</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6396</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28216</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2010</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>19774 rows × 1143 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       baldwin, timothy  bandyopadhyay, sivaji  bansal, mohit  baroni, marco  \\\n",
       "12680               0.0                    0.0            0.0            0.0   \n",
       "17292               0.0                    0.0            0.0            0.0   \n",
       "33265               0.0                    0.0            0.0            0.0   \n",
       "52850               0.0                    0.0            0.0            0.0   \n",
       "2298                0.0                    0.0            0.0            0.0   \n",
       "...                 ...                    ...            ...            ...   \n",
       "7809                0.0                    0.0            0.0            0.0   \n",
       "6236                0.0                    0.0            0.0            0.0   \n",
       "63616               0.0                    0.0            0.0            0.0   \n",
       "6396                0.0                    0.0            0.0            0.0   \n",
       "28216               0.0                    0.0            0.0            0.0   \n",
       "\n",
       "       bhattacharyya, pushpak  biemann, chris  bojar, ondřej  bond, francis  \\\n",
       "12680                     0.0             0.0            0.0            0.0   \n",
       "17292                     0.0             0.0            0.0            0.0   \n",
       "33265                     0.0             0.0            0.0            0.0   \n",
       "52850                     0.0             0.0            0.0            0.0   \n",
       "2298                      0.0             0.0            0.0            0.0   \n",
       "...                       ...             ...            ...            ...   \n",
       "7809                      0.0             0.0            0.0            0.0   \n",
       "6236                      0.0             0.0            0.0            0.0   \n",
       "63616                     0.0             0.0            0.0            0.0   \n",
       "6396                      0.0             0.0            0.0            0.0   \n",
       "28216                     0.0             0.0            0.0            0.0   \n",
       "\n",
       "       callison-burch, chris  cardie, claire  ...  wikipedia  word  words  \\\n",
       "12680                    0.0             0.0  ...        0.0   0.0    0.0   \n",
       "17292                    0.0             0.0  ...        0.0   0.0    0.0   \n",
       "33265                    0.0             0.0  ...        0.0   0.0    0.0   \n",
       "52850                    0.0             0.0  ...        0.0   0.0    0.0   \n",
       "2298                     0.0             0.0  ...        0.0   0.0    0.0   \n",
       "...                      ...             ...  ...        ...   ...    ...   \n",
       "7809                     0.0             0.0  ...        0.0   0.0    0.0   \n",
       "6236                     0.0             0.0  ...        0.0   0.0    0.0   \n",
       "63616                    0.0             0.0  ...        0.0   0.0    0.0   \n",
       "6396                     0.0             0.0  ...        0.0   0.0    0.0   \n",
       "28216                    0.0             0.0  ...        0.0   0.0    0.0   \n",
       "\n",
       "           work  works  world  written  years      zero  year  \n",
       "12680  0.000000    0.0    0.0      0.0    0.0  0.000000  2003  \n",
       "17292  0.000000    0.0    0.0      0.0    0.0  0.000000  2013  \n",
       "33265  0.000000    0.0    0.0      0.0    0.0  0.000000  2008  \n",
       "52850  0.000000    0.0    0.0      0.0    0.0  0.000000  2022  \n",
       "2298   0.062171    0.0    0.0      0.0    0.0  0.100883  2014  \n",
       "...         ...    ...    ...      ...    ...       ...   ...  \n",
       "7809   0.000000    0.0    0.0      0.0    0.0  0.000000  2012  \n",
       "6236   0.000000    0.0    0.0      0.0    0.0  0.000000  2023  \n",
       "63616  0.000000    0.0    0.0      0.0    0.0  0.000000  2019  \n",
       "6396   0.000000    0.0    0.0      0.0    0.0  0.000000  2015  \n",
       "28216  0.000000    0.0    0.0      0.0    0.0  0.000000  2010  \n",
       "\n",
       "[19774 rows x 1143 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Fit baseline model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(df, m, name):\n",
    "    model = m\n",
    "    train, val = train_test_split(df, stratify=df['year'], random_state=123)\n",
    "    model.fit(train.drop('year', axis=1), train['year'].values)\n",
    "    predictions = model.predict(val.drop('year', axis=1))\n",
    "    mae = mean_absolute_error(val['year'].values, predictions)\n",
    "    print(\"{0} mae {1}\".format(name,mae))\n",
    "    \n",
    "\n",
    "reg2 = RandomForestRegressor(random_state=3,n_estimators =100,max_depth=4)\n",
    "reg1 = XGBRegressor(n_estimators=1000)\n",
    "ereg = VotingRegressor(estimators=[('gb', reg1), ('rf', reg2)])\n",
    "ereg = ereg.fit(x_train, y_train)\n",
    "\n",
    "\n",
    "\n",
    "train_model(features_df, DecisionTreeRegressor(),\"Decision Tree Regressor\")\n",
    "train_model(features_df, RandomForestRegressor(random_state=3),\"Random Forest Regressor\")   \n",
    "train_model(features_df, XGBRegressor(n_estimators=600),\"XGBoost Regressor\")   \n",
    "train_model(features_df, GradientBoostingRegressor(random_state=1),\"Gradient Regressor\")   \n",
    "train_model(features_df, ereg,\"Voting Regressor\")\n",
    "train_model(features_df, ExtraTreesRegressor(),\"Extra Trees Regressor\")   \n",
    "train_model(features_df, LinearRegression(),\"Linear Regressor\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Splitting validation\n",
      "INFO:root:Fitting models\n",
      "INFO:root:Evaluating on validation data\n",
      "INFO:root:Ridge regress MAE: 6.521680811338317\n"
     ]
    }
   ],
   "source": [
    "# Split the train set into train (75%) and validation (25%) sets\n",
    "logging.info(\"Splitting validation\")\n",
    "train, val = train_test_split(features_df, stratify=features_df['year'], random_state=123)\n",
    "\n",
    "# Make a pipeline for the featurizer and a ridge model, that aims to minimize the sum of squares\n",
    "ridge = Ridge()\n",
    "    \n",
    "# Drop target variable column and fit both models\n",
    "logging.info(\"Fitting models\")\n",
    "ridge.fit(train.drop('year', axis=1), train['year'].values)\n",
    "    \n",
    "# Calculate and report both MAE's\n",
    "logging.info(\"Evaluating on validation data\")\n",
    "err = mean_absolute_error(val['year'].values, ridge.predict(val.drop('year', axis=1)))\n",
    "logging.info(f\"Ridge regress MAE: {err}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Splitting validation\n",
      "INFO:root:Fitting models\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Gebruiker\\MLChallenge\\notebooks\\Main experiment\\Experiments.ipynb Cell 48\u001b[0m line \u001b[0;36m1\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Gebruiker/MLChallenge/notebooks/Main%20experiment/Experiments.ipynb#X65sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39m# Drop target variable column and fit both models\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Gebruiker/MLChallenge/notebooks/Main%20experiment/Experiments.ipynb#X65sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m logging\u001b[39m.\u001b[39minfo(\u001b[39m\"\u001b[39m\u001b[39mFitting models\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Gebruiker/MLChallenge/notebooks/Main%20experiment/Experiments.ipynb#X65sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m forest\u001b[39m.\u001b[39;49mfit(train\u001b[39m.\u001b[39;49mdrop(\u001b[39m'\u001b[39;49m\u001b[39myear\u001b[39;49m\u001b[39m'\u001b[39;49m, axis\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m), train[\u001b[39m'\u001b[39;49m\u001b[39myear\u001b[39;49m\u001b[39m'\u001b[39;49m]\u001b[39m.\u001b[39;49mvalues)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Gebruiker/MLChallenge/notebooks/Main%20experiment/Experiments.ipynb#X65sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m \u001b[39m# Calculate and report both MAE's\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Gebruiker/MLChallenge/notebooks/Main%20experiment/Experiments.ipynb#X65sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m logging\u001b[39m.\u001b[39minfo(\u001b[39m\"\u001b[39m\u001b[39mEvaluating on validation data\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Gebruiker\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\base.py:1151\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1144\u001b[0m     estimator\u001b[39m.\u001b[39m_validate_params()\n\u001b[0;32m   1146\u001b[0m \u001b[39mwith\u001b[39;00m config_context(\n\u001b[0;32m   1147\u001b[0m     skip_parameter_validation\u001b[39m=\u001b[39m(\n\u001b[0;32m   1148\u001b[0m         prefer_skip_nested_validation \u001b[39mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1149\u001b[0m     )\n\u001b[0;32m   1150\u001b[0m ):\n\u001b[1;32m-> 1151\u001b[0m     \u001b[39mreturn\u001b[39;00m fit_method(estimator, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Gebruiker\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\ensemble\\_forest.py:456\u001b[0m, in \u001b[0;36mBaseForest.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    445\u001b[0m trees \u001b[39m=\u001b[39m [\n\u001b[0;32m    446\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_make_estimator(append\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, random_state\u001b[39m=\u001b[39mrandom_state)\n\u001b[0;32m    447\u001b[0m     \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(n_more_estimators)\n\u001b[0;32m    448\u001b[0m ]\n\u001b[0;32m    450\u001b[0m \u001b[39m# Parallel loop: we prefer the threading backend as the Cython code\u001b[39;00m\n\u001b[0;32m    451\u001b[0m \u001b[39m# for fitting the trees is internally releasing the Python GIL\u001b[39;00m\n\u001b[0;32m    452\u001b[0m \u001b[39m# making threading more efficient than multiprocessing in\u001b[39;00m\n\u001b[0;32m    453\u001b[0m \u001b[39m# that case. However, for joblib 0.12+ we respect any\u001b[39;00m\n\u001b[0;32m    454\u001b[0m \u001b[39m# parallel_backend contexts set at a higher level,\u001b[39;00m\n\u001b[0;32m    455\u001b[0m \u001b[39m# since correctness does not rely on using threads.\u001b[39;00m\n\u001b[1;32m--> 456\u001b[0m trees \u001b[39m=\u001b[39m Parallel(\n\u001b[0;32m    457\u001b[0m     n_jobs\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mn_jobs,\n\u001b[0;32m    458\u001b[0m     verbose\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mverbose,\n\u001b[0;32m    459\u001b[0m     prefer\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mthreads\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m    460\u001b[0m )(\n\u001b[0;32m    461\u001b[0m     delayed(_parallel_build_trees)(\n\u001b[0;32m    462\u001b[0m         t,\n\u001b[0;32m    463\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbootstrap,\n\u001b[0;32m    464\u001b[0m         X,\n\u001b[0;32m    465\u001b[0m         y,\n\u001b[0;32m    466\u001b[0m         sample_weight,\n\u001b[0;32m    467\u001b[0m         i,\n\u001b[0;32m    468\u001b[0m         \u001b[39mlen\u001b[39;49m(trees),\n\u001b[0;32m    469\u001b[0m         verbose\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mverbose,\n\u001b[0;32m    470\u001b[0m         class_weight\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mclass_weight,\n\u001b[0;32m    471\u001b[0m         n_samples_bootstrap\u001b[39m=\u001b[39;49mn_samples_bootstrap,\n\u001b[0;32m    472\u001b[0m     )\n\u001b[0;32m    473\u001b[0m     \u001b[39mfor\u001b[39;49;00m i, t \u001b[39min\u001b[39;49;00m \u001b[39menumerate\u001b[39;49m(trees)\n\u001b[0;32m    474\u001b[0m )\n\u001b[0;32m    476\u001b[0m \u001b[39m# Collect newly grown trees\u001b[39;00m\n\u001b[0;32m    477\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mestimators_\u001b[39m.\u001b[39mextend(trees)\n",
      "File \u001b[1;32mc:\\Users\\Gebruiker\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\parallel.py:65\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m     60\u001b[0m config \u001b[39m=\u001b[39m get_config()\n\u001b[0;32m     61\u001b[0m iterable_with_config \u001b[39m=\u001b[39m (\n\u001b[0;32m     62\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[0;32m     63\u001b[0m     \u001b[39mfor\u001b[39;00m delayed_func, args, kwargs \u001b[39min\u001b[39;00m iterable\n\u001b[0;32m     64\u001b[0m )\n\u001b[1;32m---> 65\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__call__\u001b[39;49m(iterable_with_config)\n",
      "File \u001b[1;32mc:\\Users\\Gebruiker\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\joblib\\parallel.py:1863\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1861\u001b[0m     output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_sequential_output(iterable)\n\u001b[0;32m   1862\u001b[0m     \u001b[39mnext\u001b[39m(output)\n\u001b[1;32m-> 1863\u001b[0m     \u001b[39mreturn\u001b[39;00m output \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreturn_generator \u001b[39melse\u001b[39;00m \u001b[39mlist\u001b[39;49m(output)\n\u001b[0;32m   1865\u001b[0m \u001b[39m# Let's create an ID that uniquely identifies the current call. If the\u001b[39;00m\n\u001b[0;32m   1866\u001b[0m \u001b[39m# call is interrupted early and that the same instance is immediately\u001b[39;00m\n\u001b[0;32m   1867\u001b[0m \u001b[39m# re-used, this id will be used to prevent workers that were\u001b[39;00m\n\u001b[0;32m   1868\u001b[0m \u001b[39m# concurrently finalizing a task from the previous call to run the\u001b[39;00m\n\u001b[0;32m   1869\u001b[0m \u001b[39m# callback.\u001b[39;00m\n\u001b[0;32m   1870\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n",
      "File \u001b[1;32mc:\\Users\\Gebruiker\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\joblib\\parallel.py:1792\u001b[0m, in \u001b[0;36mParallel._get_sequential_output\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1790\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_dispatched_batches \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m   1791\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_dispatched_tasks \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m-> 1792\u001b[0m res \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1793\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_completed_tasks \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m   1794\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprint_progress()\n",
      "File \u001b[1;32mc:\\Users\\Gebruiker\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\parallel.py:127\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    125\u001b[0m     config \u001b[39m=\u001b[39m {}\n\u001b[0;32m    126\u001b[0m \u001b[39mwith\u001b[39;00m config_context(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mconfig):\n\u001b[1;32m--> 127\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfunction(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Gebruiker\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\ensemble\\_forest.py:188\u001b[0m, in \u001b[0;36m_parallel_build_trees\u001b[1;34m(tree, bootstrap, X, y, sample_weight, tree_idx, n_trees, verbose, class_weight, n_samples_bootstrap)\u001b[0m\n\u001b[0;32m    185\u001b[0m     \u001b[39melif\u001b[39;00m class_weight \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mbalanced_subsample\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m    186\u001b[0m         curr_sample_weight \u001b[39m*\u001b[39m\u001b[39m=\u001b[39m compute_sample_weight(\u001b[39m\"\u001b[39m\u001b[39mbalanced\u001b[39m\u001b[39m\"\u001b[39m, y, indices\u001b[39m=\u001b[39mindices)\n\u001b[1;32m--> 188\u001b[0m     tree\u001b[39m.\u001b[39;49mfit(X, y, sample_weight\u001b[39m=\u001b[39;49mcurr_sample_weight, check_input\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[0;32m    189\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    190\u001b[0m     tree\u001b[39m.\u001b[39mfit(X, y, sample_weight\u001b[39m=\u001b[39msample_weight, check_input\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\Gebruiker\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\base.py:1151\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1144\u001b[0m     estimator\u001b[39m.\u001b[39m_validate_params()\n\u001b[0;32m   1146\u001b[0m \u001b[39mwith\u001b[39;00m config_context(\n\u001b[0;32m   1147\u001b[0m     skip_parameter_validation\u001b[39m=\u001b[39m(\n\u001b[0;32m   1148\u001b[0m         prefer_skip_nested_validation \u001b[39mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1149\u001b[0m     )\n\u001b[0;32m   1150\u001b[0m ):\n\u001b[1;32m-> 1151\u001b[0m     \u001b[39mreturn\u001b[39;00m fit_method(estimator, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Gebruiker\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\tree\\_classes.py:1320\u001b[0m, in \u001b[0;36mDecisionTreeRegressor.fit\u001b[1;34m(self, X, y, sample_weight, check_input)\u001b[0m\n\u001b[0;32m   1290\u001b[0m \u001b[39m@_fit_context\u001b[39m(prefer_skip_nested_validation\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m   1291\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfit\u001b[39m(\u001b[39mself\u001b[39m, X, y, sample_weight\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, check_input\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m):\n\u001b[0;32m   1292\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Build a decision tree regressor from the training set (X, y).\u001b[39;00m\n\u001b[0;32m   1293\u001b[0m \n\u001b[0;32m   1294\u001b[0m \u001b[39m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1317\u001b[0m \u001b[39m        Fitted estimator.\u001b[39;00m\n\u001b[0;32m   1318\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1320\u001b[0m     \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m_fit(\n\u001b[0;32m   1321\u001b[0m         X,\n\u001b[0;32m   1322\u001b[0m         y,\n\u001b[0;32m   1323\u001b[0m         sample_weight\u001b[39m=\u001b[39;49msample_weight,\n\u001b[0;32m   1324\u001b[0m         check_input\u001b[39m=\u001b[39;49mcheck_input,\n\u001b[0;32m   1325\u001b[0m     )\n\u001b[0;32m   1326\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\Gebruiker\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\tree\\_classes.py:443\u001b[0m, in \u001b[0;36mBaseDecisionTree._fit\u001b[1;34m(self, X, y, sample_weight, check_input, missing_values_in_feature_mask)\u001b[0m\n\u001b[0;32m    432\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    433\u001b[0m     builder \u001b[39m=\u001b[39m BestFirstTreeBuilder(\n\u001b[0;32m    434\u001b[0m         splitter,\n\u001b[0;32m    435\u001b[0m         min_samples_split,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    440\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmin_impurity_decrease,\n\u001b[0;32m    441\u001b[0m     )\n\u001b[1;32m--> 443\u001b[0m builder\u001b[39m.\u001b[39;49mbuild(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtree_, X, y, sample_weight, missing_values_in_feature_mask)\n\u001b[0;32m    445\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_outputs_ \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m \u001b[39mand\u001b[39;00m is_classifier(\u001b[39mself\u001b[39m):\n\u001b[0;32m    446\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_classes_ \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_classes_[\u001b[39m0\u001b[39m]\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Split the train set into train (75%) and validation (25%) sets\n",
    "logging.info(\"Splitting validation\")\n",
    "train, val = train_test_split(features_df, stratify=features_df['year'], random_state=123)\n",
    "\n",
    "# Make a pipeline for the featurizer and a ridge model, that aims to minimize the sum of squares\n",
    "forest = RandomForestRegressor()\n",
    "    \n",
    "# Drop target variable column and fit both models\n",
    "logging.info(\"Fitting models\")\n",
    "forest.fit(train.drop('year', axis=1), train['year'].values)\n",
    "    \n",
    "# Calculate and report both MAE's\n",
    "logging.info(\"Evaluating on validation data\")\n",
    "err = mean_absolute_error(val['year'].values, ridge.predict(val.drop('year', axis=1)))\n",
    "logging.info(f\"Ridge regress MAE: {err}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Incorporate into baseline code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# Set the logging level to INFO and set loading message\\nlogging.getLogger().setLevel(logging.INFO)\\n    \\n# Load train and test sets and change all NA values to empty values\\nlogging.info(\"Loading training/test data\")\\n#train = pd.DataFrame.from_records(json.load(open(\\'../data/train.json\\'))).fillna(\"\")\\n#test = pd.DataFrame.from_records(json.load(open(\\'../data/test.json\\'))).fillna(\"\")\\n    \\n# Split the train set into train (80%) and validation (20%) sets, 5-folds\\nlogging.info(\"Splitting validation\")\\nnum_folds = 5\\nk_fold = KFold(n_splits=num_folds, shuffle=True, random_state=123)\\n    \\n# Store a featurizer to transform the \\'title\\' column into a bag-of-words format\\nfeaturizer_1 = ColumnTransformer(\\n    transformers=[(\"title\", CountVectorizer(), \"title\")], remainder=\\'drop\\')\\nfeaturizer_2 = ColumnTransformer(\\n    transformers=[(\"title\", TfidfVectorizer(), \"title\")], remainder=\\'drop\\')\\nfeaturizer_3 = ColumnTransformer(\\n    transformers=[(\"abstract\", CountVectorizer(), \"abstract\")], remainder=\\'drop\\')\\nfeaturizer_4 = ColumnTransformer(\\n    transformers=[(\"abstract\", TfidfVectorizer(), \"abstract\")], remainder=\\'drop\\')\\nfeaturizers = [featurizer_1, featurizer_2, featurizer_3, featurizer_4]\\n\\nfor i, featurizer in enumerate(featurizers):\\n    # Make a pipeline for the featurizer and a ridge model, that aims to minimize the sum of squares\\n    ridge_cv = make_pipeline(featurizer, Ridge())\\n    \\n    # Drop target variable column and fit both models\\n    logging.info(f\"Fitting model with featurizer {i+1}\")\\n    ridge_cv.fit(train_4.drop(\\'year\\', axis=1), train_4[\\'year\\'].values)\\n    \\n    # Calculate and report both MAE\\'s\\n    logging.info(\"Evaluating on validation data\")\\n    ridge_cv_scores = cross_val_score(ridge_cv, train_4.drop(\\'year\\', axis=1), train_4[\\'year\\'].values, cv=k_fold, scoring=\\'neg_mean_absolute_error\\')\\n    logging.info(f\"Ridge regress MAE with featurizer {i+1} ({num_folds}-fold cross-validated): {-ridge_cv_scores.mean()}\")\\n'"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "# Set the logging level to INFO and set loading message\n",
    "logging.getLogger().setLevel(logging.INFO)\n",
    "    \n",
    "# Load train and test sets and change all NA values to empty values\n",
    "logging.info(\"Loading training/test data\")\n",
    "#train = pd.DataFrame.from_records(json.load(open('../data/train.json'))).fillna(\"\")\n",
    "#test = pd.DataFrame.from_records(json.load(open('../data/test.json'))).fillna(\"\")\n",
    "    \n",
    "# Split the train set into train (80%) and validation (20%) sets, 5-folds\n",
    "logging.info(\"Splitting validation\")\n",
    "num_folds = 5\n",
    "k_fold = KFold(n_splits=num_folds, shuffle=True, random_state=123)\n",
    "    \n",
    "# Store a featurizer to transform the 'title' column into a bag-of-words format\n",
    "featurizer_1 = ColumnTransformer(\n",
    "    transformers=[(\"title\", CountVectorizer(), \"title\")], remainder='drop')\n",
    "featurizer_2 = ColumnTransformer(\n",
    "    transformers=[(\"title\", TfidfVectorizer(), \"title\")], remainder='drop')\n",
    "featurizer_3 = ColumnTransformer(\n",
    "    transformers=[(\"abstract\", CountVectorizer(), \"abstract\")], remainder='drop')\n",
    "featurizer_4 = ColumnTransformer(\n",
    "    transformers=[(\"abstract\", TfidfVectorizer(), \"abstract\")], remainder='drop')\n",
    "featurizers = [featurizer_1, featurizer_2, featurizer_3, featurizer_4]\n",
    "\n",
    "for i, featurizer in enumerate(featurizers):\n",
    "    # Make a pipeline for the featurizer and a ridge model, that aims to minimize the sum of squares\n",
    "    ridge_cv = make_pipeline(featurizer, Ridge())\n",
    "    \n",
    "    # Drop target variable column and fit both models\n",
    "    logging.info(f\"Fitting model with featurizer {i+1}\")\n",
    "    ridge_cv.fit(train_4.drop('year', axis=1), train_4['year'].values)\n",
    "    \n",
    "    # Calculate and report both MAE's\n",
    "    logging.info(\"Evaluating on validation data\")\n",
    "    ridge_cv_scores = cross_val_score(ridge_cv, train_4.drop('year', axis=1), train_4['year'].values, cv=k_fold, scoring='neg_mean_absolute_error')\n",
    "    logging.info(f\"Ridge regress MAE with featurizer {i+1} ({num_folds}-fold cross-validated): {-ridge_cv_scores.mean()}\")\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
