{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Baseline code provided by uni"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import logging\n",
    "import json\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.dummy import DummyRegressor\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.metrics import mean_absolute_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Baseline function to create [predictions](https://github.com/larshanen/MLChallenge/tree/main/notebooks/predicted.json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndef main():\\n    # Set the logging level to INFO and set loading message\\n    logging.getLogger().setLevel(logging.INFO)\\n    \\n    # Load train and test sets and change all NA values to empty values\\n    logging.info(\"Loading training/test data\")\\n    train = pd.DataFrame.from_records(json.load(open(\\'../data/train.json\\'))).fillna(\"\")\\n    test = pd.DataFrame.from_records(json.load(open(\\'../data/test.json\\'))).fillna(\"\")\\n    \\n    # Split the train set into train (75%) and validation (25%) sets\\n    logging.info(\"Splitting validation\")\\n    train, val = train_test_split(train, stratify=train[\\'year\\'], random_state=123)\\n    \\n    # Store a featurizer to transform the \\'title\\' column into a bag-of-words format\\n    featurizer = ColumnTransformer(\\n        transformers=[(\"title\", CountVectorizer(), \"title\")], remainder=\\'drop\\')\\n    \\n    # Make a pipeline for the featurizer combined with a dummy regressor, that simply predicts the overall trained mean of the target variable\\n    dummy = make_pipeline(featurizer, DummyRegressor(strategy=\\'mean\\'))\\n\\n    # Make a pipeline for the featurizer and a ridge model, that aims to minimize the sum of squares\\n    ridge = make_pipeline(featurizer, Ridge())\\n    \\n    # Drop target variable column and fit both models\\n    logging.info(\"Fitting models\")\\n    dummy.fit(train.drop(\\'year\\', axis=1), train[\\'year\\'].values)\\n    ridge.fit(train.drop(\\'year\\', axis=1), train[\\'year\\'].values)\\n    \\n    # Calculate and report both MAE\\'s\\n    logging.info(\"Evaluating on validation data\")\\n    err = mean_absolute_error(val[\\'year\\'].values, dummy.predict(val.drop(\\'year\\', axis=1)))\\n    logging.info(f\"Mean baseline MAE: {err}\")\\n    err = mean_absolute_error(val[\\'year\\'].values, ridge.predict(val.drop(\\'year\\', axis=1)))\\n    logging.info(f\"Ridge regress MAE: {err}\")\\n    \\n    # Let the ridge model predict on test set\\n    logging.info(f\"Predicting on test\")\\n    pred = ridge.predict(test)\\n    test[\\'year\\'] = pred\\n    \\n    # Write JSON prediction file\\n    logging.info(\"Writing prediction file\")\\n    test.to_json(\"predicted.json\", orient=\\'records\\', indent=2)\\n'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "def main():\n",
    "    # Set the logging level to INFO and set loading message\n",
    "    logging.getLogger().setLevel(logging.INFO)\n",
    "    \n",
    "    # Load train and test sets and change all NA values to empty values\n",
    "    logging.info(\"Loading training/test data\")\n",
    "    train = pd.DataFrame.from_records(json.load(open('../data/train.json'))).fillna(\"\")\n",
    "    test = pd.DataFrame.from_records(json.load(open('../data/test.json'))).fillna(\"\")\n",
    "    \n",
    "    # Split the train set into train (75%) and validation (25%) sets\n",
    "    logging.info(\"Splitting validation\")\n",
    "    train, val = train_test_split(train, stratify=train['year'], random_state=123)\n",
    "    \n",
    "    # Store a featurizer to transform the 'title' column into a bag-of-words format\n",
    "    featurizer = ColumnTransformer(\n",
    "        transformers=[(\"title\", CountVectorizer(), \"title\")], remainder='drop')\n",
    "    \n",
    "    # Make a pipeline for the featurizer combined with a dummy regressor, that simply predicts the overall trained mean of the target variable\n",
    "    dummy = make_pipeline(featurizer, DummyRegressor(strategy='mean'))\n",
    "\n",
    "    # Make a pipeline for the featurizer and a ridge model, that aims to minimize the sum of squares\n",
    "    ridge = make_pipeline(featurizer, Ridge())\n",
    "    \n",
    "    # Drop target variable column and fit both models\n",
    "    logging.info(\"Fitting models\")\n",
    "    dummy.fit(train.drop('year', axis=1), train['year'].values)\n",
    "    ridge.fit(train.drop('year', axis=1), train['year'].values)\n",
    "    \n",
    "    # Calculate and report both MAE's\n",
    "    logging.info(\"Evaluating on validation data\")\n",
    "    err = mean_absolute_error(val['year'].values, dummy.predict(val.drop('year', axis=1)))\n",
    "    logging.info(f\"Mean baseline MAE: {err}\")\n",
    "    err = mean_absolute_error(val['year'].values, ridge.predict(val.drop('year', axis=1)))\n",
    "    logging.info(f\"Ridge regress MAE: {err}\")\n",
    "    \n",
    "    # Let the ridge model predict on test set\n",
    "    logging.info(f\"Predicting on test\")\n",
    "    pred = ridge.predict(test)\n",
    "    test['year'] = pred\n",
    "    \n",
    "    # Write JSON prediction file\n",
    "    logging.info(\"Writing prediction file\")\n",
    "    test.to_json(\"predicted.json\", orient='records', indent=2)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Team code\n",
    "\n",
    "Please follow the instructions beneath when writing or adjusting code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Describe every piece of code with comments\n",
    "# Include your name in every header so we can report our individual contributions (this is mandatory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Explore baseline performance (Lars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Loading training/test data\n"
     ]
    }
   ],
   "source": [
    "# Set the logging level to INFO and set loading message\n",
    "logging.getLogger().setLevel(logging.INFO)\n",
    "    \n",
    "# Load train and test sets and change all NA values to empty values\n",
    "logging.info(\"Loading training/test data\")\n",
    "train = pd.DataFrame.from_records(json.load(open('../../data/train.json')))\n",
    "test = pd.DataFrame.from_records(json.load(open('../../data/test.json')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# Split the train set into train (75%) and validation (25%) sets\\nlogging.info(\"Splitting validation\")\\ntrain, val = train_test_split(train, stratify=train[\\'year\\'], random_state=123)\\n    \\n# Store a featurizer to transform the \\'title\\' column into a bag-of-words format\\nfeaturizer = ColumnTransformer(\\ntransformers=[(\"title\", CountVectorizer(), \"title\")], remainder=\\'drop\\')\\n    \\n# Make a pipeline for the featurizer combined with a dummy regressor, that simply predicts the overall trained mean of the target variable\\ndummy = make_pipeline(featurizer, DummyRegressor(strategy=\\'mean\\'))\\n\\n# Make a pipeline for the featurizer and a ridge model, that aims to minimize the sum of squares\\nridge = make_pipeline(featurizer, Ridge())\\n    \\n# Drop target variable column and fit both models\\nlogging.info(\"Fitting models\")\\ndummy.fit(train.drop(\\'year\\', axis=1), train[\\'year\\'].values)\\nridge.fit(train.drop(\\'year\\', axis=1), train[\\'year\\'].values)\\n    \\n# Calculate and report both MAE\\'s\\nlogging.info(\"Evaluating on validation data\")\\nerr = mean_absolute_error(val[\\'year\\'].values, dummy.predict(val.drop(\\'year\\', axis=1)))\\nlogging.info(f\"Mean baseline MAE: {err}\")\\nerr = mean_absolute_error(val[\\'year\\'].values, ridge.predict(val.drop(\\'year\\', axis=1)))\\nlogging.info(f\"Ridge regress MAE: {err}\")\\n'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "# Split the train set into train (75%) and validation (25%) sets\n",
    "logging.info(\"Splitting validation\")\n",
    "train, val = train_test_split(train, stratify=train['year'], random_state=123)\n",
    "    \n",
    "# Store a featurizer to transform the 'title' column into a bag-of-words format\n",
    "featurizer = ColumnTransformer(\n",
    "transformers=[(\"title\", CountVectorizer(), \"title\")], remainder='drop')\n",
    "    \n",
    "# Make a pipeline for the featurizer combined with a dummy regressor, that simply predicts the overall trained mean of the target variable\n",
    "dummy = make_pipeline(featurizer, DummyRegressor(strategy='mean'))\n",
    "\n",
    "# Make a pipeline for the featurizer and a ridge model, that aims to minimize the sum of squares\n",
    "ridge = make_pipeline(featurizer, Ridge())\n",
    "    \n",
    "# Drop target variable column and fit both models\n",
    "logging.info(\"Fitting models\")\n",
    "dummy.fit(train.drop('year', axis=1), train['year'].values)\n",
    "ridge.fit(train.drop('year', axis=1), train['year'].values)\n",
    "    \n",
    "# Calculate and report both MAE's\n",
    "logging.info(\"Evaluating on validation data\")\n",
    "err = mean_absolute_error(val['year'].values, dummy.predict(val.drop('year', axis=1)))\n",
    "logging.info(f\"Mean baseline MAE: {err}\")\n",
    "err = mean_absolute_error(val['year'].values, ridge.predict(val.drop('year', axis=1)))\n",
    "logging.info(f\"Ridge regress MAE: {err}\")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Preprocessing (Lars)\n",
    "\n",
    "This paragraph build upon the previous baseline code. It entails the following adjustments/additions chronologically:\n",
    "\n",
    "- [x] Removal of dummy regressor, since ridge works better from the very start;\n",
    "- [x] 5-fold cross validation to reduce variability (Ridge regress MAE (5.773));\n",
    "- [x] Entrytype: Dummy-encoded;\n",
    "- [x] Title: TF-IDF vectorized, limited to top X (hyperparameter) occuring (non-stopword) words;\n",
    "- [x] Editor: Removed, >75% missing values;\n",
    "- [x] Publisher: Count vectorized, limited to top X (hyperparameter) occuring publishers;\n",
    "- [x] Author: Count vectorized, limited to top X (hyperparameter) occuring authors;\n",
    "- [x] Abstract: TF-IDF vectorized, limited to top X (hyperparameter) occuring (non-stopword) words;\n",
    "- [ ] Try tasks other than regression, like lazy learning (kNN)(?);\n",
    "- [ ] Try BERTopic modelling;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import extra modules\n",
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For experimental purposes we start working with a 10% subset of the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ENTRYTYPE</th>\n",
       "      <th>title</th>\n",
       "      <th>editor</th>\n",
       "      <th>year</th>\n",
       "      <th>publisher</th>\n",
       "      <th>author</th>\n",
       "      <th>abstract</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>12680</th>\n",
       "      <td>inproceedings</td>\n",
       "      <td>Question-Answering Based on Virtually Integrat...</td>\n",
       "      <td>None</td>\n",
       "      <td>2003</td>\n",
       "      <td>Association for Computational Linguistics</td>\n",
       "      <td>[Choi, Key-Sun, Kim, Jae-Ho, Miyazaki, Masaru,...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17292</th>\n",
       "      <td>inproceedings</td>\n",
       "      <td>AMI&amp;ERIC: How to Learn with Naive Bayes and Pr...</td>\n",
       "      <td>None</td>\n",
       "      <td>2013</td>\n",
       "      <td>Association for Computational Linguistics</td>\n",
       "      <td>[Dermouche, Mohamed, Khouas, Leila, Velcin, Ju...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33265</th>\n",
       "      <td>inproceedings</td>\n",
       "      <td>Inducing Gazetteers for Named Entity Recogniti...</td>\n",
       "      <td>None</td>\n",
       "      <td>2008</td>\n",
       "      <td>Association for Computational Linguistics</td>\n",
       "      <td>[Kazama, Jun'ichi, Torisawa, Kentaro]</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52850</th>\n",
       "      <td>inproceedings</td>\n",
       "      <td>Leveraging Explicit Lexico-logical Alignments ...</td>\n",
       "      <td>None</td>\n",
       "      <td>2022</td>\n",
       "      <td>Association for Computational Linguistics</td>\n",
       "      <td>[Sun, Runxin, He, Shizhu, Zhu, Chong, He, Yaoh...</td>\n",
       "      <td>Text-to-SQL aims to parse natural language que...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2298</th>\n",
       "      <td>inproceedings</td>\n",
       "      <td>CLAM: Quickly deploy NLP command-line tools on...</td>\n",
       "      <td>None</td>\n",
       "      <td>2014</td>\n",
       "      <td>Dublin City University and Association for Com...</td>\n",
       "      <td>[van Gompel, Maarten, Reynaert, Martin]</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           ENTRYTYPE                                              title  \\\n",
       "12680  inproceedings  Question-Answering Based on Virtually Integrat...   \n",
       "17292  inproceedings  AMI&ERIC: How to Learn with Naive Bayes and Pr...   \n",
       "33265  inproceedings  Inducing Gazetteers for Named Entity Recogniti...   \n",
       "52850  inproceedings  Leveraging Explicit Lexico-logical Alignments ...   \n",
       "2298   inproceedings  CLAM: Quickly deploy NLP command-line tools on...   \n",
       "\n",
       "      editor  year                                          publisher  \\\n",
       "12680   None  2003          Association for Computational Linguistics   \n",
       "17292   None  2013          Association for Computational Linguistics   \n",
       "33265   None  2008          Association for Computational Linguistics   \n",
       "52850   None  2022          Association for Computational Linguistics   \n",
       "2298    None  2014  Dublin City University and Association for Com...   \n",
       "\n",
       "                                                  author  \\\n",
       "12680  [Choi, Key-Sun, Kim, Jae-Ho, Miyazaki, Masaru,...   \n",
       "17292  [Dermouche, Mohamed, Khouas, Leila, Velcin, Ju...   \n",
       "33265              [Kazama, Jun'ichi, Torisawa, Kentaro]   \n",
       "52850  [Sun, Runxin, He, Shizhu, Zhu, Chong, He, Yaoh...   \n",
       "2298             [van Gompel, Maarten, Reynaert, Martin]   \n",
       "\n",
       "                                                abstract  \n",
       "12680                                               None  \n",
       "17292                                               None  \n",
       "33265                                               None  \n",
       "52850  Text-to-SQL aims to parse natural language que...  \n",
       "2298                                                None  "
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Randomly save 10% of the train set for velocity purposes\n",
    "percentage_to_save = 30\n",
    "\n",
    "# Calculate the number of rows to save\n",
    "num_rows_to_save = int(len(train) * (percentage_to_save / 100))\n",
    "\n",
    "# Use the sample method to randomly select rows\n",
    "train_sample = train.sample(n=num_rows_to_save, random_state=42)  # Set a random_state for reproducibility\n",
    "\n",
    "train_sample.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.1 Drop all columns with over 75% of missing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19774\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ENTRYTYPE</th>\n",
       "      <th>title</th>\n",
       "      <th>year</th>\n",
       "      <th>publisher</th>\n",
       "      <th>author</th>\n",
       "      <th>abstract</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>12680</th>\n",
       "      <td>inproceedings</td>\n",
       "      <td>Question-Answering Based on Virtually Integrat...</td>\n",
       "      <td>2003</td>\n",
       "      <td>Association for Computational Linguistics</td>\n",
       "      <td>[Choi, Key-Sun, Kim, Jae-Ho, Miyazaki, Masaru,...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17292</th>\n",
       "      <td>inproceedings</td>\n",
       "      <td>AMI&amp;ERIC: How to Learn with Naive Bayes and Pr...</td>\n",
       "      <td>2013</td>\n",
       "      <td>Association for Computational Linguistics</td>\n",
       "      <td>[Dermouche, Mohamed, Khouas, Leila, Velcin, Ju...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33265</th>\n",
       "      <td>inproceedings</td>\n",
       "      <td>Inducing Gazetteers for Named Entity Recogniti...</td>\n",
       "      <td>2008</td>\n",
       "      <td>Association for Computational Linguistics</td>\n",
       "      <td>[Kazama, Jun'ichi, Torisawa, Kentaro]</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52850</th>\n",
       "      <td>inproceedings</td>\n",
       "      <td>Leveraging Explicit Lexico-logical Alignments ...</td>\n",
       "      <td>2022</td>\n",
       "      <td>Association for Computational Linguistics</td>\n",
       "      <td>[Sun, Runxin, He, Shizhu, Zhu, Chong, He, Yaoh...</td>\n",
       "      <td>Text-to-SQL aims to parse natural language que...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2298</th>\n",
       "      <td>inproceedings</td>\n",
       "      <td>CLAM: Quickly deploy NLP command-line tools on...</td>\n",
       "      <td>2014</td>\n",
       "      <td>Dublin City University and Association for Com...</td>\n",
       "      <td>[van Gompel, Maarten, Reynaert, Martin]</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           ENTRYTYPE                                              title  year  \\\n",
       "12680  inproceedings  Question-Answering Based on Virtually Integrat...  2003   \n",
       "17292  inproceedings  AMI&ERIC: How to Learn with Naive Bayes and Pr...  2013   \n",
       "33265  inproceedings  Inducing Gazetteers for Named Entity Recogniti...  2008   \n",
       "52850  inproceedings  Leveraging Explicit Lexico-logical Alignments ...  2022   \n",
       "2298   inproceedings  CLAM: Quickly deploy NLP command-line tools on...  2014   \n",
       "\n",
       "                                               publisher  \\\n",
       "12680          Association for Computational Linguistics   \n",
       "17292          Association for Computational Linguistics   \n",
       "33265          Association for Computational Linguistics   \n",
       "52850          Association for Computational Linguistics   \n",
       "2298   Dublin City University and Association for Com...   \n",
       "\n",
       "                                                  author  \\\n",
       "12680  [Choi, Key-Sun, Kim, Jae-Ho, Miyazaki, Masaru,...   \n",
       "17292  [Dermouche, Mohamed, Khouas, Leila, Velcin, Ju...   \n",
       "33265              [Kazama, Jun'ichi, Torisawa, Kentaro]   \n",
       "52850  [Sun, Runxin, He, Shizhu, Zhu, Chong, He, Yaoh...   \n",
       "2298             [van Gompel, Maarten, Reynaert, Martin]   \n",
       "\n",
       "                                                abstract  \n",
       "12680                                               None  \n",
       "17292                                               None  \n",
       "33265                                               None  \n",
       "52850  Text-to-SQL aims to parse natural language que...  \n",
       "2298                                                None  "
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set threshold on 75%\n",
    "threshold = 0.25\n",
    "\n",
    "# Calculate the threshold for each column\n",
    "missing_threshold = int(threshold * len(train_sample))\n",
    "\n",
    "# Drop columns with more than the specified percentage of missing data\n",
    "train_filtered = train_sample.dropna(axis=1, thresh=missing_threshold)\n",
    "\n",
    "print(len(train_filtered))\n",
    "train_filtered.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.2 Featurize 'author' column (count-vectors, reduced to top X most frequent authors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19774\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Gebruiker\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>baldwin, timothy</th>\n",
       "      <th>bandyopadhyay, sivaji</th>\n",
       "      <th>bansal, mohit</th>\n",
       "      <th>baroni, marco</th>\n",
       "      <th>bhattacharyya, pushpak</th>\n",
       "      <th>biemann, chris</th>\n",
       "      <th>bojar, ondřej</th>\n",
       "      <th>bond, francis</th>\n",
       "      <th>callison-burch, chris</th>\n",
       "      <th>cardie, claire</th>\n",
       "      <th>...</th>\n",
       "      <th>zhang, min</th>\n",
       "      <th>zhang, qi</th>\n",
       "      <th>zhang, yue</th>\n",
       "      <th>zhao, hai</th>\n",
       "      <th>zhao, jun</th>\n",
       "      <th>zhou, guodong</th>\n",
       "      <th>zhou, jie</th>\n",
       "      <th>zhou, ming</th>\n",
       "      <th>zong, chengqing</th>\n",
       "      <th>øvrelid, lilja</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 119 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   baldwin, timothy  bandyopadhyay, sivaji  bansal, mohit  baroni, marco  \\\n",
       "0                 0                      0              0              0   \n",
       "1                 0                      0              0              0   \n",
       "2                 0                      0              0              0   \n",
       "3                 0                      0              0              0   \n",
       "4                 0                      0              0              0   \n",
       "\n",
       "   bhattacharyya, pushpak  biemann, chris  bojar, ondřej  bond, francis  \\\n",
       "0                       0               0              0              0   \n",
       "1                       0               0              0              0   \n",
       "2                       0               0              0              0   \n",
       "3                       0               0              0              0   \n",
       "4                       0               0              0              0   \n",
       "\n",
       "   callison-burch, chris  cardie, claire  ...  zhang, min  zhang, qi  \\\n",
       "0                      0               0  ...           0          0   \n",
       "1                      0               0  ...           0          0   \n",
       "2                      0               0  ...           0          0   \n",
       "3                      0               0  ...           0          0   \n",
       "4                      0               0  ...           0          0   \n",
       "\n",
       "   zhang, yue  zhao, hai  zhao, jun  zhou, guodong  zhou, jie  zhou, ming  \\\n",
       "0           0          0          0              0          0           0   \n",
       "1           0          0          0              0          0           0   \n",
       "2           0          0          0              0          0           0   \n",
       "3           0          0          1              0          0           0   \n",
       "4           0          0          0              0          0           0   \n",
       "\n",
       "   zong, chengqing  øvrelid, lilja  \n",
       "0                0               0  \n",
       "1                0               0  \n",
       "2                0               0  \n",
       "3                0               0  \n",
       "4                0               0  \n",
       "\n",
       "[5 rows x 119 columns]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert lists of strings, accounting for None values\n",
    "train_filtered['author_str'] = train_filtered['author'].apply(lambda x: ';'.join(map(str, x)) if x is not None else 'unknown')\n",
    "\n",
    "# Add a column to store the original row numbers\n",
    "# train_filtered['original_index'] = train_filtered.index\n",
    "\n",
    "# Count the number of papers for each author\n",
    "author_paper_counts = train_filtered['author_str'].str.split(';').explode().value_counts()\n",
    "\n",
    "# Set the number of most frequent authors you want to include\n",
    "n_mostfreq_authors = 120  # Adjust this value to the desired number of most frequent authors\n",
    "\n",
    "# Filter authors based on the X most frequent authors\n",
    "top_authors = author_paper_counts.head(n_mostfreq_authors).index.tolist()\n",
    "\n",
    "# Filter only the top authors in 'author_str'\n",
    "train_filtered['author_str_filtered'] = train_filtered['author_str'].apply(lambda x: ';'.join([author for author in x.split(';') if author in top_authors]))\n",
    "\n",
    "# Count-vectorize 'author_str_filtered'\n",
    "count_vectorizer = CountVectorizer(tokenizer=lambda x: x.split(';'))\n",
    "count_matrix = count_vectorizer.fit_transform(train_filtered['author_str_filtered'])\n",
    "\n",
    "# Extract and create columns\n",
    "feature_names = count_vectorizer.get_feature_names_out()\n",
    "author_count_df = pd.DataFrame(count_matrix.toarray(), columns=feature_names)\n",
    "\n",
    "# Set the original_index column as the index\n",
    "# author_count_df.set_index(train_filtered['original_index'], inplace=True)\n",
    "\n",
    "author_count_df = author_count_df.drop(['unknown', ''], axis=1) # See if this approach always works out\n",
    "\n",
    "print(len(author_count_df))\n",
    "author_count_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merge with train_filtered, meaning we drop the author column and then add author_count_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop all redundant columns\n",
    "# train_filtered_buffer = train_filtered.drop(['author', 'author_str', 'author_str_filtered'], axis=1)\n",
    "\n",
    "# Concatenate the original with dropped redundants and the extracted features for author\n",
    "# train_2 = pd.concat([train_filtered_buffer, author_count_df], axis=1).reindex(train_filtered_buffer.index)\n",
    "\n",
    "# print(len(train_2))\n",
    "# train_2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.3 Vectorize 'ENTRYTYPE' column (3-categorical variable one-hot encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category_article</th>\n",
       "      <th>category_inproceedings</th>\n",
       "      <th>category_proceedings</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>19139</td>\n",
       "      <td>1177</td>\n",
       "      <td>19232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>635</td>\n",
       "      <td>18597</td>\n",
       "      <td>542</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   category_article  category_inproceedings  category_proceedings\n",
       "0             19139                    1177                 19232\n",
       "1               635                   18597                   542"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Perform one-hot encoding\n",
    "train_encoded_entrytype = pd.get_dummies(train_filtered['ENTRYTYPE'], columns=['category'], prefix='category')\n",
    "\n",
    "# Show count-values for each of the columns\n",
    "train_encoded_entrytype.apply(lambda x: x.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merge with train_filtered, meaning we drop the ENTRYTYPE column and then add train_encoded_entrytype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop all redundant columns\n",
    "# train_2 = train_2.drop(['ENTRYTYPE'], axis=1)\n",
    "\n",
    "# Concatenate the original with dropped redundants and the extracted features for ENTRYTYPE\n",
    "# train_3 = pd.concat([train_2, train_encoded_entrytype], axis=1).reindex(train_2.index)\n",
    "\n",
    "# print(len(train_3))\n",
    "# train_3.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.4 Vectorize 'Publisher' column (116-categorical variable count-vectorized, and reduced to X most frequent publishers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>publisher_ATALA</th>\n",
       "      <th>publisher_ATALA (Association pour le Traitement Automatique des Langues)</th>\n",
       "      <th>publisher_Asian Federation of Natural Language Processing</th>\n",
       "      <th>publisher_Aslib</th>\n",
       "      <th>publisher_Association for Computational Linguistics</th>\n",
       "      <th>publisher_Association for Machine Translation in the Americas</th>\n",
       "      <th>publisher_COLING</th>\n",
       "      <th>publisher_Coling 2008 Organizing Committee</th>\n",
       "      <th>publisher_Coling 2010 Organizing Committee</th>\n",
       "      <th>publisher_European Association for Machine Translation</th>\n",
       "      <th>publisher_European Language Resources Association</th>\n",
       "      <th>publisher_European Language Resources Association (ELRA)</th>\n",
       "      <th>publisher_INCOMA Ltd.</th>\n",
       "      <th>publisher_INCOMA Ltd. Shoumen, BULGARIA</th>\n",
       "      <th>publisher_International Committee on Computational Linguistics</th>\n",
       "      <th>publisher_MIT Press</th>\n",
       "      <th>publisher_NLP Association of India</th>\n",
       "      <th>publisher_The Association for Computational Linguistics and Chinese Language Processing (ACLCLP)</th>\n",
       "      <th>publisher_The COLING 2012 Organizing Committee</th>\n",
       "      <th>publisher_The COLING 2016 Organizing Committee</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>15526</td>\n",
       "      <td>15839</td>\n",
       "      <td>15710</td>\n",
       "      <td>15776</td>\n",
       "      <td>4748</td>\n",
       "      <td>15726</td>\n",
       "      <td>15804</td>\n",
       "      <td>15826</td>\n",
       "      <td>15812</td>\n",
       "      <td>15710</td>\n",
       "      <td>15357</td>\n",
       "      <td>14578</td>\n",
       "      <td>15778</td>\n",
       "      <td>15839</td>\n",
       "      <td>15603</td>\n",
       "      <td>15524</td>\n",
       "      <td>15844</td>\n",
       "      <td>15674</td>\n",
       "      <td>15787</td>\n",
       "      <td>15734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>379</td>\n",
       "      <td>66</td>\n",
       "      <td>195</td>\n",
       "      <td>129</td>\n",
       "      <td>11157</td>\n",
       "      <td>179</td>\n",
       "      <td>101</td>\n",
       "      <td>79</td>\n",
       "      <td>93</td>\n",
       "      <td>195</td>\n",
       "      <td>548</td>\n",
       "      <td>1327</td>\n",
       "      <td>127</td>\n",
       "      <td>66</td>\n",
       "      <td>302</td>\n",
       "      <td>381</td>\n",
       "      <td>61</td>\n",
       "      <td>231</td>\n",
       "      <td>118</td>\n",
       "      <td>171</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   publisher_ATALA  \\\n",
       "0            15526   \n",
       "1              379   \n",
       "\n",
       "   publisher_ATALA (Association pour le Traitement Automatique des Langues)  \\\n",
       "0                                              15839                          \n",
       "1                                                 66                          \n",
       "\n",
       "   publisher_Asian Federation of Natural Language Processing  publisher_Aslib  \\\n",
       "0                                              15710                    15776   \n",
       "1                                                195                      129   \n",
       "\n",
       "   publisher_Association for Computational Linguistics  \\\n",
       "0                                               4748     \n",
       "1                                              11157     \n",
       "\n",
       "   publisher_Association for Machine Translation in the Americas  \\\n",
       "0                                              15726               \n",
       "1                                                179               \n",
       "\n",
       "   publisher_COLING  publisher_Coling 2008 Organizing Committee  \\\n",
       "0             15804                                       15826   \n",
       "1               101                                          79   \n",
       "\n",
       "   publisher_Coling 2010 Organizing Committee  \\\n",
       "0                                       15812   \n",
       "1                                          93   \n",
       "\n",
       "   publisher_European Association for Machine Translation  \\\n",
       "0                                              15710        \n",
       "1                                                195        \n",
       "\n",
       "   publisher_European Language Resources Association  \\\n",
       "0                                              15357   \n",
       "1                                                548   \n",
       "\n",
       "   publisher_European Language Resources Association (ELRA)  \\\n",
       "0                                              14578          \n",
       "1                                               1327          \n",
       "\n",
       "   publisher_INCOMA Ltd.  publisher_INCOMA Ltd. Shoumen, BULGARIA  \\\n",
       "0                  15778                                    15839   \n",
       "1                    127                                       66   \n",
       "\n",
       "   publisher_International Committee on Computational Linguistics  \\\n",
       "0                                              15603                \n",
       "1                                                302                \n",
       "\n",
       "   publisher_MIT Press  publisher_NLP Association of India  \\\n",
       "0                15524                               15844   \n",
       "1                  381                                  61   \n",
       "\n",
       "   publisher_The Association for Computational Linguistics and Chinese Language Processing (ACLCLP)  \\\n",
       "0                                              15674                                                  \n",
       "1                                                231                                                  \n",
       "\n",
       "   publisher_The COLING 2012 Organizing Committee  \\\n",
       "0                                           15787   \n",
       "1                                             118   \n",
       "\n",
       "   publisher_The COLING 2016 Organizing Committee  \n",
       "0                                           15734  \n",
       "1                                             171  "
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set the number of most frequent publishers to include\n",
    "n_mostfreq_publishers = 20  # Adjust this value as needed\n",
    "\n",
    "# Get the X most frequent publishers\n",
    "top_publishers = train_filtered['publisher'].value_counts().head(n_mostfreq_publishers).index.tolist()\n",
    "\n",
    "# Create a new DataFrame with one-hot encoding for the X most frequent publishers\n",
    "train_encoded_publisher = pd.get_dummies(train_filtered['publisher'][train_filtered['publisher'].isin(top_publishers)], prefix='publisher')\n",
    "\n",
    "# Show count-values for each of the columns\n",
    "train_encoded_publisher.apply(lambda x: x.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merge with train_filtered, meaning we drop the 'publisher' column and then add train_encoded_entrytype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop all redundant columns\n",
    "# train_3 = train_3.drop(['publisher'], axis=1)\n",
    "\n",
    "# Concatenate the original with dropped redundants and the extracted features for ENTRYTYPE\n",
    "# train_4 = pd.concat([train_3, train_encoded_publisher], axis=1).reindex(train_3.index).fillna(0)\n",
    "\n",
    "# print(len(train_4))\n",
    "# train_4.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.5.1 Vectorize 'title' and 'abstract' column (English-translated with stop-words removal and/or synonym replacement)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfrom googletrans import Translator\\nfrom langdetect import detect\\n'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "from googletrans import Translator\n",
    "from langdetect import detect\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\ndef det(x):\\n    try:\\n        lang = detect(x)\\n    except:\\n        lang = 'Other'\\n    return lang\\n\""
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "def det(x):\n",
    "    try:\n",
    "        lang = detect(x)\n",
    "    except:\n",
    "        lang = 'Other'\n",
    "    return lang\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\ntrain_filtered['language_title'] = train_filtered['title'].apply(det)\\ntrain_filtered['language_abstract'] = train_filtered['abstract'].apply(det)\\n\""
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "train_filtered['language_title'] = train_filtered['title'].apply(det)\n",
    "train_filtered['language_abstract'] = train_filtered['abstract'].apply(det)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\ntranslator = Translator(service_urls=['translate.googleapis.com'])\\n\\n# Function to translate non-English titles to English based on 'translated_title' column\\ndef translate_to_english(dataframe, column, translated_column):\\n    for i in dataframe[column].index:\\n        # Check if the value in 'translated_title' is not 'en' or 'Other' before translation\\n        if dataframe[translated_column][i] not in ['en', 'Other']:\\n            dataframe[column][i] = translator.translate(dataframe[column][i], dest='en').text\\n\""
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "translator = Translator(service_urls=['translate.googleapis.com'])\n",
    "\n",
    "# Function to translate non-English titles to English based on 'translated_title' column\n",
    "def translate_to_english(dataframe, column, translated_column):\n",
    "    for i in dataframe[column].index:\n",
    "        # Check if the value in 'translated_title' is not 'en' or 'Other' before translation\n",
    "        if dataframe[translated_column][i] not in ['en', 'Other']:\n",
    "            dataframe[column][i] = translator.translate(dataframe[column][i], dest='en').text\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nfrom deep_translator import GoogleTranslator\\n\\n# Function to translate non-English titles to English based on 'translated_title' column\\ndef translate_to_english(dataframe, column, translated_column):\\n    for i in dataframe[column].index:\\n        # Check if the value in 'translated_title' is not 'en' or 'Other' before translation\\n        if dataframe[translated_column][i] not in ['en', 'Other']:\\n            dataframe[column][i] = GoogleTranslator(source='auto', target='en').translate(text=dataframe[column][i])\\n\""
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "from deep_translator import GoogleTranslator\n",
    "\n",
    "# Function to translate non-English titles to English based on 'translated_title' column\n",
    "def translate_to_english(dataframe, column, translated_column):\n",
    "    for i in dataframe[column].index:\n",
    "        # Check if the value in 'translated_title' is not 'en' or 'Other' before translation\n",
    "        if dataframe[translated_column][i] not in ['en', 'Other']:\n",
    "            dataframe[column][i] = GoogleTranslator(source='auto', target='en').translate(text=dataframe[column][i])\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\ntranslate_to_english(train_filtered, 'title', 'language_title')\\ntranslate_to_english(train_filtered, 'abstract', 'language_abstract')\\n\""
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "translate_to_english(train_filtered, 'title', 'language_title')\n",
    "translate_to_english(train_filtered, 'abstract', 'language_abstract')\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_4['new_language_title'] = train_4['title'].apply(det)\n",
    "# train_4['new_language_abstract'] = train_4['abstract'].apply(det)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_4['new_language_title'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_4['new_language_abstract'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>abstract</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>12680</th>\n",
       "      <td>Question-Answering Based on Virtually Integrat...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17292</th>\n",
       "      <td>AMI&amp;ERIC: How to Learn with Naive Bayes and Pr...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33265</th>\n",
       "      <td>Inducing Gazetteers for Named Entity Recogniti...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52850</th>\n",
       "      <td>Leveraging Explicit Lexico-logical Alignments ...</td>\n",
       "      <td>Text-to-SQL aims to parse natural language que...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2298</th>\n",
       "      <td>CLAM: Quickly deploy NLP command-line tools on...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7809</th>\n",
       "      <td>Usability Recommendations for Annotation Tools</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6236</th>\n",
       "      <td>Deep Span Representations for Named Entity Rec...</td>\n",
       "      <td>Span-based models are one of the most straight...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63616</th>\n",
       "      <td>Comparing a Hand-crafted to an Automatically G...</td>\n",
       "      <td>The automatic evaluation of machine translatio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6396</th>\n",
       "      <td>Thai Stock News Sentiment Classification using...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28216</th>\n",
       "      <td>Effects of Empty Categories on Machine Transla...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>19774 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   title  \\\n",
       "12680  Question-Answering Based on Virtually Integrat...   \n",
       "17292  AMI&ERIC: How to Learn with Naive Bayes and Pr...   \n",
       "33265  Inducing Gazetteers for Named Entity Recogniti...   \n",
       "52850  Leveraging Explicit Lexico-logical Alignments ...   \n",
       "2298   CLAM: Quickly deploy NLP command-line tools on...   \n",
       "...                                                  ...   \n",
       "7809      Usability Recommendations for Annotation Tools   \n",
       "6236   Deep Span Representations for Named Entity Rec...   \n",
       "63616  Comparing a Hand-crafted to an Automatically G...   \n",
       "6396   Thai Stock News Sentiment Classification using...   \n",
       "28216  Effects of Empty Categories on Machine Transla...   \n",
       "\n",
       "                                                abstract  \n",
       "12680                                               None  \n",
       "17292                                               None  \n",
       "33265                                               None  \n",
       "52850  Text-to-SQL aims to parse natural language que...  \n",
       "2298                                                None  \n",
       "...                                                  ...  \n",
       "7809                                                None  \n",
       "6236   Span-based models are one of the most straight...  \n",
       "63616  The automatic evaluation of machine translatio...  \n",
       "6396                                                None  \n",
       "28216                                               None  \n",
       "\n",
       "[19774 rows x 2 columns]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_textcolumns = train_filtered[['title', 'abstract']]\n",
    "train_textcolumns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We've transformed the 'title' column to a dataframe of 500 columns.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>19</th>\n",
       "      <th>2016</th>\n",
       "      <th>2017</th>\n",
       "      <th>2018</th>\n",
       "      <th>2019</th>\n",
       "      <th>2020</th>\n",
       "      <th>2021</th>\n",
       "      <th>2022</th>\n",
       "      <th>2023</th>\n",
       "      <th>abstract</th>\n",
       "      <th>...</th>\n",
       "      <th>web</th>\n",
       "      <th>wikipedia</th>\n",
       "      <th>wmt</th>\n",
       "      <th>word</th>\n",
       "      <th>wordnet</th>\n",
       "      <th>words</th>\n",
       "      <th>workshop</th>\n",
       "      <th>world</th>\n",
       "      <th>writing</th>\n",
       "      <th>zero</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.545583</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 500 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    19  2016  2017  2018  2019  2020  2021  2022  2023  abstract  ...  \\\n",
       "0  0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0       0.0  ...   \n",
       "1  0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0       0.0  ...   \n",
       "2  0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0       0.0  ...   \n",
       "3  0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0       0.0  ...   \n",
       "4  0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0       0.0  ...   \n",
       "\n",
       "        web  wikipedia  wmt  word  wordnet  words  workshop  world  writing  \\\n",
       "0  0.000000        0.0  0.0   0.0      0.0    0.0       0.0    0.0      0.0   \n",
       "1  0.000000        0.0  0.0   0.0      0.0    0.0       0.0    0.0      0.0   \n",
       "2  0.000000        0.0  0.0   0.0      0.0    0.0       0.0    0.0      0.0   \n",
       "3  0.000000        0.0  0.0   0.0      0.0    0.0       0.0    0.0      0.0   \n",
       "4  0.545583        0.0  0.0   0.0      0.0    0.0       0.0    0.0      0.0   \n",
       "\n",
       "   zero  \n",
       "0   0.0  \n",
       "1   0.0  \n",
       "2   0.0  \n",
       "3   0.0  \n",
       "4   0.0  \n",
       "\n",
       "[5 rows x 500 columns]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Specify the number of most frequent tokens you want to keep (replace X with the desired value)\n",
    "max_features_title = 500\n",
    "\n",
    "# Create a list of English stopwords\n",
    "stop_words = 'english'\n",
    "\n",
    "# Apply the TF-IDF vectorizer to column 'title' with max_features parameter\n",
    "tfidf_vectorizer = TfidfVectorizer(stop_words=stop_words, max_features=max_features_title)\n",
    "\n",
    "# Apply the TF-IDF vectorizer to column 'title'\n",
    "tfidf_matrix_title = tfidf_vectorizer.fit_transform(train_textcolumns['title'])\n",
    "\n",
    "# Extract and create columns\n",
    "feature_names = tfidf_vectorizer.get_feature_names_out()\n",
    "tfidf_df_title = pd.DataFrame(tfidf_matrix_title.toarray(), columns=feature_names)\n",
    "\n",
    "print(f\"We've transformed the 'title' column to a dataframe of {len(tfidf_df_title.columns)} columns.\")\n",
    "tfidf_df_title.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Gebruiker\\AppData\\Local\\Temp\\ipykernel_14408\\261390455.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  train_textcolumns['abstract'].replace({None: '', '0': ''}, inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We've transformed the 'abstract' column to a dataframe of 500 columns.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ability</th>\n",
       "      <th>able</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>achieve</th>\n",
       "      <th>achieved</th>\n",
       "      <th>achieves</th>\n",
       "      <th>adaptation</th>\n",
       "      <th>addition</th>\n",
       "      <th>additional</th>\n",
       "      <th>address</th>\n",
       "      <th>...</th>\n",
       "      <th>web</th>\n",
       "      <th>wikipedia</th>\n",
       "      <th>word</th>\n",
       "      <th>words</th>\n",
       "      <th>work</th>\n",
       "      <th>works</th>\n",
       "      <th>world</th>\n",
       "      <th>written</th>\n",
       "      <th>years</th>\n",
       "      <th>zero</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.173568</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.115918</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 500 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   ability  able  accuracy  achieve  achieved  achieves  adaptation  addition  \\\n",
       "0      0.0   0.0       0.0      0.0       0.0       0.0         0.0       0.0   \n",
       "1      0.0   0.0       0.0      0.0       0.0       0.0         0.0       0.0   \n",
       "2      0.0   0.0       0.0      0.0       0.0       0.0         0.0       0.0   \n",
       "3      0.0   0.0       0.0      0.0       0.0       0.0         0.0       0.0   \n",
       "4      0.0   0.0       0.0      0.0       0.0       0.0         0.0       0.0   \n",
       "\n",
       "   additional  address  ...  web  wikipedia  word  words      work  works  \\\n",
       "0    0.000000      0.0  ...  0.0        0.0   0.0    0.0  0.000000    0.0   \n",
       "1    0.000000      0.0  ...  0.0        0.0   0.0    0.0  0.000000    0.0   \n",
       "2    0.000000      0.0  ...  0.0        0.0   0.0    0.0  0.000000    0.0   \n",
       "3    0.173568      0.0  ...  0.0        0.0   0.0    0.0  0.115918    0.0   \n",
       "4    0.000000      0.0  ...  0.0        0.0   0.0    0.0  0.000000    0.0   \n",
       "\n",
       "   world  written  years  zero  \n",
       "0    0.0      0.0    0.0   0.0  \n",
       "1    0.0      0.0    0.0   0.0  \n",
       "2    0.0      0.0    0.0   0.0  \n",
       "3    0.0      0.0    0.0   0.0  \n",
       "4    0.0      0.0    0.0   0.0  \n",
       "\n",
       "[5 rows x 500 columns]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Specify the number of most frequent tokens you want to keep (replace X with the desired value)\n",
    "max_features_abstract = 500\n",
    "\n",
    "# Create a list of English stopwords\n",
    "stop_words = 'english'\n",
    "\n",
    "# Handle missing values and '0's in the 'abstract' column\n",
    "train_textcolumns['abstract'].replace({None: '', '0': ''}, inplace=True)\n",
    "\n",
    "# Apply the TF-IDF vectorizer to column 'title' with max_features parameter\n",
    "tfidf_vectorizer = TfidfVectorizer(stop_words=stop_words, max_features=max_features_abstract)\n",
    "\n",
    "# Apply the TF-IDF vectorizer to column 'title'\n",
    "tfidf_matrix_abstract = tfidf_vectorizer.fit_transform(train_textcolumns['abstract'])\n",
    "\n",
    "# Extract and create columns\n",
    "feature_names = tfidf_vectorizer.get_feature_names_out()\n",
    "tfidf_df_abstract = pd.DataFrame(tfidf_matrix_abstract.toarray(), columns=feature_names)\n",
    "\n",
    "print(f\"We've transformed the 'abstract' column to a dataframe of {len(tfidf_df_abstract.columns)} columns.\")\n",
    "tfidf_df_abstract.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Merge the following dataframes: author_count_df, train_encoded_entrytype, train_encoded_publisher, tfidf_df_title, tfidf_df_abstract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "year_df = train_sample['year']\n",
    "\n",
    "features_df = pd.concat([author_count_df, train_encoded_entrytype, train_encoded_publisher, tfidf_df_title, tfidf_df_abstract, year_df], axis=1).reindex(year_df.index).fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>baldwin, timothy</th>\n",
       "      <th>bandyopadhyay, sivaji</th>\n",
       "      <th>bansal, mohit</th>\n",
       "      <th>baroni, marco</th>\n",
       "      <th>bhattacharyya, pushpak</th>\n",
       "      <th>biemann, chris</th>\n",
       "      <th>bojar, ondřej</th>\n",
       "      <th>bond, francis</th>\n",
       "      <th>callison-burch, chris</th>\n",
       "      <th>cardie, claire</th>\n",
       "      <th>...</th>\n",
       "      <th>wikipedia</th>\n",
       "      <th>word</th>\n",
       "      <th>words</th>\n",
       "      <th>work</th>\n",
       "      <th>works</th>\n",
       "      <th>world</th>\n",
       "      <th>written</th>\n",
       "      <th>years</th>\n",
       "      <th>zero</th>\n",
       "      <th>year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>12680</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17292</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33265</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52850</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2298</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.062171</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.100883</td>\n",
       "      <td>2014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7809</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6236</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63616</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6396</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28216</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2010</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>19774 rows × 1143 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       baldwin, timothy  bandyopadhyay, sivaji  bansal, mohit  baroni, marco  \\\n",
       "12680               0.0                    0.0            0.0            0.0   \n",
       "17292               0.0                    0.0            0.0            0.0   \n",
       "33265               0.0                    0.0            0.0            0.0   \n",
       "52850               0.0                    0.0            0.0            0.0   \n",
       "2298                0.0                    0.0            0.0            0.0   \n",
       "...                 ...                    ...            ...            ...   \n",
       "7809                0.0                    0.0            0.0            0.0   \n",
       "6236                0.0                    0.0            0.0            0.0   \n",
       "63616               0.0                    0.0            0.0            0.0   \n",
       "6396                0.0                    0.0            0.0            0.0   \n",
       "28216               0.0                    0.0            0.0            0.0   \n",
       "\n",
       "       bhattacharyya, pushpak  biemann, chris  bojar, ondřej  bond, francis  \\\n",
       "12680                     0.0             0.0            0.0            0.0   \n",
       "17292                     0.0             0.0            0.0            0.0   \n",
       "33265                     0.0             0.0            0.0            0.0   \n",
       "52850                     0.0             0.0            0.0            0.0   \n",
       "2298                      0.0             0.0            0.0            0.0   \n",
       "...                       ...             ...            ...            ...   \n",
       "7809                      0.0             0.0            0.0            0.0   \n",
       "6236                      0.0             0.0            0.0            0.0   \n",
       "63616                     0.0             0.0            0.0            0.0   \n",
       "6396                      0.0             0.0            0.0            0.0   \n",
       "28216                     0.0             0.0            0.0            0.0   \n",
       "\n",
       "       callison-burch, chris  cardie, claire  ...  wikipedia  word  words  \\\n",
       "12680                    0.0             0.0  ...        0.0   0.0    0.0   \n",
       "17292                    0.0             0.0  ...        0.0   0.0    0.0   \n",
       "33265                    0.0             0.0  ...        0.0   0.0    0.0   \n",
       "52850                    0.0             0.0  ...        0.0   0.0    0.0   \n",
       "2298                     0.0             0.0  ...        0.0   0.0    0.0   \n",
       "...                      ...             ...  ...        ...   ...    ...   \n",
       "7809                     0.0             0.0  ...        0.0   0.0    0.0   \n",
       "6236                     0.0             0.0  ...        0.0   0.0    0.0   \n",
       "63616                    0.0             0.0  ...        0.0   0.0    0.0   \n",
       "6396                     0.0             0.0  ...        0.0   0.0    0.0   \n",
       "28216                    0.0             0.0  ...        0.0   0.0    0.0   \n",
       "\n",
       "           work  works  world  written  years      zero  year  \n",
       "12680  0.000000    0.0    0.0      0.0    0.0  0.000000  2003  \n",
       "17292  0.000000    0.0    0.0      0.0    0.0  0.000000  2013  \n",
       "33265  0.000000    0.0    0.0      0.0    0.0  0.000000  2008  \n",
       "52850  0.000000    0.0    0.0      0.0    0.0  0.000000  2022  \n",
       "2298   0.062171    0.0    0.0      0.0    0.0  0.100883  2014  \n",
       "...         ...    ...    ...      ...    ...       ...   ...  \n",
       "7809   0.000000    0.0    0.0      0.0    0.0  0.000000  2012  \n",
       "6236   0.000000    0.0    0.0      0.0    0.0  0.000000  2023  \n",
       "63616  0.000000    0.0    0.0      0.0    0.0  0.000000  2019  \n",
       "6396   0.000000    0.0    0.0      0.0    0.0  0.000000  2015  \n",
       "28216  0.000000    0.0    0.0      0.0    0.0  0.000000  2010  \n",
       "\n",
       "[19774 rows x 1143 columns]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Fit baseline model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Splitting validation\n",
      "INFO:root:Fitting models\n",
      "INFO:root:Evaluating on validation data\n",
      "INFO:root:Ridge regress MAE: 6.521680811338317\n"
     ]
    }
   ],
   "source": [
    "# Split the train set into train (75%) and validation (25%) sets\n",
    "logging.info(\"Splitting validation\")\n",
    "train, val = train_test_split(features_df, stratify=features_df['year'], random_state=123)\n",
    "\n",
    "# Make a pipeline for the featurizer and a ridge model, that aims to minimize the sum of squares\n",
    "ridge = Ridge()\n",
    "    \n",
    "# Drop target variable column and fit both models\n",
    "logging.info(\"Fitting models\")\n",
    "ridge.fit(train.drop('year', axis=1), train['year'].values)\n",
    "    \n",
    "# Calculate and report both MAE's\n",
    "logging.info(\"Evaluating on validation data\")\n",
    "err = mean_absolute_error(val['year'].values, ridge.predict(val.drop('year', axis=1)))\n",
    "logging.info(f\"Ridge regress MAE: {err}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Incorporate into baseline code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# Set the logging level to INFO and set loading message\\nlogging.getLogger().setLevel(logging.INFO)\\n    \\n# Load train and test sets and change all NA values to empty values\\nlogging.info(\"Loading training/test data\")\\n#train = pd.DataFrame.from_records(json.load(open(\\'../data/train.json\\'))).fillna(\"\")\\n#test = pd.DataFrame.from_records(json.load(open(\\'../data/test.json\\'))).fillna(\"\")\\n    \\n# Split the train set into train (80%) and validation (20%) sets, 5-folds\\nlogging.info(\"Splitting validation\")\\nnum_folds = 5\\nk_fold = KFold(n_splits=num_folds, shuffle=True, random_state=123)\\n    \\n# Store a featurizer to transform the \\'title\\' column into a bag-of-words format\\nfeaturizer_1 = ColumnTransformer(\\n    transformers=[(\"title\", CountVectorizer(), \"title\")], remainder=\\'drop\\')\\nfeaturizer_2 = ColumnTransformer(\\n    transformers=[(\"title\", TfidfVectorizer(), \"title\")], remainder=\\'drop\\')\\nfeaturizer_3 = ColumnTransformer(\\n    transformers=[(\"abstract\", CountVectorizer(), \"abstract\")], remainder=\\'drop\\')\\nfeaturizer_4 = ColumnTransformer(\\n    transformers=[(\"abstract\", TfidfVectorizer(), \"abstract\")], remainder=\\'drop\\')\\nfeaturizers = [featurizer_1, featurizer_2, featurizer_3, featurizer_4]\\n\\nfor i, featurizer in enumerate(featurizers):\\n    # Make a pipeline for the featurizer and a ridge model, that aims to minimize the sum of squares\\n    ridge_cv = make_pipeline(featurizer, Ridge())\\n    \\n    # Drop target variable column and fit both models\\n    logging.info(f\"Fitting model with featurizer {i+1}\")\\n    ridge_cv.fit(train_4.drop(\\'year\\', axis=1), train_4[\\'year\\'].values)\\n    \\n    # Calculate and report both MAE\\'s\\n    logging.info(\"Evaluating on validation data\")\\n    ridge_cv_scores = cross_val_score(ridge_cv, train_4.drop(\\'year\\', axis=1), train_4[\\'year\\'].values, cv=k_fold, scoring=\\'neg_mean_absolute_error\\')\\n    logging.info(f\"Ridge regress MAE with featurizer {i+1} ({num_folds}-fold cross-validated): {-ridge_cv_scores.mean()}\")\\n'"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "# Set the logging level to INFO and set loading message\n",
    "logging.getLogger().setLevel(logging.INFO)\n",
    "    \n",
    "# Load train and test sets and change all NA values to empty values\n",
    "logging.info(\"Loading training/test data\")\n",
    "#train = pd.DataFrame.from_records(json.load(open('../data/train.json'))).fillna(\"\")\n",
    "#test = pd.DataFrame.from_records(json.load(open('../data/test.json'))).fillna(\"\")\n",
    "    \n",
    "# Split the train set into train (80%) and validation (20%) sets, 5-folds\n",
    "logging.info(\"Splitting validation\")\n",
    "num_folds = 5\n",
    "k_fold = KFold(n_splits=num_folds, shuffle=True, random_state=123)\n",
    "    \n",
    "# Store a featurizer to transform the 'title' column into a bag-of-words format\n",
    "featurizer_1 = ColumnTransformer(\n",
    "    transformers=[(\"title\", CountVectorizer(), \"title\")], remainder='drop')\n",
    "featurizer_2 = ColumnTransformer(\n",
    "    transformers=[(\"title\", TfidfVectorizer(), \"title\")], remainder='drop')\n",
    "featurizer_3 = ColumnTransformer(\n",
    "    transformers=[(\"abstract\", CountVectorizer(), \"abstract\")], remainder='drop')\n",
    "featurizer_4 = ColumnTransformer(\n",
    "    transformers=[(\"abstract\", TfidfVectorizer(), \"abstract\")], remainder='drop')\n",
    "featurizers = [featurizer_1, featurizer_2, featurizer_3, featurizer_4]\n",
    "\n",
    "for i, featurizer in enumerate(featurizers):\n",
    "    # Make a pipeline for the featurizer and a ridge model, that aims to minimize the sum of squares\n",
    "    ridge_cv = make_pipeline(featurizer, Ridge())\n",
    "    \n",
    "    # Drop target variable column and fit both models\n",
    "    logging.info(f\"Fitting model with featurizer {i+1}\")\n",
    "    ridge_cv.fit(train_4.drop('year', axis=1), train_4['year'].values)\n",
    "    \n",
    "    # Calculate and report both MAE's\n",
    "    logging.info(\"Evaluating on validation data\")\n",
    "    ridge_cv_scores = cross_val_score(ridge_cv, train_4.drop('year', axis=1), train_4['year'].values, cv=k_fold, scoring='neg_mean_absolute_error')\n",
    "    logging.info(f\"Ridge regress MAE with featurizer {i+1} ({num_folds}-fold cross-validated): {-ridge_cv_scores.mean()}\")\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
