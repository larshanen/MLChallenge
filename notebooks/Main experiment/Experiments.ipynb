{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Baseline code provided by uni"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import logging\n",
    "import json\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.dummy import DummyRegressor\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.metrics import mean_absolute_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Baseline function to create [predictions](https://github.com/larshanen/MLChallenge/tree/main/notebooks/predicted.json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    # Set the logging level to INFO and set loading message\n",
    "    logging.getLogger().setLevel(logging.INFO)\n",
    "    \n",
    "    # Load train and test sets and change all NA values to empty values\n",
    "    logging.info(\"Loading training/test data\")\n",
    "    train = pd.DataFrame.from_records(json.load(open('../data/train.json'))).fillna(\"\")\n",
    "    test = pd.DataFrame.from_records(json.load(open('../data/test.json'))).fillna(\"\")\n",
    "    \n",
    "    # Split the train set into train (75%) and validation (25%) sets\n",
    "    logging.info(\"Splitting validation\")\n",
    "    train, val = train_test_split(train, stratify=train['year'], random_state=123)\n",
    "    \n",
    "    # Store a featurizer to transform the 'title' column into a bag-of-words format\n",
    "    featurizer = ColumnTransformer(\n",
    "        transformers=[(\"title\", CountVectorizer(), \"title\")], remainder='drop')\n",
    "    \n",
    "    # Make a pipeline for the featurizer combined with a dummy regressor, that simply predicts the overall trained mean of the target variable\n",
    "    dummy = make_pipeline(featurizer, DummyRegressor(strategy='mean'))\n",
    "\n",
    "    # Make a pipeline for the featurizer and a ridge model, that aims to minimize the sum of squares\n",
    "    ridge = make_pipeline(featurizer, Ridge())\n",
    "    \n",
    "    # Drop target variable column and fit both models\n",
    "    logging.info(\"Fitting models\")\n",
    "    dummy.fit(train.drop('year', axis=1), train['year'].values)\n",
    "    ridge.fit(train.drop('year', axis=1), train['year'].values)\n",
    "    \n",
    "    # Calculate and report both MAE's\n",
    "    logging.info(\"Evaluating on validation data\")\n",
    "    err = mean_absolute_error(val['year'].values, dummy.predict(val.drop('year', axis=1)))\n",
    "    logging.info(f\"Mean baseline MAE: {err}\")\n",
    "    err = mean_absolute_error(val['year'].values, ridge.predict(val.drop('year', axis=1)))\n",
    "    logging.info(f\"Ridge regress MAE: {err}\")\n",
    "    \n",
    "    # Let the ridge model predict on test set\n",
    "    logging.info(f\"Predicting on test\")\n",
    "    pred = ridge.predict(test)\n",
    "    test['year'] = pred\n",
    "    \n",
    "    # Write JSON prediction file\n",
    "    logging.info(\"Writing prediction file\")\n",
    "    test.to_json(\"predicted.json\", orient='records', indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Loading training/test data\n",
      "INFO:root:Splitting validation\n",
      "INFO:root:Fitting models\n",
      "INFO:root:Evaluating on validation data\n",
      "INFO:root:Mean baseline MAE: 7.8054390754858805\n",
      "INFO:root:Ridge regress MAE: 5.812345349001838\n",
      "INFO:root:Predicting on test\n",
      "INFO:root:Writing prediction file\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Team code\n",
    "\n",
    "Please follow the instructions beneath when writing or adjusting code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Describe every piece of code with comments\n",
    "# Include your name in every header so we can report our individual contributions (this is mandatory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Explore baseline performance (Lars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Loading training/test data\n"
     ]
    }
   ],
   "source": [
    "# Set the logging level to INFO and set loading message\n",
    "logging.getLogger().setLevel(logging.INFO)\n",
    "    \n",
    "# Load train and test sets and change all NA values to empty values\n",
    "logging.info(\"Loading training/test data\")\n",
    "train = pd.DataFrame.from_records(json.load(open('../../data/train.json'))).fillna(\"\")\n",
    "test = pd.DataFrame.from_records(json.load(open('../../data/test.json'))).fillna(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Splitting validation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Fitting models\n",
      "INFO:root:Evaluating on validation data\n",
      "INFO:root:Mean baseline MAE: 7.8054390754858805\n",
      "INFO:root:Ridge regress MAE: 5.812345349001838\n"
     ]
    }
   ],
   "source": [
    "# Split the train set into train (75%) and validation (25%) sets\n",
    "logging.info(\"Splitting validation\")\n",
    "train, val = train_test_split(train, stratify=train['year'], random_state=123)\n",
    "    \n",
    "# Store a featurizer to transform the 'title' column into a bag-of-words format\n",
    "featurizer = ColumnTransformer(\n",
    "transformers=[(\"title\", CountVectorizer(), \"title\")], remainder='drop')\n",
    "    \n",
    "# Make a pipeline for the featurizer combined with a dummy regressor, that simply predicts the overall trained mean of the target variable\n",
    "dummy = make_pipeline(featurizer, DummyRegressor(strategy='mean'))\n",
    "\n",
    "# Make a pipeline for the featurizer and a ridge model, that aims to minimize the sum of squares\n",
    "ridge = make_pipeline(featurizer, Ridge())\n",
    "    \n",
    "# Drop target variable column and fit both models\n",
    "logging.info(\"Fitting models\")\n",
    "dummy.fit(train.drop('year', axis=1), train['year'].values)\n",
    "ridge.fit(train.drop('year', axis=1), train['year'].values)\n",
    "    \n",
    "# Calculate and report both MAE's\n",
    "logging.info(\"Evaluating on validation data\")\n",
    "err = mean_absolute_error(val['year'].values, dummy.predict(val.drop('year', axis=1)))\n",
    "logging.info(f\"Mean baseline MAE: {err}\")\n",
    "err = mean_absolute_error(val['year'].values, ridge.predict(val.drop('year', axis=1)))\n",
    "logging.info(f\"Ridge regress MAE: {err}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ENTRYTYPE</th>\n",
       "      <th>title</th>\n",
       "      <th>editor</th>\n",
       "      <th>year</th>\n",
       "      <th>publisher</th>\n",
       "      <th>author</th>\n",
       "      <th>abstract</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2603</th>\n",
       "      <td>inproceedings</td>\n",
       "      <td>Use of Heuristic Knowledge in Chinese Language...</td>\n",
       "      <td>None</td>\n",
       "      <td>1984</td>\n",
       "      <td>Association for Computational Linguistics</td>\n",
       "      <td>[Yang, Yiming, Nishida, Toyoaki, Doshita, Shuji]</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3258</th>\n",
       "      <td>inproceedings</td>\n",
       "      <td>Deciding the Twins Property for Weighted Tree ...</td>\n",
       "      <td>None</td>\n",
       "      <td>2012</td>\n",
       "      <td>Association for Computational Linguistics</td>\n",
       "      <td>[Büchse, Matthias, Fischer, Anja]</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48785</th>\n",
       "      <td>inproceedings</td>\n",
       "      <td>Large Margin Neural Language Model</td>\n",
       "      <td>None</td>\n",
       "      <td>2018</td>\n",
       "      <td>Association for Computational Linguistics</td>\n",
       "      <td>[Huang, Jiaji, Li, Yi, Ping, Wei, Huang, Liang]</td>\n",
       "      <td>We propose a large margin criterion for traini...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8822</th>\n",
       "      <td>inproceedings</td>\n",
       "      <td>Plot-guided Adversarial Example Construction f...</td>\n",
       "      <td>None</td>\n",
       "      <td>2021</td>\n",
       "      <td>Association for Computational Linguistics</td>\n",
       "      <td>[Ghazarian, Sarik, Liu, Zixi, S M, Akash, Weis...</td>\n",
       "      <td>With the recent advances of open-domain story ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24495</th>\n",
       "      <td>inproceedings</td>\n",
       "      <td>Towards a terminological resource for biomedic...</td>\n",
       "      <td>None</td>\n",
       "      <td>2006</td>\n",
       "      <td>European Language Resources Association (ELRA)</td>\n",
       "      <td>[Nenadic, Goran, Okazaki, Naoki, Ananiadou, So...</td>\n",
       "      <td>One of the main challenges in biomedical text ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12457</th>\n",
       "      <td>inproceedings</td>\n",
       "      <td>Annotating Events in an Emotion Corpus</td>\n",
       "      <td>None</td>\n",
       "      <td>2014</td>\n",
       "      <td>European Language Resources Association (ELRA)</td>\n",
       "      <td>[Lee, Sophia, Li, Shoushan, Huang, Chu-Ren]</td>\n",
       "      <td>This paper presents the development of a Chine...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29939</th>\n",
       "      <td>inproceedings</td>\n",
       "      <td>POSTECH Submission on Duolingo Shared Task</td>\n",
       "      <td>None</td>\n",
       "      <td>2020</td>\n",
       "      <td>Association for Computational Linguistics</td>\n",
       "      <td>[Park, Junsu, Kwon, Hongseok, Lee, Jong-Hyeok]</td>\n",
       "      <td>In this paper, we propose a transfer learning ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63102</th>\n",
       "      <td>inproceedings</td>\n",
       "      <td>Annotation Trees: LDC's customizable, extensib...</td>\n",
       "      <td>None</td>\n",
       "      <td>2012</td>\n",
       "      <td>European Language Resources Association (ELRA)</td>\n",
       "      <td>[Wright, Jonathan, Griffitt, Kira, Ellis, Joe,...</td>\n",
       "      <td>In recent months, LDC has developed a web-base...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19430</th>\n",
       "      <td>inproceedings</td>\n",
       "      <td>Long Nights, Rainy Days, and Misspent Youth: A...</td>\n",
       "      <td>None</td>\n",
       "      <td>2015</td>\n",
       "      <td>Association for Computational Linguistics</td>\n",
       "      <td>[Bracewell, David]</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32248</th>\n",
       "      <td>inproceedings</td>\n",
       "      <td>Detecting late-life depression in Alzheimer's ...</td>\n",
       "      <td>None</td>\n",
       "      <td>2016</td>\n",
       "      <td>Association for Computational Linguistics</td>\n",
       "      <td>[Fraser, Kathleen C., Rudzicz, Frank, Hirst, G...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>16479 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           ENTRYTYPE                                              title  \\\n",
       "2603   inproceedings  Use of Heuristic Knowledge in Chinese Language...   \n",
       "3258   inproceedings  Deciding the Twins Property for Weighted Tree ...   \n",
       "48785  inproceedings                 Large Margin Neural Language Model   \n",
       "8822   inproceedings  Plot-guided Adversarial Example Construction f...   \n",
       "24495  inproceedings  Towards a terminological resource for biomedic...   \n",
       "...              ...                                                ...   \n",
       "12457  inproceedings             Annotating Events in an Emotion Corpus   \n",
       "29939  inproceedings         POSTECH Submission on Duolingo Shared Task   \n",
       "63102  inproceedings  Annotation Trees: LDC's customizable, extensib...   \n",
       "19430  inproceedings  Long Nights, Rainy Days, and Misspent Youth: A...   \n",
       "32248  inproceedings  Detecting late-life depression in Alzheimer's ...   \n",
       "\n",
       "      editor  year                                       publisher  \\\n",
       "2603    None  1984       Association for Computational Linguistics   \n",
       "3258    None  2012       Association for Computational Linguistics   \n",
       "48785   None  2018       Association for Computational Linguistics   \n",
       "8822    None  2021       Association for Computational Linguistics   \n",
       "24495   None  2006  European Language Resources Association (ELRA)   \n",
       "...      ...   ...                                             ...   \n",
       "12457   None  2014  European Language Resources Association (ELRA)   \n",
       "29939   None  2020       Association for Computational Linguistics   \n",
       "63102   None  2012  European Language Resources Association (ELRA)   \n",
       "19430   None  2015       Association for Computational Linguistics   \n",
       "32248   None  2016       Association for Computational Linguistics   \n",
       "\n",
       "                                                  author  \\\n",
       "2603    [Yang, Yiming, Nishida, Toyoaki, Doshita, Shuji]   \n",
       "3258                   [Büchse, Matthias, Fischer, Anja]   \n",
       "48785    [Huang, Jiaji, Li, Yi, Ping, Wei, Huang, Liang]   \n",
       "8822   [Ghazarian, Sarik, Liu, Zixi, S M, Akash, Weis...   \n",
       "24495  [Nenadic, Goran, Okazaki, Naoki, Ananiadou, So...   \n",
       "...                                                  ...   \n",
       "12457        [Lee, Sophia, Li, Shoushan, Huang, Chu-Ren]   \n",
       "29939     [Park, Junsu, Kwon, Hongseok, Lee, Jong-Hyeok]   \n",
       "63102  [Wright, Jonathan, Griffitt, Kira, Ellis, Joe,...   \n",
       "19430                                 [Bracewell, David]   \n",
       "32248  [Fraser, Kathleen C., Rudzicz, Frank, Hirst, G...   \n",
       "\n",
       "                                                abstract  \n",
       "2603                                                None  \n",
       "3258                                                None  \n",
       "48785  We propose a large margin criterion for traini...  \n",
       "8822   With the recent advances of open-domain story ...  \n",
       "24495  One of the main challenges in biomedical text ...  \n",
       "...                                                  ...  \n",
       "12457  This paper presents the development of a Chine...  \n",
       "29939  In this paper, we propose a transfer learning ...  \n",
       "63102  In recent months, LDC has developed a web-base...  \n",
       "19430                                               None  \n",
       "32248                                               None  \n",
       "\n",
       "[16479 rows x 7 columns]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check what validation set looks like\n",
    "val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16479\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([2005.16848241, 2016.72621127, 2013.17436633, ..., 2014.96570928,\n",
       "       2014.87896051, 2020.60932351])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check what array with predicted values looks like\n",
    "pred_array = ridge.predict(val.drop('year', axis=1))\n",
    "print(len(pred_array))\n",
    "pred_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16479\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array(['1984', '2012', '2018', ..., '2012', '2015', '2016'], dtype=object)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check what array with true values looks like\n",
    "true_array = val['year'].values\n",
    "print(len(true_array))\n",
    "true_array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Preprocessing (Lars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import extra modules\n",
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ENTRYTYPE</th>\n",
       "      <th>title</th>\n",
       "      <th>editor</th>\n",
       "      <th>year</th>\n",
       "      <th>publisher</th>\n",
       "      <th>author</th>\n",
       "      <th>abstract</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>49187</th>\n",
       "      <td>inproceedings</td>\n",
       "      <td>Learning Bilingual Sentence Embeddings via Aut...</td>\n",
       "      <td>None</td>\n",
       "      <td>2019</td>\n",
       "      <td>Association for Computational Linguistics</td>\n",
       "      <td>[Kim, Yunsu, Rosendahl, Hendrik, Rossenbach, N...</td>\n",
       "      <td>We propose a novel model architecture and trai...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27721</th>\n",
       "      <td>inproceedings</td>\n",
       "      <td>How Could Rhetorical Relations Be Used in Mach...</td>\n",
       "      <td>None</td>\n",
       "      <td>1993</td>\n",
       "      <td>None</td>\n",
       "      <td>[Mitkov, Ruslan]</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28449</th>\n",
       "      <td>inproceedings</td>\n",
       "      <td>The Rhetorical Parsing of Unrestricted Natural...</td>\n",
       "      <td>None</td>\n",
       "      <td>1997</td>\n",
       "      <td>Association for Computational Linguistics</td>\n",
       "      <td>[Marcu, Daniel]</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36059</th>\n",
       "      <td>inproceedings</td>\n",
       "      <td>CoToHiLi at LSCDiscovery: the Role of Linguist...</td>\n",
       "      <td>None</td>\n",
       "      <td>2022</td>\n",
       "      <td>Association for Computational Linguistics</td>\n",
       "      <td>[Sabina Uban, Ana, Maria Cristea, Alina, Danie...</td>\n",
       "      <td>This paper presents the contributions of the C...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35564</th>\n",
       "      <td>inproceedings</td>\n",
       "      <td>Presentation</td>\n",
       "      <td>None</td>\n",
       "      <td>2006</td>\n",
       "      <td>Association for Machine Translation in the Ame...</td>\n",
       "      <td>[Habash, Nizar]</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           ENTRYTYPE                                              title  \\\n",
       "49187  inproceedings  Learning Bilingual Sentence Embeddings via Aut...   \n",
       "27721  inproceedings  How Could Rhetorical Relations Be Used in Mach...   \n",
       "28449  inproceedings  The Rhetorical Parsing of Unrestricted Natural...   \n",
       "36059  inproceedings  CoToHiLi at LSCDiscovery: the Role of Linguist...   \n",
       "35564  inproceedings                                       Presentation   \n",
       "\n",
       "      editor  year                                          publisher  \\\n",
       "49187   None  2019          Association for Computational Linguistics   \n",
       "27721   None  1993                                               None   \n",
       "28449   None  1997          Association for Computational Linguistics   \n",
       "36059   None  2022          Association for Computational Linguistics   \n",
       "35564   None  2006  Association for Machine Translation in the Ame...   \n",
       "\n",
       "                                                  author  \\\n",
       "49187  [Kim, Yunsu, Rosendahl, Hendrik, Rossenbach, N...   \n",
       "27721                                   [Mitkov, Ruslan]   \n",
       "28449                                    [Marcu, Daniel]   \n",
       "36059  [Sabina Uban, Ana, Maria Cristea, Alina, Danie...   \n",
       "35564                                    [Habash, Nizar]   \n",
       "\n",
       "                                                abstract  \n",
       "49187  We propose a novel model architecture and trai...  \n",
       "27721                                               None  \n",
       "28449                                               None  \n",
       "36059  This paper presents the contributions of the C...  \n",
       "35564                                               None  "
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Randomly save 5% of the train set for velocity purposes\n",
    "percentage_to_save = 5\n",
    "\n",
    "# Calculate the number of rows to save\n",
    "num_rows_to_save = int(len(train) * (percentage_to_save / 100))\n",
    "\n",
    "# Use the sample method to randomly select rows\n",
    "train_sample = train.sample(n=num_rows_to_save, random_state=42)  # Set a random_state for reproducibility\n",
    "\n",
    "train_sample.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.1 Drop all columns with over 75% of missing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ENTRYTYPE</th>\n",
       "      <th>title</th>\n",
       "      <th>editor</th>\n",
       "      <th>year</th>\n",
       "      <th>publisher</th>\n",
       "      <th>author</th>\n",
       "      <th>abstract</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>inproceedings</td>\n",
       "      <td>Philippine Language Resources: Trends and Dire...</td>\n",
       "      <td></td>\n",
       "      <td>2009</td>\n",
       "      <td>Association for Computational Linguistics</td>\n",
       "      <td>[Roxas, Rachel Edita, Cheng, Charibeth, Lim, N...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>inproceedings</td>\n",
       "      <td>A System for Translating Locative Prepositions...</td>\n",
       "      <td></td>\n",
       "      <td>1991</td>\n",
       "      <td>Association for Computational Linguistics</td>\n",
       "      <td>[Japkowicz, Nathalie, Wiebe, Janyce M.]</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>inproceedings</td>\n",
       "      <td>Introduction to the Shared Task on Comparing S...</td>\n",
       "      <td></td>\n",
       "      <td>2008</td>\n",
       "      <td>College Publications</td>\n",
       "      <td>[Bos, Johan]</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>inproceedings</td>\n",
       "      <td>Pynini: A Python library for weighted finite-s...</td>\n",
       "      <td></td>\n",
       "      <td>2016</td>\n",
       "      <td>Association for Computational Linguistics</td>\n",
       "      <td>[Gorman, Kyle]</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>inproceedings</td>\n",
       "      <td>Improving Readability of Swedish Electronic He...</td>\n",
       "      <td></td>\n",
       "      <td>2014</td>\n",
       "      <td>Association for Computational Linguistics</td>\n",
       "      <td>[Grigonyte, Gintarė, Kvist, Maria, Velupillai,...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65909</th>\n",
       "      <td>inproceedings</td>\n",
       "      <td>Optimizing the weighted sequence alignment alg...</td>\n",
       "      <td></td>\n",
       "      <td>2022</td>\n",
       "      <td>Association for Computational Linguistics</td>\n",
       "      <td>[Janicki, Maciej]</td>\n",
       "      <td>We present an optimized implementation of the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65910</th>\n",
       "      <td>proceedings</td>\n",
       "      <td>Proceedings of the 25th Conference on Computat...</td>\n",
       "      <td>[Bisazza, Arianna, Abend, Omri]</td>\n",
       "      <td>2021</td>\n",
       "      <td>Association for Computational Linguistics</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65911</th>\n",
       "      <td>article</td>\n",
       "      <td>A Large-Scale Pseudoword-Based Evaluation Fram...</td>\n",
       "      <td></td>\n",
       "      <td>2014</td>\n",
       "      <td>MIT Press</td>\n",
       "      <td>[Pilehvar, Mohammad Taher, Navigli, Roberto]</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65912</th>\n",
       "      <td>inproceedings</td>\n",
       "      <td>CIST System for CL-SciSumm 2016 Shared Task</td>\n",
       "      <td></td>\n",
       "      <td>2016</td>\n",
       "      <td></td>\n",
       "      <td>[Li, Lei, Mao, Liyuan, Zhang, Yazhao, Chi, Jun...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65913</th>\n",
       "      <td>inproceedings</td>\n",
       "      <td>Ontology Engineering and Knowledge Extraction ...</td>\n",
       "      <td></td>\n",
       "      <td>2009</td>\n",
       "      <td>Association for Computational Linguistics</td>\n",
       "      <td>[Trapman, Jantine, Monachesi, Paola]</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>65914 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           ENTRYTYPE                                              title  \\\n",
       "0      inproceedings  Philippine Language Resources: Trends and Dire...   \n",
       "1      inproceedings  A System for Translating Locative Prepositions...   \n",
       "2      inproceedings  Introduction to the Shared Task on Comparing S...   \n",
       "3      inproceedings  Pynini: A Python library for weighted finite-s...   \n",
       "4      inproceedings  Improving Readability of Swedish Electronic He...   \n",
       "...              ...                                                ...   \n",
       "65909  inproceedings  Optimizing the weighted sequence alignment alg...   \n",
       "65910    proceedings  Proceedings of the 25th Conference on Computat...   \n",
       "65911        article  A Large-Scale Pseudoword-Based Evaluation Fram...   \n",
       "65912  inproceedings        CIST System for CL-SciSumm 2016 Shared Task   \n",
       "65913  inproceedings  Ontology Engineering and Knowledge Extraction ...   \n",
       "\n",
       "                                editor  year  \\\n",
       "0                                       2009   \n",
       "1                                       1991   \n",
       "2                                       2008   \n",
       "3                                       2016   \n",
       "4                                       2014   \n",
       "...                                ...   ...   \n",
       "65909                                   2022   \n",
       "65910  [Bisazza, Arianna, Abend, Omri]  2021   \n",
       "65911                                   2014   \n",
       "65912                                   2016   \n",
       "65913                                   2009   \n",
       "\n",
       "                                       publisher  \\\n",
       "0      Association for Computational Linguistics   \n",
       "1      Association for Computational Linguistics   \n",
       "2                           College Publications   \n",
       "3      Association for Computational Linguistics   \n",
       "4      Association for Computational Linguistics   \n",
       "...                                          ...   \n",
       "65909  Association for Computational Linguistics   \n",
       "65910  Association for Computational Linguistics   \n",
       "65911                                  MIT Press   \n",
       "65912                                              \n",
       "65913  Association for Computational Linguistics   \n",
       "\n",
       "                                                  author  \\\n",
       "0      [Roxas, Rachel Edita, Cheng, Charibeth, Lim, N...   \n",
       "1                [Japkowicz, Nathalie, Wiebe, Janyce M.]   \n",
       "2                                           [Bos, Johan]   \n",
       "3                                         [Gorman, Kyle]   \n",
       "4      [Grigonyte, Gintarė, Kvist, Maria, Velupillai,...   \n",
       "...                                                  ...   \n",
       "65909                                  [Janicki, Maciej]   \n",
       "65910                                                      \n",
       "65911       [Pilehvar, Mohammad Taher, Navigli, Roberto]   \n",
       "65912  [Li, Lei, Mao, Liyuan, Zhang, Yazhao, Chi, Jun...   \n",
       "65913               [Trapman, Jantine, Monachesi, Paola]   \n",
       "\n",
       "                                                abstract  \n",
       "0                                                         \n",
       "1                                                         \n",
       "2                                                         \n",
       "3                                                         \n",
       "4                                                         \n",
       "...                                                  ...  \n",
       "65909  We present an optimized implementation of the ...  \n",
       "65910                                                     \n",
       "65911                                                     \n",
       "65912                                                     \n",
       "65913                                                     \n",
       "\n",
       "[65914 rows x 7 columns]"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set threshold on 75%\n",
    "threshold = 0.25\n",
    "\n",
    "# Calculate the threshold for each column\n",
    "missing_threshold = int(threshold * len(train))\n",
    "\n",
    "# Drop columns with more than the specified percentage of missing data\n",
    "train_filtered = train.dropna(axis=1, thresh=missing_threshold)\n",
    "\n",
    "train_filtered"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.2 Vectorize 'author' column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Gebruiker\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>abdul-mageed, muhammad</th>\n",
       "      <th>agirre, eneko</th>\n",
       "      <th>anastasopoulos, antonios</th>\n",
       "      <th>antoine, jean-yves</th>\n",
       "      <th>baldwin, timothy</th>\n",
       "      <th>besacier, laurent</th>\n",
       "      <th>bethard, steven</th>\n",
       "      <th>bhattacharyya, pushpak</th>\n",
       "      <th>callison-burch, chris</th>\n",
       "      <th>...</th>\n",
       "      <th>wen, ji-rong</th>\n",
       "      <th>wiebe, janyce</th>\n",
       "      <th>xiong, deyi</th>\n",
       "      <th>yvon, françois</th>\n",
       "      <th>zhang, min</th>\n",
       "      <th>zhang, yue</th>\n",
       "      <th>zhao, hai</th>\n",
       "      <th>zhao, jun</th>\n",
       "      <th>zhou, ming</th>\n",
       "      <th>øvrelid, lilja</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2466</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2467</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2468</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2469</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2470</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2471 rows × 101 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         abdul-mageed, muhammad  agirre, eneko  anastasopoulos, antonios  \\\n",
       "0     1                       0              0                         0   \n",
       "1     1                       0              0                         0   \n",
       "2     1                       0              0                         0   \n",
       "3     1                       0              0                         0   \n",
       "4     0                       0              0                         0   \n",
       "...  ..                     ...            ...                       ...   \n",
       "2466  1                       0              0                         0   \n",
       "2467  1                       0              0                         0   \n",
       "2468  1                       0              0                         0   \n",
       "2469  1                       0              0                         0   \n",
       "2470  1                       0              0                         0   \n",
       "\n",
       "      antoine, jean-yves  baldwin, timothy  besacier, laurent  \\\n",
       "0                      0                 0                  0   \n",
       "1                      0                 0                  0   \n",
       "2                      0                 0                  0   \n",
       "3                      0                 0                  0   \n",
       "4                      0                 0                  0   \n",
       "...                  ...               ...                ...   \n",
       "2466                   0                 0                  0   \n",
       "2467                   0                 0                  0   \n",
       "2468                   0                 0                  0   \n",
       "2469                   0                 0                  0   \n",
       "2470                   0                 0                  0   \n",
       "\n",
       "      bethard, steven  bhattacharyya, pushpak  callison-burch, chris  ...  \\\n",
       "0                   0                       0                      0  ...   \n",
       "1                   0                       0                      0  ...   \n",
       "2                   0                       0                      0  ...   \n",
       "3                   0                       0                      0  ...   \n",
       "4                   0                       0                      0  ...   \n",
       "...               ...                     ...                    ...  ...   \n",
       "2466                0                       0                      0  ...   \n",
       "2467                0                       0                      0  ...   \n",
       "2468                0                       0                      0  ...   \n",
       "2469                0                       0                      0  ...   \n",
       "2470                0                       0                      0  ...   \n",
       "\n",
       "      wen, ji-rong  wiebe, janyce  xiong, deyi  yvon, françois  zhang, min  \\\n",
       "0                0              0            0               0           0   \n",
       "1                0              0            0               0           0   \n",
       "2                0              0            0               0           0   \n",
       "3                0              0            0               0           0   \n",
       "4                0              0            0               0           0   \n",
       "...            ...            ...          ...             ...         ...   \n",
       "2466             0              0            0               0           0   \n",
       "2467             0              0            0               0           0   \n",
       "2468             0              0            0               0           0   \n",
       "2469             0              0            0               0           0   \n",
       "2470             0              0            0               0           0   \n",
       "\n",
       "      zhang, yue  zhao, hai  zhao, jun  zhou, ming  øvrelid, lilja  \n",
       "0              0          0          0           0               0  \n",
       "1              0          0          0           0               0  \n",
       "2              0          0          0           0               0  \n",
       "3              0          0          0           0               0  \n",
       "4              0          0          0           0               0  \n",
       "...          ...        ...        ...         ...             ...  \n",
       "2466           0          0          0           0               0  \n",
       "2467           0          0          0           0               0  \n",
       "2468           0          0          0           0               0  \n",
       "2469           0          0          0           0               0  \n",
       "2470           0          0          0           0               0  \n",
       "\n",
       "[2471 rows x 101 columns]"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert lists of strings, accounting for None values\n",
    "train_sample_filtered['author_str'] = train_sample_filtered['author'].apply(lambda x: ';'.join(map(str, x)) if x is not None else 'unknown_author')\n",
    "\n",
    "# Count the number of papers for each author\n",
    "author_paper_counts = train_sample_filtered['author_str'].str.split(';').explode().value_counts()\n",
    "\n",
    "# Set the number of most frequent authors you want to include\n",
    "X = 100  # Adjust this value to the desired number of most frequent authors\n",
    "\n",
    "# Filter authors based on the X most frequent authors\n",
    "top_authors = author_paper_counts.head(X).index.tolist()\n",
    "\n",
    "# Filter only the top authors in 'author_str'\n",
    "train_sample_filtered['author_str_filtered'] = train_sample_filtered['author_str'].apply(lambda x: ';'.join([author for author in x.split(';') if author in top_authors]))\n",
    "\n",
    "# Count-vectorize 'author_str_filtered'\n",
    "count_vectorizer = CountVectorizer(tokenizer=lambda x: x.split(';'))\n",
    "count_matrix = count_vectorizer.fit_transform(train_sample_filtered['author_str_filtered'])\n",
    "\n",
    "# Extract and create columns\n",
    "feature_names = count_vectorizer.get_feature_names_out()\n",
    "count_df = pd.DataFrame(count_matrix.toarray(), columns=feature_names)\n",
    "\n",
    "# Display the resulting DataFrame\n",
    "count_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We've transformed the 'author' column to a dataframe of 6263 columns.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Gebruiker\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>-, mausam</th>\n",
       "      <th>aakhus, mark</th>\n",
       "      <th>abad, alberto</th>\n",
       "      <th>abadi, david</th>\n",
       "      <th>abascal, julio g.</th>\n",
       "      <th>abate, solomon teferra</th>\n",
       "      <th>abbas, mourad</th>\n",
       "      <th>abdelali, ahmed</th>\n",
       "      <th>abdelghaffar, mohamed</th>\n",
       "      <th>abdelghaffar, mohamed a</th>\n",
       "      <th>...</th>\n",
       "      <th>øvrelid, lilja</th>\n",
       "      <th>üksik, tiiu</th>\n",
       "      <th>čmejrek, martin</th>\n",
       "      <th>šarkutė, ligita</th>\n",
       "      <th>šimon, petr</th>\n",
       "      <th>šnajder, jan</th>\n",
       "      <th>šojat, krešimir</th>\n",
       "      <th>šuster, simon</th>\n",
       "      <th>žabokrtský, zdeněk</th>\n",
       "      <th>žganec gros, jerneja</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2466</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2467</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2468</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2469</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2470</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2471 rows × 6263 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      -, mausam  aakhus, mark  abad, alberto  abadi, david  abascal, julio g.  \\\n",
       "0             0             0              0             0                  0   \n",
       "1             0             0              0             0                  0   \n",
       "2             0             0              0             0                  0   \n",
       "3             0             0              0             0                  0   \n",
       "4             0             0              0             0                  0   \n",
       "...         ...           ...            ...           ...                ...   \n",
       "2466          0             0              0             0                  0   \n",
       "2467          0             0              0             0                  0   \n",
       "2468          0             0              0             0                  0   \n",
       "2469          0             0              0             0                  0   \n",
       "2470          0             0              0             0                  0   \n",
       "\n",
       "      abate, solomon teferra  abbas, mourad  abdelali, ahmed  \\\n",
       "0                          0              0                0   \n",
       "1                          0              0                0   \n",
       "2                          0              0                0   \n",
       "3                          0              0                0   \n",
       "4                          0              0                0   \n",
       "...                      ...            ...              ...   \n",
       "2466                       0              0                0   \n",
       "2467                       0              0                0   \n",
       "2468                       0              0                0   \n",
       "2469                       0              0                0   \n",
       "2470                       0              0                0   \n",
       "\n",
       "      abdelghaffar, mohamed  abdelghaffar, mohamed a   ...  øvrelid, lilja  \\\n",
       "0                         0                         0  ...               0   \n",
       "1                         0                         0  ...               0   \n",
       "2                         0                         0  ...               0   \n",
       "3                         0                         0  ...               0   \n",
       "4                         0                         0  ...               0   \n",
       "...                     ...                       ...  ...             ...   \n",
       "2466                      0                         0  ...               0   \n",
       "2467                      0                         0  ...               0   \n",
       "2468                      0                         0  ...               0   \n",
       "2469                      0                         0  ...               0   \n",
       "2470                      0                         0  ...               0   \n",
       "\n",
       "      üksik, tiiu  čmejrek, martin  šarkutė, ligita  šimon, petr  \\\n",
       "0               0                0                0            0   \n",
       "1               0                0                0            0   \n",
       "2               0                0                0            0   \n",
       "3               0                0                0            0   \n",
       "4               0                0                0            0   \n",
       "...           ...              ...              ...          ...   \n",
       "2466            0                0                0            0   \n",
       "2467            0                0                0            0   \n",
       "2468            0                0                0            0   \n",
       "2469            0                0                0            0   \n",
       "2470            0                0                0            0   \n",
       "\n",
       "      šnajder, jan  šojat, krešimir  šuster, simon  žabokrtský, zdeněk  \\\n",
       "0                0                0              0                   0   \n",
       "1                0                0              0                   0   \n",
       "2                0                0              0                   0   \n",
       "3                0                0              0                   0   \n",
       "4                0                0              0                   0   \n",
       "...            ...              ...            ...                 ...   \n",
       "2466             0                0              0                   0   \n",
       "2467             0                0              0                   0   \n",
       "2468             0                0              0                   0   \n",
       "2469             0                0              0                   0   \n",
       "2470             0                0              0                   0   \n",
       "\n",
       "      žganec gros, jerneja  \n",
       "0                        0  \n",
       "1                        0  \n",
       "2                        0  \n",
       "3                        0  \n",
       "4                        0  \n",
       "...                    ...  \n",
       "2466                     0  \n",
       "2467                     0  \n",
       "2468                     0  \n",
       "2469                     0  \n",
       "2470                     0  \n",
       "\n",
       "[2471 rows x 6263 columns]"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert lists of strings, accounting for None values\n",
    "train_sample_filtered['author_str'] = train_sample_filtered['author'].apply(lambda x: ';'.join(map(str, x)) if x is not None else 'unknown_author')\n",
    "\n",
    "# Count-vectorize author_str\n",
    "count_vectorizer = CountVectorizer(tokenizer=lambda x: x.split(';'))\n",
    "count_matrix = count_vectorizer.fit_transform(train_sample_filtered['author_str'])\n",
    "\n",
    "# Extract and create columns\n",
    "feature_names = count_vectorizer.get_feature_names_out()\n",
    "count_df = pd.DataFrame(count_matrix.toarray(), columns=feature_names)\n",
    "\n",
    "print(f\"We've transformed the 'author' column to a dataframe of {len(count_df.columns)} columns.\")\n",
    "count_df\n",
    "\n",
    "# To reduce dimensionalities here, we could see if we could remove all authors that only have 1 paper in the set\n",
    "# Dimensionalities have to be reduced, doesn't fit inside dataframe\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.3 Vectorize 'title' column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langdetect import detect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sample_filtered['language'] = train_sample_filtered['title'].apply(lambda x: detect(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "en    2358\n",
       "fr      80\n",
       "it       6\n",
       "de       6\n",
       "ca       4\n",
       "ro       4\n",
       "es       2\n",
       "da       2\n",
       "tl       2\n",
       "pt       2\n",
       "nl       2\n",
       "sw       1\n",
       "sv       1\n",
       "af       1\n",
       "Name: language, dtype: int64"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_sample_filtered['language'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We've transformed the 'author' column to a dataframe of 4820 columns.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>05</th>\n",
       "      <th>08</th>\n",
       "      <th>10</th>\n",
       "      <th>10th</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>14</th>\n",
       "      <th>14th</th>\n",
       "      <th>14ème</th>\n",
       "      <th>15</th>\n",
       "      <th>...</th>\n",
       "      <th>基于bilstm</th>\n",
       "      <th>基于语料库的形容词性别偏度历时研究</th>\n",
       "      <th>多模态表述视域下的小学数学课堂语言计量初探</th>\n",
       "      <th>多語語碼轉換之未知詞擷取</th>\n",
       "      <th>大規模詞彙語意關係自動標示之初步研究</th>\n",
       "      <th>字里行间的道德</th>\n",
       "      <th>為例</th>\n",
       "      <th>融合多层语义特征图的缅甸语图像文本识别方法</th>\n",
       "      <th>調變頻譜正規化法使用於強健語音辨識之研究</th>\n",
       "      <th>非監督式學習於中文電視新聞自動轉寫之初步應用</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2466</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2467</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2468</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2469</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2470</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2471 rows × 4820 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       05   08   10  10th   11   12   14  14th  14ème   15  ...  基于bilstm  \\\n",
       "0     0.0  0.0  0.0   0.0  0.0  0.0  0.0   0.0    0.0  0.0  ...       0.0   \n",
       "1     0.0  0.0  0.0   0.0  0.0  0.0  0.0   0.0    0.0  0.0  ...       0.0   \n",
       "2     0.0  0.0  0.0   0.0  0.0  0.0  0.0   0.0    0.0  0.0  ...       0.0   \n",
       "3     0.0  0.0  0.0   0.0  0.0  0.0  0.0   0.0    0.0  0.0  ...       0.0   \n",
       "4     0.0  0.0  0.0   0.0  0.0  0.0  0.0   0.0    0.0  0.0  ...       0.0   \n",
       "...   ...  ...  ...   ...  ...  ...  ...   ...    ...  ...  ...       ...   \n",
       "2466  0.0  0.0  0.0   0.0  0.0  0.0  0.0   0.0    0.0  0.0  ...       0.0   \n",
       "2467  0.0  0.0  0.0   0.0  0.0  0.0  0.0   0.0    0.0  0.0  ...       0.0   \n",
       "2468  0.0  0.0  0.0   0.0  0.0  0.0  0.0   0.0    0.0  0.0  ...       0.0   \n",
       "2469  0.0  0.0  0.0   0.0  0.0  0.0  0.0   0.0    0.0  0.0  ...       0.0   \n",
       "2470  0.0  0.0  0.0   0.0  0.0  0.0  0.0   0.0    0.0  0.0  ...       0.0   \n",
       "\n",
       "      基于语料库的形容词性别偏度历时研究  多模态表述视域下的小学数学课堂语言计量初探  多語語碼轉換之未知詞擷取  \\\n",
       "0                   0.0                    0.0           0.0   \n",
       "1                   0.0                    0.0           0.0   \n",
       "2                   0.0                    0.0           0.0   \n",
       "3                   0.0                    0.0           0.0   \n",
       "4                   0.0                    0.0           0.0   \n",
       "...                 ...                    ...           ...   \n",
       "2466                0.0                    0.0           0.0   \n",
       "2467                0.0                    0.0           0.0   \n",
       "2468                0.0                    0.0           0.0   \n",
       "2469                0.0                    0.0           0.0   \n",
       "2470                0.0                    0.0           0.0   \n",
       "\n",
       "      大規模詞彙語意關係自動標示之初步研究  字里行间的道德   為例  融合多层语义特征图的缅甸语图像文本识别方法  \\\n",
       "0                    0.0      0.0  0.0                    0.0   \n",
       "1                    0.0      0.0  0.0                    0.0   \n",
       "2                    0.0      0.0  0.0                    0.0   \n",
       "3                    0.0      0.0  0.0                    0.0   \n",
       "4                    0.0      0.0  0.0                    0.0   \n",
       "...                  ...      ...  ...                    ...   \n",
       "2466                 0.0      0.0  0.0                    0.0   \n",
       "2467                 0.0      0.0  0.0                    0.0   \n",
       "2468                 0.0      0.0  0.0                    0.0   \n",
       "2469                 0.0      0.0  0.0                    0.0   \n",
       "2470                 0.0      0.0  0.0                    0.0   \n",
       "\n",
       "      調變頻譜正規化法使用於強健語音辨識之研究  非監督式學習於中文電視新聞自動轉寫之初步應用  \n",
       "0                      0.0                     0.0  \n",
       "1                      0.0                     0.0  \n",
       "2                      0.0                     0.0  \n",
       "3                      0.0                     0.0  \n",
       "4                      0.0                     0.0  \n",
       "...                    ...                     ...  \n",
       "2466                   0.0                     0.0  \n",
       "2467                   0.0                     0.0  \n",
       "2468                   0.0                     0.0  \n",
       "2469                   0.0                     0.0  \n",
       "2470                   0.0                     0.0  \n",
       "\n",
       "[2471 rows x 4820 columns]"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Apply the TF-IDF vectorizer to column 'title'\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(train_sample_filtered['title'])\n",
    "\n",
    "# Extract and create columns\n",
    "feature_names = tfidf_vectorizer.get_feature_names_out()\n",
    "tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=feature_names)\n",
    "\n",
    "print(f\"We've transformed the 'author' column to a dataframe of {len(tfidf_df.columns)} columns.\")\n",
    "tfidf_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Feature selection/extraction (Lars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ENTRYTYPE</th>\n",
       "      <th>title</th>\n",
       "      <th>year</th>\n",
       "      <th>publisher</th>\n",
       "      <th>abstract</th>\n",
       "      <th>-, Mausam</th>\n",
       "      <th>Aakhus, Mark</th>\n",
       "      <th>Abad, Alberto</th>\n",
       "      <th>Abadi, David</th>\n",
       "      <th>Abascal, Julio G.</th>\n",
       "      <th>...</th>\n",
       "      <th>Øvrelid, Lilja</th>\n",
       "      <th>Üksik, Tiiu</th>\n",
       "      <th>Čmejrek, Martin</th>\n",
       "      <th>Šarkutė, Ligita</th>\n",
       "      <th>Šimon, Petr</th>\n",
       "      <th>Šnajder, Jan</th>\n",
       "      <th>Šojat, Krešimir</th>\n",
       "      <th>Šuster, Simon</th>\n",
       "      <th>Žabokrtský, Zdeněk</th>\n",
       "      <th>Žganec Gros, Jerneja</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>49187</th>\n",
       "      <td>inproceedings</td>\n",
       "      <td>Learning Bilingual Sentence Embeddings via Aut...</td>\n",
       "      <td>2019</td>\n",
       "      <td>Association for Computational Linguistics</td>\n",
       "      <td>We propose a novel model architecture and trai...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27721</th>\n",
       "      <td>inproceedings</td>\n",
       "      <td>How Could Rhetorical Relations Be Used in Mach...</td>\n",
       "      <td>1993</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28449</th>\n",
       "      <td>inproceedings</td>\n",
       "      <td>The Rhetorical Parsing of Unrestricted Natural...</td>\n",
       "      <td>1997</td>\n",
       "      <td>Association for Computational Linguistics</td>\n",
       "      <td></td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36059</th>\n",
       "      <td>inproceedings</td>\n",
       "      <td>CoToHiLi at LSCDiscovery: the Role of Linguist...</td>\n",
       "      <td>2022</td>\n",
       "      <td>Association for Computational Linguistics</td>\n",
       "      <td>This paper presents the contributions of the C...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35564</th>\n",
       "      <td>inproceedings</td>\n",
       "      <td>Presentation</td>\n",
       "      <td>2006</td>\n",
       "      <td>Association for Machine Translation in the Ame...</td>\n",
       "      <td></td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 6269 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           ENTRYTYPE                                              title  year  \\\n",
       "49187  inproceedings  Learning Bilingual Sentence Embeddings via Aut...  2019   \n",
       "27721  inproceedings  How Could Rhetorical Relations Be Used in Mach...  1993   \n",
       "28449  inproceedings  The Rhetorical Parsing of Unrestricted Natural...  1997   \n",
       "36059  inproceedings  CoToHiLi at LSCDiscovery: the Role of Linguist...  2022   \n",
       "35564  inproceedings                                       Presentation  2006   \n",
       "\n",
       "                                               publisher  \\\n",
       "49187          Association for Computational Linguistics   \n",
       "27721                                                      \n",
       "28449          Association for Computational Linguistics   \n",
       "36059          Association for Computational Linguistics   \n",
       "35564  Association for Machine Translation in the Ame...   \n",
       "\n",
       "                                                abstract  -, Mausam  \\\n",
       "49187  We propose a novel model architecture and trai...        NaN   \n",
       "27721                                                           NaN   \n",
       "28449                                                           NaN   \n",
       "36059  This paper presents the contributions of the C...        NaN   \n",
       "35564                                                           NaN   \n",
       "\n",
       "       Aakhus, Mark  Abad, Alberto  Abadi, David  Abascal, Julio G.  ...  \\\n",
       "49187           NaN            NaN           NaN                NaN  ...   \n",
       "27721           NaN            NaN           NaN                NaN  ...   \n",
       "28449           NaN            NaN           NaN                NaN  ...   \n",
       "36059           NaN            NaN           NaN                NaN  ...   \n",
       "35564           NaN            NaN           NaN                NaN  ...   \n",
       "\n",
       "       Øvrelid, Lilja  Üksik, Tiiu  Čmejrek, Martin  Šarkutė, Ligita  \\\n",
       "49187             NaN          NaN              NaN              NaN   \n",
       "27721             NaN          NaN              NaN              NaN   \n",
       "28449             NaN          NaN              NaN              NaN   \n",
       "36059             NaN          NaN              NaN              NaN   \n",
       "35564             NaN          NaN              NaN              NaN   \n",
       "\n",
       "       Šimon, Petr  Šnajder, Jan  Šojat, Krešimir  Šuster, Simon  \\\n",
       "49187          NaN           NaN              NaN            NaN   \n",
       "27721          NaN           NaN              NaN            NaN   \n",
       "28449          NaN           NaN              NaN            NaN   \n",
       "36059          NaN           NaN              NaN            NaN   \n",
       "35564          NaN           NaN              NaN            NaN   \n",
       "\n",
       "       Žabokrtský, Zdeněk  Žganec Gros, Jerneja  \n",
       "49187                 NaN                   NaN  \n",
       "27721                 NaN                   NaN  \n",
       "28449                 NaN                   NaN  \n",
       "36059                 NaN                   NaN  \n",
       "35564                 NaN                   NaN  \n",
       "\n",
       "[5 rows x 6269 columns]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_sample_filtered_author.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Loading training/test data\n",
      "INFO:root:Splitting validation\n",
      "INFO:root:Fitting model with featurizer 1\n",
      "INFO:root:Evaluating on validation data\n",
      "INFO:root:Ridge regress MAE with featurizer 1 (5-fold cross-validated): 5.773010450586702\n",
      "INFO:root:Fitting model with featurizer 2\n",
      "INFO:root:Evaluating on validation data\n",
      "INFO:root:Ridge regress MAE with featurizer 2 (5-fold cross-validated): 5.384430333156983\n",
      "INFO:root:Fitting model with featurizer 3\n",
      "INFO:root:Evaluating on validation data\n",
      "INFO:root:Ridge regress MAE with featurizer 3 (5-fold cross-validated): 6.340921782179531\n",
      "INFO:root:Fitting model with featurizer 4\n",
      "INFO:root:Evaluating on validation data\n",
      "INFO:root:Ridge regress MAE with featurizer 4 (5-fold cross-validated): 5.480748043346883\n",
      "INFO:root:Fitting model with featurizer 5\n"
     ]
    },
    {
     "ename": "NotFittedError",
     "evalue": "This MultiLabelBinarizer instance is not fitted yet. Call 'fit' with appropriate arguments before using this estimator.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNotFittedError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Gebruiker\\MLChallenge\\notebooks\\Experiments.ipynb Cell 16\u001b[0m line \u001b[0;36m4\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Gebruiker/MLChallenge/notebooks/Experiments.ipynb#X21sZmlsZQ%3D%3D?line=41'>42</a>\u001b[0m \u001b[39m# Drop target variable column and fit both models\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Gebruiker/MLChallenge/notebooks/Experiments.ipynb#X21sZmlsZQ%3D%3D?line=42'>43</a>\u001b[0m logging\u001b[39m.\u001b[39minfo(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mFitting model with featurizer \u001b[39m\u001b[39m{\u001b[39;00mi\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Gebruiker/MLChallenge/notebooks/Experiments.ipynb#X21sZmlsZQ%3D%3D?line=43'>44</a>\u001b[0m ridge_cv\u001b[39m.\u001b[39;49mfit(train\u001b[39m.\u001b[39;49mdrop(\u001b[39m'\u001b[39;49m\u001b[39myear\u001b[39;49m\u001b[39m'\u001b[39;49m, axis\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m), train[\u001b[39m'\u001b[39;49m\u001b[39myear\u001b[39;49m\u001b[39m'\u001b[39;49m]\u001b[39m.\u001b[39;49mvalues)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Gebruiker/MLChallenge/notebooks/Experiments.ipynb#X21sZmlsZQ%3D%3D?line=45'>46</a>\u001b[0m \u001b[39m# Calculate and report both MAE's\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Gebruiker/MLChallenge/notebooks/Experiments.ipynb#X21sZmlsZQ%3D%3D?line=46'>47</a>\u001b[0m logging\u001b[39m.\u001b[39minfo(\u001b[39m\"\u001b[39m\u001b[39mEvaluating on validation data\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Gebruiker\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\base.py:1151\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1144\u001b[0m     estimator\u001b[39m.\u001b[39m_validate_params()\n\u001b[0;32m   1146\u001b[0m \u001b[39mwith\u001b[39;00m config_context(\n\u001b[0;32m   1147\u001b[0m     skip_parameter_validation\u001b[39m=\u001b[39m(\n\u001b[0;32m   1148\u001b[0m         prefer_skip_nested_validation \u001b[39mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1149\u001b[0m     )\n\u001b[0;32m   1150\u001b[0m ):\n\u001b[1;32m-> 1151\u001b[0m     \u001b[39mreturn\u001b[39;00m fit_method(estimator, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Gebruiker\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\pipeline.py:416\u001b[0m, in \u001b[0;36mPipeline.fit\u001b[1;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[0;32m    390\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Fit the model.\u001b[39;00m\n\u001b[0;32m    391\u001b[0m \n\u001b[0;32m    392\u001b[0m \u001b[39mFit all the transformers one after the other and transform the\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    413\u001b[0m \u001b[39m    Pipeline with fitted steps.\u001b[39;00m\n\u001b[0;32m    414\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    415\u001b[0m fit_params_steps \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_fit_params(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfit_params)\n\u001b[1;32m--> 416\u001b[0m Xt \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fit(X, y, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mfit_params_steps)\n\u001b[0;32m    417\u001b[0m \u001b[39mwith\u001b[39;00m _print_elapsed_time(\u001b[39m\"\u001b[39m\u001b[39mPipeline\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_log_message(\u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msteps) \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m)):\n\u001b[0;32m    418\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_final_estimator \u001b[39m!=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mpassthrough\u001b[39m\u001b[39m\"\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\Gebruiker\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\pipeline.py:370\u001b[0m, in \u001b[0;36mPipeline._fit\u001b[1;34m(self, X, y, **fit_params_steps)\u001b[0m\n\u001b[0;32m    368\u001b[0m     cloned_transformer \u001b[39m=\u001b[39m clone(transformer)\n\u001b[0;32m    369\u001b[0m \u001b[39m# Fit or load from cache the current transformer\u001b[39;00m\n\u001b[1;32m--> 370\u001b[0m X, fitted_transformer \u001b[39m=\u001b[39m fit_transform_one_cached(\n\u001b[0;32m    371\u001b[0m     cloned_transformer,\n\u001b[0;32m    372\u001b[0m     X,\n\u001b[0;32m    373\u001b[0m     y,\n\u001b[0;32m    374\u001b[0m     \u001b[39mNone\u001b[39;49;00m,\n\u001b[0;32m    375\u001b[0m     message_clsname\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mPipeline\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m    376\u001b[0m     message\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_log_message(step_idx),\n\u001b[0;32m    377\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mfit_params_steps[name],\n\u001b[0;32m    378\u001b[0m )\n\u001b[0;32m    379\u001b[0m \u001b[39m# Replace the transformer of the step with the fitted\u001b[39;00m\n\u001b[0;32m    380\u001b[0m \u001b[39m# transformer. This is necessary when loading the transformer\u001b[39;00m\n\u001b[0;32m    381\u001b[0m \u001b[39m# from the cache.\u001b[39;00m\n\u001b[0;32m    382\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msteps[step_idx] \u001b[39m=\u001b[39m (name, fitted_transformer)\n",
      "File \u001b[1;32mc:\\Users\\Gebruiker\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\joblib\\memory.py:353\u001b[0m, in \u001b[0;36mNotMemorizedFunc.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    352\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m--> 353\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfunc(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Gebruiker\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\pipeline.py:950\u001b[0m, in \u001b[0;36m_fit_transform_one\u001b[1;34m(transformer, X, y, weight, message_clsname, message, **fit_params)\u001b[0m\n\u001b[0;32m    948\u001b[0m \u001b[39mwith\u001b[39;00m _print_elapsed_time(message_clsname, message):\n\u001b[0;32m    949\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(transformer, \u001b[39m\"\u001b[39m\u001b[39mfit_transform\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m--> 950\u001b[0m         res \u001b[39m=\u001b[39m transformer\u001b[39m.\u001b[39;49mfit_transform(X, y, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mfit_params)\n\u001b[0;32m    951\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    952\u001b[0m         res \u001b[39m=\u001b[39m transformer\u001b[39m.\u001b[39mfit(X, y, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfit_params)\u001b[39m.\u001b[39mtransform(X)\n",
      "File \u001b[1;32mc:\\Users\\Gebruiker\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\_set_output.py:140\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[1;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[0;32m    138\u001b[0m \u001b[39m@wraps\u001b[39m(f)\n\u001b[0;32m    139\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrapped\u001b[39m(\u001b[39mself\u001b[39m, X, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m--> 140\u001b[0m     data_to_wrap \u001b[39m=\u001b[39m f(\u001b[39mself\u001b[39;49m, X, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m    141\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(data_to_wrap, \u001b[39mtuple\u001b[39m):\n\u001b[0;32m    142\u001b[0m         \u001b[39m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[0;32m    143\u001b[0m         return_tuple \u001b[39m=\u001b[39m (\n\u001b[0;32m    144\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[39m0\u001b[39m], X, \u001b[39mself\u001b[39m),\n\u001b[0;32m    145\u001b[0m             \u001b[39m*\u001b[39mdata_to_wrap[\u001b[39m1\u001b[39m:],\n\u001b[0;32m    146\u001b[0m         )\n",
      "File \u001b[1;32mc:\\Users\\Gebruiker\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\base.py:1151\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1144\u001b[0m     estimator\u001b[39m.\u001b[39m_validate_params()\n\u001b[0;32m   1146\u001b[0m \u001b[39mwith\u001b[39;00m config_context(\n\u001b[0;32m   1147\u001b[0m     skip_parameter_validation\u001b[39m=\u001b[39m(\n\u001b[0;32m   1148\u001b[0m         prefer_skip_nested_validation \u001b[39mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1149\u001b[0m     )\n\u001b[0;32m   1150\u001b[0m ):\n\u001b[1;32m-> 1151\u001b[0m     \u001b[39mreturn\u001b[39;00m fit_method(estimator, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Gebruiker\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\compose\\_column_transformer.py:743\u001b[0m, in \u001b[0;36mColumnTransformer.fit_transform\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    740\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_validate_column_callables(X)\n\u001b[0;32m    741\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_validate_remainder(X)\n\u001b[1;32m--> 743\u001b[0m result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fit_transform(X, y, _fit_transform_one)\n\u001b[0;32m    745\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m result:\n\u001b[0;32m    746\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_update_fitted_transformers([])\n",
      "File \u001b[1;32mc:\\Users\\Gebruiker\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\compose\\_column_transformer.py:670\u001b[0m, in \u001b[0;36mColumnTransformer._fit_transform\u001b[1;34m(self, X, y, func, fitted, column_as_strings)\u001b[0m\n\u001b[0;32m    664\u001b[0m transformers \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(\n\u001b[0;32m    665\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_iter(\n\u001b[0;32m    666\u001b[0m         fitted\u001b[39m=\u001b[39mfitted, replace_strings\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, column_as_strings\u001b[39m=\u001b[39mcolumn_as_strings\n\u001b[0;32m    667\u001b[0m     )\n\u001b[0;32m    668\u001b[0m )\n\u001b[0;32m    669\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 670\u001b[0m     \u001b[39mreturn\u001b[39;00m Parallel(n_jobs\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mn_jobs)(\n\u001b[0;32m    671\u001b[0m         delayed(func)(\n\u001b[0;32m    672\u001b[0m             transformer\u001b[39m=\u001b[39;49mclone(trans) \u001b[39mif\u001b[39;49;00m \u001b[39mnot\u001b[39;49;00m fitted \u001b[39melse\u001b[39;49;00m trans,\n\u001b[0;32m    673\u001b[0m             X\u001b[39m=\u001b[39;49m_safe_indexing(X, column, axis\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m),\n\u001b[0;32m    674\u001b[0m             y\u001b[39m=\u001b[39;49my,\n\u001b[0;32m    675\u001b[0m             weight\u001b[39m=\u001b[39;49mweight,\n\u001b[0;32m    676\u001b[0m             message_clsname\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mColumnTransformer\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m    677\u001b[0m             message\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_log_message(name, idx, \u001b[39mlen\u001b[39;49m(transformers)),\n\u001b[0;32m    678\u001b[0m         )\n\u001b[0;32m    679\u001b[0m         \u001b[39mfor\u001b[39;49;00m idx, (name, trans, column, weight) \u001b[39min\u001b[39;49;00m \u001b[39menumerate\u001b[39;49m(transformers, \u001b[39m1\u001b[39;49m)\n\u001b[0;32m    680\u001b[0m     )\n\u001b[0;32m    681\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mValueError\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    682\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mExpected 2D array, got 1D array instead\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m \u001b[39mstr\u001b[39m(e):\n",
      "File \u001b[1;32mc:\\Users\\Gebruiker\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\parallel.py:65\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m     60\u001b[0m config \u001b[39m=\u001b[39m get_config()\n\u001b[0;32m     61\u001b[0m iterable_with_config \u001b[39m=\u001b[39m (\n\u001b[0;32m     62\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[0;32m     63\u001b[0m     \u001b[39mfor\u001b[39;00m delayed_func, args, kwargs \u001b[39min\u001b[39;00m iterable\n\u001b[0;32m     64\u001b[0m )\n\u001b[1;32m---> 65\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__call__\u001b[39;49m(iterable_with_config)\n",
      "File \u001b[1;32mc:\\Users\\Gebruiker\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\joblib\\parallel.py:1863\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1861\u001b[0m     output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_sequential_output(iterable)\n\u001b[0;32m   1862\u001b[0m     \u001b[39mnext\u001b[39m(output)\n\u001b[1;32m-> 1863\u001b[0m     \u001b[39mreturn\u001b[39;00m output \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreturn_generator \u001b[39melse\u001b[39;00m \u001b[39mlist\u001b[39m(output)\n\u001b[0;32m   1865\u001b[0m \u001b[39m# Let's create an ID that uniquely identifies the current call. If the\u001b[39;00m\n\u001b[0;32m   1866\u001b[0m \u001b[39m# call is interrupted early and that the same instance is immediately\u001b[39;00m\n\u001b[0;32m   1867\u001b[0m \u001b[39m# re-used, this id will be used to prevent workers that were\u001b[39;00m\n\u001b[0;32m   1868\u001b[0m \u001b[39m# concurrently finalizing a task from the previous call to run the\u001b[39;00m\n\u001b[0;32m   1869\u001b[0m \u001b[39m# callback.\u001b[39;00m\n\u001b[0;32m   1870\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n",
      "File \u001b[1;32mc:\\Users\\Gebruiker\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\joblib\\parallel.py:1792\u001b[0m, in \u001b[0;36mParallel._get_sequential_output\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1790\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_dispatched_batches \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m   1791\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_dispatched_tasks \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m-> 1792\u001b[0m res \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1793\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_completed_tasks \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m   1794\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprint_progress()\n",
      "File \u001b[1;32mc:\\Users\\Gebruiker\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\parallel.py:127\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    125\u001b[0m     config \u001b[39m=\u001b[39m {}\n\u001b[0;32m    126\u001b[0m \u001b[39mwith\u001b[39;00m config_context(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mconfig):\n\u001b[1;32m--> 127\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfunction(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Gebruiker\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\pipeline.py:950\u001b[0m, in \u001b[0;36m_fit_transform_one\u001b[1;34m(transformer, X, y, weight, message_clsname, message, **fit_params)\u001b[0m\n\u001b[0;32m    948\u001b[0m \u001b[39mwith\u001b[39;00m _print_elapsed_time(message_clsname, message):\n\u001b[0;32m    949\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(transformer, \u001b[39m\"\u001b[39m\u001b[39mfit_transform\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m--> 950\u001b[0m         res \u001b[39m=\u001b[39m transformer\u001b[39m.\u001b[39;49mfit_transform(X, y, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mfit_params)\n\u001b[0;32m    951\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    952\u001b[0m         res \u001b[39m=\u001b[39m transformer\u001b[39m.\u001b[39mfit(X, y, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfit_params)\u001b[39m.\u001b[39mtransform(X)\n",
      "File \u001b[1;32mc:\\Users\\Gebruiker\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\_set_output.py:140\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[1;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[0;32m    138\u001b[0m \u001b[39m@wraps\u001b[39m(f)\n\u001b[0;32m    139\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrapped\u001b[39m(\u001b[39mself\u001b[39m, X, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m--> 140\u001b[0m     data_to_wrap \u001b[39m=\u001b[39m f(\u001b[39mself\u001b[39;49m, X, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m    141\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(data_to_wrap, \u001b[39mtuple\u001b[39m):\n\u001b[0;32m    142\u001b[0m         \u001b[39m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[0;32m    143\u001b[0m         return_tuple \u001b[39m=\u001b[39m (\n\u001b[0;32m    144\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[39m0\u001b[39m], X, \u001b[39mself\u001b[39m),\n\u001b[0;32m    145\u001b[0m             \u001b[39m*\u001b[39mdata_to_wrap[\u001b[39m1\u001b[39m:],\n\u001b[0;32m    146\u001b[0m         )\n",
      "File \u001b[1;32mc:\\Users\\Gebruiker\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\base.py:918\u001b[0m, in \u001b[0;36mTransformerMixin.fit_transform\u001b[1;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[0;32m    915\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfit(X, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfit_params)\u001b[39m.\u001b[39mtransform(X)\n\u001b[0;32m    916\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    917\u001b[0m     \u001b[39m# fit method of arity 2 (supervised transformation)\u001b[39;00m\n\u001b[1;32m--> 918\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfit(X, y, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mfit_params)\u001b[39m.\u001b[39;49mtransform(X)\n",
      "File \u001b[1;32mc:\\Users\\Gebruiker\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\_set_output.py:140\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[1;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[0;32m    138\u001b[0m \u001b[39m@wraps\u001b[39m(f)\n\u001b[0;32m    139\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrapped\u001b[39m(\u001b[39mself\u001b[39m, X, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m--> 140\u001b[0m     data_to_wrap \u001b[39m=\u001b[39m f(\u001b[39mself\u001b[39;49m, X, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m    141\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(data_to_wrap, \u001b[39mtuple\u001b[39m):\n\u001b[0;32m    142\u001b[0m         \u001b[39m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[0;32m    143\u001b[0m         return_tuple \u001b[39m=\u001b[39m (\n\u001b[0;32m    144\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[39m0\u001b[39m], X, \u001b[39mself\u001b[39m),\n\u001b[0;32m    145\u001b[0m             \u001b[39m*\u001b[39mdata_to_wrap[\u001b[39m1\u001b[39m:],\n\u001b[0;32m    146\u001b[0m         )\n",
      "\u001b[1;32mc:\\Users\\Gebruiker\\MLChallenge\\notebooks\\Experiments.ipynb Cell 16\u001b[0m line \u001b[0;36m2\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Gebruiker/MLChallenge/notebooks/Experiments.ipynb#X21sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtransform\u001b[39m(\u001b[39mself\u001b[39m, X):\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Gebruiker/MLChallenge/notebooks/Experiments.ipynb#X21sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmlb\u001b[39m.\u001b[39;49mtransform(X)\n",
      "File \u001b[1;32mc:\\Users\\Gebruiker\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\_set_output.py:140\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[1;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[0;32m    138\u001b[0m \u001b[39m@wraps\u001b[39m(f)\n\u001b[0;32m    139\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrapped\u001b[39m(\u001b[39mself\u001b[39m, X, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m--> 140\u001b[0m     data_to_wrap \u001b[39m=\u001b[39m f(\u001b[39mself\u001b[39;49m, X, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m    141\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(data_to_wrap, \u001b[39mtuple\u001b[39m):\n\u001b[0;32m    142\u001b[0m         \u001b[39m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[0;32m    143\u001b[0m         return_tuple \u001b[39m=\u001b[39m (\n\u001b[0;32m    144\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[39m0\u001b[39m], X, \u001b[39mself\u001b[39m),\n\u001b[0;32m    145\u001b[0m             \u001b[39m*\u001b[39mdata_to_wrap[\u001b[39m1\u001b[39m:],\n\u001b[0;32m    146\u001b[0m         )\n",
      "File \u001b[1;32mc:\\Users\\Gebruiker\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\preprocessing\\_label.py:853\u001b[0m, in \u001b[0;36mMultiLabelBinarizer.transform\u001b[1;34m(self, y)\u001b[0m\n\u001b[0;32m    837\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtransform\u001b[39m(\u001b[39mself\u001b[39m, y):\n\u001b[0;32m    838\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Transform the given label sets.\u001b[39;00m\n\u001b[0;32m    839\u001b[0m \n\u001b[0;32m    840\u001b[0m \u001b[39m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    851\u001b[0m \u001b[39m        `y[i]`, and 0 otherwise.\u001b[39;00m\n\u001b[0;32m    852\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 853\u001b[0m     check_is_fitted(\u001b[39mself\u001b[39;49m)\n\u001b[0;32m    855\u001b[0m     class_to_index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_build_cache()\n\u001b[0;32m    856\u001b[0m     yt \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_transform(y, class_to_index)\n",
      "File \u001b[1;32mc:\\Users\\Gebruiker\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\validation.py:1462\u001b[0m, in \u001b[0;36mcheck_is_fitted\u001b[1;34m(estimator, attributes, msg, all_or_any)\u001b[0m\n\u001b[0;32m   1459\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m is not an estimator instance.\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m (estimator))\n\u001b[0;32m   1461\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m _is_fitted(estimator, attributes, all_or_any):\n\u001b[1;32m-> 1462\u001b[0m     \u001b[39mraise\u001b[39;00m NotFittedError(msg \u001b[39m%\u001b[39m {\u001b[39m\"\u001b[39m\u001b[39mname\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mtype\u001b[39m(estimator)\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m})\n",
      "\u001b[1;31mNotFittedError\u001b[0m: This MultiLabelBinarizer instance is not fitted yet. Call 'fit' with appropriate arguments before using this estimator."
     ]
    }
   ],
   "source": [
    "# Set the logging level to INFO and set loading message\n",
    "logging.getLogger().setLevel(logging.INFO)\n",
    "    \n",
    "# Load train and test sets and change all NA values to empty values\n",
    "logging.info(\"Loading training/test data\")\n",
    "train = pd.DataFrame.from_records(json.load(open('../data/train.json'))).fillna(\"\")\n",
    "test = pd.DataFrame.from_records(json.load(open('../data/test.json'))).fillna(\"\")\n",
    "    \n",
    "# Split the train set into train (80%) and validation (20%) sets, 5-folds\n",
    "logging.info(\"Splitting validation\")\n",
    "num_folds = 5\n",
    "k_fold = KFold(n_splits=num_folds, shuffle=True, random_state=123)\n",
    "    \n",
    "# Store a featurizer to transform the 'title' column into a bag-of-words format\n",
    "featurizer_1 = ColumnTransformer(\n",
    "    transformers=[(\"title\", CountVectorizer(), \"title\")], remainder='drop')\n",
    "featurizer_2 = ColumnTransformer(\n",
    "    transformers=[(\"title\", TfidfVectorizer(), \"title\")], remainder='drop')\n",
    "featurizer_3 = ColumnTransformer(\n",
    "    transformers=[(\"abstract\", CountVectorizer(), \"abstract\")], remainder='drop')\n",
    "featurizer_4 = ColumnTransformer(\n",
    "    transformers=[(\"abstract\", TfidfVectorizer(), \"abstract\")], remainder='drop')\n",
    "featurizer_5 = ColumnTransformer(\n",
    "    transformers=[(\"author\", MultiLabelBinarizerTransformer(), \"author\")], remainder='drop')\n",
    "featurizer_6 = ColumnTransformer(\n",
    "    transformers=[(\"author\", MultiLabelBinarizerTransformer(), \"author\")], remainder='drop')\n",
    "featurizers = [featurizer_1, featurizer_2, featurizer_3, featurizer_4, featurizer_5, featurizer_6]\n",
    "\n",
    "for i, featurizer in enumerate(featurizers):\n",
    "    # Make a pipeline for the featurizer and a ridge model, that aims to minimize the sum of squares\n",
    "    ridge_cv = make_pipeline(featurizer, Ridge())\n",
    "    \n",
    "    # Drop target variable column and fit both models\n",
    "    logging.info(f\"Fitting model with featurizer {i+1}\")\n",
    "    ridge_cv.fit(train.drop('year', axis=1), train['year'].values)\n",
    "    \n",
    "    # Calculate and report both MAE's\n",
    "    logging.info(\"Evaluating on validation data\")\n",
    "    ridge_cv_scores = cross_val_score(ridge_cv, train.drop('year', axis=1), train['year'].values, cv=k_fold, scoring='neg_mean_absolute_error')\n",
    "    logging.info(f\"Ridge regress MAE with featurizer {i+1} ({num_folds}-fold cross-validated): {-ridge_cv_scores.mean()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>00</th>\n",
       "      <th>000</th>\n",
       "      <th>001</th>\n",
       "      <th>0099</th>\n",
       "      <th>01</th>\n",
       "      <th>02</th>\n",
       "      <th>03</th>\n",
       "      <th>04</th>\n",
       "      <th>07</th>\n",
       "      <th>08</th>\n",
       "      <th>...</th>\n",
       "      <th>雜訊環境下應用線性估測編碼於特徵時序列之強健性語音辨識</th>\n",
       "      <th>雜訊環境與說話內容因素分析之強健性語音辨認</th>\n",
       "      <th>電腦輔助句子重組試題編製</th>\n",
       "      <th>電話查詢口語對話系統中語音辨識不確定性之處理</th>\n",
       "      <th>電話轉接對話模式與表達轉接要求句型的分析</th>\n",
       "      <th>非負矩陣分解法於語音調變頻譜強化之研究</th>\n",
       "      <th>面向中文口语理解的基于依赖引导的字特征槽填充模型</th>\n",
       "      <th>面向对话文本的实体关系抽取</th>\n",
       "      <th>面向机器阅读理解的高质量藏语数据集构建</th>\n",
       "      <th>領域相關詞彙極性分析及文件情緒分類之研究</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 13978 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    00  000  001  0099   01   02   03   04   07   08  ...  \\\n",
       "0  0.0  0.0  0.0   0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   \n",
       "1  0.0  0.0  0.0   0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   \n",
       "2  0.0  0.0  0.0   0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   \n",
       "3  0.0  0.0  0.0   0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   \n",
       "4  0.0  0.0  0.0   0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   \n",
       "\n",
       "   雜訊環境下應用線性估測編碼於特徵時序列之強健性語音辨識  雜訊環境與說話內容因素分析之強健性語音辨認  電腦輔助句子重組試題編製  \\\n",
       "0                          0.0                    0.0           0.0   \n",
       "1                          0.0                    0.0           0.0   \n",
       "2                          0.0                    0.0           0.0   \n",
       "3                          0.0                    0.0           0.0   \n",
       "4                          0.0                    0.0           0.0   \n",
       "\n",
       "   電話查詢口語對話系統中語音辨識不確定性之處理  電話轉接對話模式與表達轉接要求句型的分析  非負矩陣分解法於語音調變頻譜強化之研究  \\\n",
       "0                     0.0                   0.0                  0.0   \n",
       "1                     0.0                   0.0                  0.0   \n",
       "2                     0.0                   0.0                  0.0   \n",
       "3                     0.0                   0.0                  0.0   \n",
       "4                     0.0                   0.0                  0.0   \n",
       "\n",
       "   面向中文口语理解的基于依赖引导的字特征槽填充模型  面向对话文本的实体关系抽取  面向机器阅读理解的高质量藏语数据集构建  \\\n",
       "0                       0.0            0.0                  0.0   \n",
       "1                       0.0            0.0                  0.0   \n",
       "2                       0.0            0.0                  0.0   \n",
       "3                       0.0            0.0                  0.0   \n",
       "4                       0.0            0.0                  0.0   \n",
       "\n",
       "   領域相關詞彙極性分析及文件情緒分類之研究  \n",
       "0                   0.0  \n",
       "1                   0.0  \n",
       "2                   0.0  \n",
       "3                   0.0  \n",
       "4                   0.0  \n",
       "\n",
       "[5 rows x 13978 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(val['title'])\n",
    "feature_names1 = tfidf_vectorizer.get_feature_names_out()\n",
    "tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=feature_names1)\n",
    "tfidf_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>00</th>\n",
       "      <th>000</th>\n",
       "      <th>001</th>\n",
       "      <th>0099</th>\n",
       "      <th>01</th>\n",
       "      <th>02</th>\n",
       "      <th>03</th>\n",
       "      <th>04</th>\n",
       "      <th>07</th>\n",
       "      <th>08</th>\n",
       "      <th>...</th>\n",
       "      <th>雜訊環境下應用線性估測編碼於特徵時序列之強健性語音辨識</th>\n",
       "      <th>雜訊環境與說話內容因素分析之強健性語音辨認</th>\n",
       "      <th>電腦輔助句子重組試題編製</th>\n",
       "      <th>電話查詢口語對話系統中語音辨識不確定性之處理</th>\n",
       "      <th>電話轉接對話模式與表達轉接要求句型的分析</th>\n",
       "      <th>非負矩陣分解法於語音調變頻譜強化之研究</th>\n",
       "      <th>面向中文口语理解的基于依赖引导的字特征槽填充模型</th>\n",
       "      <th>面向对话文本的实体关系抽取</th>\n",
       "      <th>面向机器阅读理解的高质量藏语数据集构建</th>\n",
       "      <th>領域相關詞彙極性分析及文件情緒分類之研究</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 13978 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   00  000  001  0099  01  02  03  04  07  08  ...  \\\n",
       "0   0    0    0     0   0   0   0   0   0   0  ...   \n",
       "1   0    0    0     0   0   0   0   0   0   0  ...   \n",
       "2   0    0    0     0   0   0   0   0   0   0  ...   \n",
       "3   0    0    0     0   0   0   0   0   0   0  ...   \n",
       "4   0    0    0     0   0   0   0   0   0   0  ...   \n",
       "\n",
       "   雜訊環境下應用線性估測編碼於特徵時序列之強健性語音辨識  雜訊環境與說話內容因素分析之強健性語音辨認  電腦輔助句子重組試題編製  \\\n",
       "0                            0                      0             0   \n",
       "1                            0                      0             0   \n",
       "2                            0                      0             0   \n",
       "3                            0                      0             0   \n",
       "4                            0                      0             0   \n",
       "\n",
       "   電話查詢口語對話系統中語音辨識不確定性之處理  電話轉接對話模式與表達轉接要求句型的分析  非負矩陣分解法於語音調變頻譜強化之研究  \\\n",
       "0                       0                     0                    0   \n",
       "1                       0                     0                    0   \n",
       "2                       0                     0                    0   \n",
       "3                       0                     0                    0   \n",
       "4                       0                     0                    0   \n",
       "\n",
       "   面向中文口语理解的基于依赖引导的字特征槽填充模型  面向对话文本的实体关系抽取  面向机器阅读理解的高质量藏语数据集构建  \\\n",
       "0                         0              0                    0   \n",
       "1                         0              0                    0   \n",
       "2                         0              0                    0   \n",
       "3                         0              0                    0   \n",
       "4                         0              0                    0   \n",
       "\n",
       "   領域相關詞彙極性分析及文件情緒分類之研究  \n",
       "0                     0  \n",
       "1                     0  \n",
       "2                     0  \n",
       "3                     0  \n",
       "4                     0  \n",
       "\n",
       "[5 rows x 13978 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_vectorizer = CountVectorizer()\n",
    "count_matrix = count_vectorizer.fit_transform(val['title'])\n",
    "feature_names2 = count_vectorizer.get_feature_names_out()\n",
    "count_df = pd.DataFrame(count_matrix.toarray(), columns=feature_names2)\n",
    "count_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        0.0\n",
       "1        0.0\n",
       "2        0.0\n",
       "3        0.0\n",
       "4        0.0\n",
       "        ... \n",
       "16474    0.0\n",
       "16475    0.0\n",
       "16476    0.0\n",
       "16477    0.0\n",
       "16478    0.0\n",
       "Name: author, Length: 16479, dtype: float64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_df['author']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This paragraph build upon the previous baseline code. It entails the following adjustments/additions chronologically:\n",
    "\n",
    "- [x] Removal of dummy regressor, since ridge works better from the very start;\n",
    "- [x] 5-fold cross validation to reduce variability (Ridge regress MAE (5.773));\n",
    "- [x] Try sklearn's other feature vectorizers (tf-idf (5.384), ...);\n",
    "- [ ] Perform custom preprocessing, tokenizations within sklearn;\n",
    "- [ ] Smooth sparse matrices (?);\n",
    "- [ ] Tune hyperparameters of feature vectorizers (n-gram size);\n",
    "- [ ] Test for or include other columns (abstract, authors (?));\n",
    "- [ ] Try tasks other than regression, like lazy learning (kNN)(?);\n",
    "- [ ] Try BERTopic modelling;\n",
    "- [ ] "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
