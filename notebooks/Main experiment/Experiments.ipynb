{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Baseline code provided by uni"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import logging\n",
    "import json\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.dummy import DummyRegressor\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.metrics import mean_absolute_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Baseline function to create [predictions](https://github.com/larshanen/MLChallenge/tree/main/notebooks/predicted.json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    # Set the logging level to INFO and set loading message\n",
    "    logging.getLogger().setLevel(logging.INFO)\n",
    "    \n",
    "    # Load train and test sets and change all NA values to empty values\n",
    "    logging.info(\"Loading training/test data\")\n",
    "    train = pd.DataFrame.from_records(json.load(open('../data/train.json'))).fillna(\"\")\n",
    "    test = pd.DataFrame.from_records(json.load(open('../data/test.json'))).fillna(\"\")\n",
    "    \n",
    "    # Split the train set into train (75%) and validation (25%) sets\n",
    "    logging.info(\"Splitting validation\")\n",
    "    train, val = train_test_split(train, stratify=train['year'], random_state=123)\n",
    "    \n",
    "    # Store a featurizer to transform the 'title' column into a bag-of-words format\n",
    "    featurizer = ColumnTransformer(\n",
    "        transformers=[(\"title\", CountVectorizer(), \"title\")], remainder='drop')\n",
    "    \n",
    "    # Make a pipeline for the featurizer combined with a dummy regressor, that simply predicts the overall trained mean of the target variable\n",
    "    dummy = make_pipeline(featurizer, DummyRegressor(strategy='mean'))\n",
    "\n",
    "    # Make a pipeline for the featurizer and a ridge model, that aims to minimize the sum of squares\n",
    "    ridge = make_pipeline(featurizer, Ridge())\n",
    "    \n",
    "    # Drop target variable column and fit both models\n",
    "    logging.info(\"Fitting models\")\n",
    "    dummy.fit(train.drop('year', axis=1), train['year'].values)\n",
    "    ridge.fit(train.drop('year', axis=1), train['year'].values)\n",
    "    \n",
    "    # Calculate and report both MAE's\n",
    "    logging.info(\"Evaluating on validation data\")\n",
    "    err = mean_absolute_error(val['year'].values, dummy.predict(val.drop('year', axis=1)))\n",
    "    logging.info(f\"Mean baseline MAE: {err}\")\n",
    "    err = mean_absolute_error(val['year'].values, ridge.predict(val.drop('year', axis=1)))\n",
    "    logging.info(f\"Ridge regress MAE: {err}\")\n",
    "    \n",
    "    # Let the ridge model predict on test set\n",
    "    logging.info(f\"Predicting on test\")\n",
    "    pred = ridge.predict(test)\n",
    "    test['year'] = pred\n",
    "    \n",
    "    # Write JSON prediction file\n",
    "    logging.info(\"Writing prediction file\")\n",
    "    test.to_json(\"predicted.json\", orient='records', indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Loading training/test data\n",
      "INFO:root:Splitting validation\n",
      "INFO:root:Fitting models\n",
      "INFO:root:Evaluating on validation data\n",
      "INFO:root:Mean baseline MAE: 7.8054390754858805\n",
      "INFO:root:Ridge regress MAE: 5.812345349001838\n",
      "INFO:root:Predicting on test\n",
      "INFO:root:Writing prediction file\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Team code\n",
    "\n",
    "Please follow the instructions beneath when writing or adjusting code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Describe every piece of code with comments\n",
    "# Include your name in every header so we can report our individual contributions (this is mandatory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Explore baseline performance (Lars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Loading training/test data\n"
     ]
    }
   ],
   "source": [
    "# Set the logging level to INFO and set loading message\n",
    "logging.getLogger().setLevel(logging.INFO)\n",
    "    \n",
    "# Load train and test sets and change all NA values to empty values\n",
    "logging.info(\"Loading training/test data\")\n",
    "train = pd.DataFrame.from_records(json.load(open('../../data/train.json')))\n",
    "test = pd.DataFrame.from_records(json.load(open('../../data/test.json')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Splitting validation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Fitting models\n",
      "INFO:root:Evaluating on validation data\n",
      "INFO:root:Mean baseline MAE: 7.8054390754858805\n",
      "INFO:root:Ridge regress MAE: 5.812345349001838\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "# Split the train set into train (75%) and validation (25%) sets\n",
    "logging.info(\"Splitting validation\")\n",
    "train, val = train_test_split(train, stratify=train['year'], random_state=123)\n",
    "    \n",
    "# Store a featurizer to transform the 'title' column into a bag-of-words format\n",
    "featurizer = ColumnTransformer(\n",
    "transformers=[(\"title\", CountVectorizer(), \"title\")], remainder='drop')\n",
    "    \n",
    "# Make a pipeline for the featurizer combined with a dummy regressor, that simply predicts the overall trained mean of the target variable\n",
    "dummy = make_pipeline(featurizer, DummyRegressor(strategy='mean'))\n",
    "\n",
    "# Make a pipeline for the featurizer and a ridge model, that aims to minimize the sum of squares\n",
    "ridge = make_pipeline(featurizer, Ridge())\n",
    "    \n",
    "# Drop target variable column and fit both models\n",
    "logging.info(\"Fitting models\")\n",
    "dummy.fit(train.drop('year', axis=1), train['year'].values)\n",
    "ridge.fit(train.drop('year', axis=1), train['year'].values)\n",
    "    \n",
    "# Calculate and report both MAE's\n",
    "logging.info(\"Evaluating on validation data\")\n",
    "err = mean_absolute_error(val['year'].values, dummy.predict(val.drop('year', axis=1)))\n",
    "logging.info(f\"Mean baseline MAE: {err}\")\n",
    "err = mean_absolute_error(val['year'].values, ridge.predict(val.drop('year', axis=1)))\n",
    "logging.info(f\"Ridge regress MAE: {err}\")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Preprocessing (Lars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import extra modules\n",
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For experimental purposes we start working with a 10% subset of the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ENTRYTYPE</th>\n",
       "      <th>title</th>\n",
       "      <th>editor</th>\n",
       "      <th>year</th>\n",
       "      <th>publisher</th>\n",
       "      <th>author</th>\n",
       "      <th>abstract</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>12680</th>\n",
       "      <td>inproceedings</td>\n",
       "      <td>Question-Answering Based on Virtually Integrat...</td>\n",
       "      <td>None</td>\n",
       "      <td>2003</td>\n",
       "      <td>Association for Computational Linguistics</td>\n",
       "      <td>[Choi, Key-Sun, Kim, Jae-Ho, Miyazaki, Masaru,...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17292</th>\n",
       "      <td>inproceedings</td>\n",
       "      <td>AMI&amp;ERIC: How to Learn with Naive Bayes and Pr...</td>\n",
       "      <td>None</td>\n",
       "      <td>2013</td>\n",
       "      <td>Association for Computational Linguistics</td>\n",
       "      <td>[Dermouche, Mohamed, Khouas, Leila, Velcin, Ju...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33265</th>\n",
       "      <td>inproceedings</td>\n",
       "      <td>Inducing Gazetteers for Named Entity Recogniti...</td>\n",
       "      <td>None</td>\n",
       "      <td>2008</td>\n",
       "      <td>Association for Computational Linguistics</td>\n",
       "      <td>[Kazama, Jun'ichi, Torisawa, Kentaro]</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52850</th>\n",
       "      <td>inproceedings</td>\n",
       "      <td>Leveraging Explicit Lexico-logical Alignments ...</td>\n",
       "      <td>None</td>\n",
       "      <td>2022</td>\n",
       "      <td>Association for Computational Linguistics</td>\n",
       "      <td>[Sun, Runxin, He, Shizhu, Zhu, Chong, He, Yaoh...</td>\n",
       "      <td>Text-to-SQL aims to parse natural language que...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2298</th>\n",
       "      <td>inproceedings</td>\n",
       "      <td>CLAM: Quickly deploy NLP command-line tools on...</td>\n",
       "      <td>None</td>\n",
       "      <td>2014</td>\n",
       "      <td>Dublin City University and Association for Com...</td>\n",
       "      <td>[van Gompel, Maarten, Reynaert, Martin]</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>505</th>\n",
       "      <td>inproceedings</td>\n",
       "      <td>XUXEN: A Spelling Checker/Corrector for Basque...</td>\n",
       "      <td>None</td>\n",
       "      <td>1992</td>\n",
       "      <td>Association for Computational Linguistics</td>\n",
       "      <td>[Agirre, E., Alegria, I, Arregi, X, Artola, X,...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39223</th>\n",
       "      <td>inproceedings</td>\n",
       "      <td>Towards Building a Spoken Dialogue System for ...</td>\n",
       "      <td>None</td>\n",
       "      <td>2022</td>\n",
       "      <td>European Language Resources Association</td>\n",
       "      <td>[Aicher, Annalena, Gerstenlauer, Nadine, Feust...</td>\n",
       "      <td>Speech interfaces for argumentative dialogue s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14117</th>\n",
       "      <td>inproceedings</td>\n",
       "      <td>Combining Multiple, Large-Scale Resources in a...</td>\n",
       "      <td>None</td>\n",
       "      <td>1998</td>\n",
       "      <td>None</td>\n",
       "      <td>[Jing, Hongyan, McKeown, Kathleen]</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29826</th>\n",
       "      <td>inproceedings</td>\n",
       "      <td>Lying Through One's Teeth: A Study on Verbal L...</td>\n",
       "      <td>None</td>\n",
       "      <td>2021</td>\n",
       "      <td>Association for Computational Linguistics</td>\n",
       "      <td>[Yeh, Min-Hsuan, Ku, Lun-Wei]</td>\n",
       "      <td>Although many studies use the LIWC lexicon to ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12496</th>\n",
       "      <td>inproceedings</td>\n",
       "      <td>Toward an Interagency Language Roundtable Base...</td>\n",
       "      <td>None</td>\n",
       "      <td>2006</td>\n",
       "      <td>Association for Machine Translation in the Ame...</td>\n",
       "      <td>[Jones, Douglas, Anderson, Timothy, Atwell, Sa...</td>\n",
       "      <td>We present observations from three exercises d...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6591 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           ENTRYTYPE                                              title  \\\n",
       "12680  inproceedings  Question-Answering Based on Virtually Integrat...   \n",
       "17292  inproceedings  AMI&ERIC: How to Learn with Naive Bayes and Pr...   \n",
       "33265  inproceedings  Inducing Gazetteers for Named Entity Recogniti...   \n",
       "52850  inproceedings  Leveraging Explicit Lexico-logical Alignments ...   \n",
       "2298   inproceedings  CLAM: Quickly deploy NLP command-line tools on...   \n",
       "...              ...                                                ...   \n",
       "505    inproceedings  XUXEN: A Spelling Checker/Corrector for Basque...   \n",
       "39223  inproceedings  Towards Building a Spoken Dialogue System for ...   \n",
       "14117  inproceedings  Combining Multiple, Large-Scale Resources in a...   \n",
       "29826  inproceedings  Lying Through One's Teeth: A Study on Verbal L...   \n",
       "12496  inproceedings  Toward an Interagency Language Roundtable Base...   \n",
       "\n",
       "      editor  year                                          publisher  \\\n",
       "12680   None  2003          Association for Computational Linguistics   \n",
       "17292   None  2013          Association for Computational Linguistics   \n",
       "33265   None  2008          Association for Computational Linguistics   \n",
       "52850   None  2022          Association for Computational Linguistics   \n",
       "2298    None  2014  Dublin City University and Association for Com...   \n",
       "...      ...   ...                                                ...   \n",
       "505     None  1992          Association for Computational Linguistics   \n",
       "39223   None  2022            European Language Resources Association   \n",
       "14117   None  1998                                               None   \n",
       "29826   None  2021          Association for Computational Linguistics   \n",
       "12496   None  2006  Association for Machine Translation in the Ame...   \n",
       "\n",
       "                                                  author  \\\n",
       "12680  [Choi, Key-Sun, Kim, Jae-Ho, Miyazaki, Masaru,...   \n",
       "17292  [Dermouche, Mohamed, Khouas, Leila, Velcin, Ju...   \n",
       "33265              [Kazama, Jun'ichi, Torisawa, Kentaro]   \n",
       "52850  [Sun, Runxin, He, Shizhu, Zhu, Chong, He, Yaoh...   \n",
       "2298             [van Gompel, Maarten, Reynaert, Martin]   \n",
       "...                                                  ...   \n",
       "505    [Agirre, E., Alegria, I, Arregi, X, Artola, X,...   \n",
       "39223  [Aicher, Annalena, Gerstenlauer, Nadine, Feust...   \n",
       "14117                 [Jing, Hongyan, McKeown, Kathleen]   \n",
       "29826                      [Yeh, Min-Hsuan, Ku, Lun-Wei]   \n",
       "12496  [Jones, Douglas, Anderson, Timothy, Atwell, Sa...   \n",
       "\n",
       "                                                abstract  \n",
       "12680                                               None  \n",
       "17292                                               None  \n",
       "33265                                               None  \n",
       "52850  Text-to-SQL aims to parse natural language que...  \n",
       "2298                                                None  \n",
       "...                                                  ...  \n",
       "505                                                 None  \n",
       "39223  Speech interfaces for argumentative dialogue s...  \n",
       "14117                                               None  \n",
       "29826  Although many studies use the LIWC lexicon to ...  \n",
       "12496  We present observations from three exercises d...  \n",
       "\n",
       "[6591 rows x 7 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Randomly save 10% of the train set for velocity purposes\n",
    "percentage_to_save = 10\n",
    "\n",
    "# Calculate the number of rows to save\n",
    "num_rows_to_save = int(len(train) * (percentage_to_save / 100))\n",
    "\n",
    "# Use the sample method to randomly select rows\n",
    "train_sample = train.sample(n=num_rows_to_save, random_state=42)  # Set a random_state for reproducibility\n",
    "\n",
    "train_sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.1 Drop all columns with over 75% of missing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ENTRYTYPE</th>\n",
       "      <th>title</th>\n",
       "      <th>year</th>\n",
       "      <th>publisher</th>\n",
       "      <th>author</th>\n",
       "      <th>abstract</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>inproceedings</td>\n",
       "      <td>Philippine Language Resources: Trends and Dire...</td>\n",
       "      <td>2009</td>\n",
       "      <td>Association for Computational Linguistics</td>\n",
       "      <td>[Roxas, Rachel Edita, Cheng, Charibeth, Lim, N...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>inproceedings</td>\n",
       "      <td>A System for Translating Locative Prepositions...</td>\n",
       "      <td>1991</td>\n",
       "      <td>Association for Computational Linguistics</td>\n",
       "      <td>[Japkowicz, Nathalie, Wiebe, Janyce M.]</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>inproceedings</td>\n",
       "      <td>Introduction to the Shared Task on Comparing S...</td>\n",
       "      <td>2008</td>\n",
       "      <td>College Publications</td>\n",
       "      <td>[Bos, Johan]</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>inproceedings</td>\n",
       "      <td>Pynini: A Python library for weighted finite-s...</td>\n",
       "      <td>2016</td>\n",
       "      <td>Association for Computational Linguistics</td>\n",
       "      <td>[Gorman, Kyle]</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>inproceedings</td>\n",
       "      <td>Improving Readability of Swedish Electronic He...</td>\n",
       "      <td>2014</td>\n",
       "      <td>Association for Computational Linguistics</td>\n",
       "      <td>[Grigonyte, Gintarė, Kvist, Maria, Velupillai,...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65909</th>\n",
       "      <td>inproceedings</td>\n",
       "      <td>Optimizing the weighted sequence alignment alg...</td>\n",
       "      <td>2022</td>\n",
       "      <td>Association for Computational Linguistics</td>\n",
       "      <td>[Janicki, Maciej]</td>\n",
       "      <td>We present an optimized implementation of the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65910</th>\n",
       "      <td>proceedings</td>\n",
       "      <td>Proceedings of the 25th Conference on Computat...</td>\n",
       "      <td>2021</td>\n",
       "      <td>Association for Computational Linguistics</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65911</th>\n",
       "      <td>article</td>\n",
       "      <td>A Large-Scale Pseudoword-Based Evaluation Fram...</td>\n",
       "      <td>2014</td>\n",
       "      <td>MIT Press</td>\n",
       "      <td>[Pilehvar, Mohammad Taher, Navigli, Roberto]</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65912</th>\n",
       "      <td>inproceedings</td>\n",
       "      <td>CIST System for CL-SciSumm 2016 Shared Task</td>\n",
       "      <td>2016</td>\n",
       "      <td>None</td>\n",
       "      <td>[Li, Lei, Mao, Liyuan, Zhang, Yazhao, Chi, Jun...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65913</th>\n",
       "      <td>inproceedings</td>\n",
       "      <td>Ontology Engineering and Knowledge Extraction ...</td>\n",
       "      <td>2009</td>\n",
       "      <td>Association for Computational Linguistics</td>\n",
       "      <td>[Trapman, Jantine, Monachesi, Paola]</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>65914 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           ENTRYTYPE                                              title  year  \\\n",
       "0      inproceedings  Philippine Language Resources: Trends and Dire...  2009   \n",
       "1      inproceedings  A System for Translating Locative Prepositions...  1991   \n",
       "2      inproceedings  Introduction to the Shared Task on Comparing S...  2008   \n",
       "3      inproceedings  Pynini: A Python library for weighted finite-s...  2016   \n",
       "4      inproceedings  Improving Readability of Swedish Electronic He...  2014   \n",
       "...              ...                                                ...   ...   \n",
       "65909  inproceedings  Optimizing the weighted sequence alignment alg...  2022   \n",
       "65910    proceedings  Proceedings of the 25th Conference on Computat...  2021   \n",
       "65911        article  A Large-Scale Pseudoword-Based Evaluation Fram...  2014   \n",
       "65912  inproceedings        CIST System for CL-SciSumm 2016 Shared Task  2016   \n",
       "65913  inproceedings  Ontology Engineering and Knowledge Extraction ...  2009   \n",
       "\n",
       "                                       publisher  \\\n",
       "0      Association for Computational Linguistics   \n",
       "1      Association for Computational Linguistics   \n",
       "2                           College Publications   \n",
       "3      Association for Computational Linguistics   \n",
       "4      Association for Computational Linguistics   \n",
       "...                                          ...   \n",
       "65909  Association for Computational Linguistics   \n",
       "65910  Association for Computational Linguistics   \n",
       "65911                                  MIT Press   \n",
       "65912                                       None   \n",
       "65913  Association for Computational Linguistics   \n",
       "\n",
       "                                                  author  \\\n",
       "0      [Roxas, Rachel Edita, Cheng, Charibeth, Lim, N...   \n",
       "1                [Japkowicz, Nathalie, Wiebe, Janyce M.]   \n",
       "2                                           [Bos, Johan]   \n",
       "3                                         [Gorman, Kyle]   \n",
       "4      [Grigonyte, Gintarė, Kvist, Maria, Velupillai,...   \n",
       "...                                                  ...   \n",
       "65909                                  [Janicki, Maciej]   \n",
       "65910                                               None   \n",
       "65911       [Pilehvar, Mohammad Taher, Navigli, Roberto]   \n",
       "65912  [Li, Lei, Mao, Liyuan, Zhang, Yazhao, Chi, Jun...   \n",
       "65913               [Trapman, Jantine, Monachesi, Paola]   \n",
       "\n",
       "                                                abstract  \n",
       "0                                                   None  \n",
       "1                                                   None  \n",
       "2                                                   None  \n",
       "3                                                   None  \n",
       "4                                                   None  \n",
       "...                                                  ...  \n",
       "65909  We present an optimized implementation of the ...  \n",
       "65910                                               None  \n",
       "65911                                               None  \n",
       "65912                                               None  \n",
       "65913                                               None  \n",
       "\n",
       "[65914 rows x 6 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set threshold on 75%\n",
    "threshold = 0.25\n",
    "\n",
    "# Calculate the threshold for each column\n",
    "missing_threshold = int(threshold * len(train))\n",
    "\n",
    "# Drop columns with more than the specified percentage of missing data\n",
    "train_filtered = train.dropna(axis=1, thresh=missing_threshold)\n",
    "\n",
    "train_filtered"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.2 Featurize 'author' column (count-vectors, reduced to top X most frequent authors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Gebruiker\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bhattacharyya, pushpak</th>\n",
       "      <th>gurevych, iryna</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65909</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65910</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65911</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65912</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65913</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>65914 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       bhattacharyya, pushpak  gurevych, iryna\n",
       "0                           0                0\n",
       "1                           0                0\n",
       "2                           0                0\n",
       "3                           0                0\n",
       "4                           0                0\n",
       "...                       ...              ...\n",
       "65909                       0                0\n",
       "65910                       0                0\n",
       "65911                       0                0\n",
       "65912                       0                0\n",
       "65913                       0                0\n",
       "\n",
       "[65914 rows x 2 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert lists of strings, accounting for None values\n",
    "train_filtered['author_str'] = train_filtered['author'].apply(lambda x: ';'.join(map(str, x)) if x is not None else 'unknown')\n",
    "\n",
    "# Add a column to store the original row numbers\n",
    "# train_filtered['original_index'] = train_filtered.index\n",
    "\n",
    "# Count the number of papers for each author\n",
    "author_paper_counts = train_filtered['author_str'].str.split(';').explode().value_counts()\n",
    "\n",
    "# Set the number of most frequent authors you want to include\n",
    "n_mostfreq_authors = 3  # Adjust this value to the desired number of most frequent authors\n",
    "\n",
    "# Filter authors based on the X most frequent authors\n",
    "top_authors = author_paper_counts.head(n_mostfreq_authors).index.tolist()\n",
    "\n",
    "# Filter only the top authors in 'author_str'\n",
    "train_filtered['author_str_filtered'] = train_filtered['author_str'].apply(lambda x: ';'.join([author for author in x.split(';') if author in top_authors]))\n",
    "\n",
    "# Count-vectorize 'author_str_filtered'\n",
    "count_vectorizer = CountVectorizer(tokenizer=lambda x: x.split(';'))\n",
    "count_matrix = count_vectorizer.fit_transform(train_filtered['author_str_filtered'])\n",
    "\n",
    "# Extract and create columns\n",
    "feature_names = count_vectorizer.get_feature_names_out()\n",
    "author_count_df = pd.DataFrame(count_matrix.toarray(), columns=feature_names)\n",
    "\n",
    "# Set the original_index column as the index\n",
    "# author_count_df.set_index(train_filtered['original_index'], inplace=True)\n",
    "\n",
    "author_count_df = author_count_df.drop(['unknown', ''], axis=1) # See if this approach always works out\n",
    "author_count_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merge with train_filtered, meaning we drop the author column and then add author_count_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ENTRYTYPE</th>\n",
       "      <th>title</th>\n",
       "      <th>year</th>\n",
       "      <th>publisher</th>\n",
       "      <th>abstract</th>\n",
       "      <th>bhattacharyya, pushpak</th>\n",
       "      <th>gurevych, iryna</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>inproceedings</td>\n",
       "      <td>Philippine Language Resources: Trends and Dire...</td>\n",
       "      <td>2009</td>\n",
       "      <td>Association for Computational Linguistics</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>inproceedings</td>\n",
       "      <td>A System for Translating Locative Prepositions...</td>\n",
       "      <td>1991</td>\n",
       "      <td>Association for Computational Linguistics</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>inproceedings</td>\n",
       "      <td>Introduction to the Shared Task on Comparing S...</td>\n",
       "      <td>2008</td>\n",
       "      <td>College Publications</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>inproceedings</td>\n",
       "      <td>Pynini: A Python library for weighted finite-s...</td>\n",
       "      <td>2016</td>\n",
       "      <td>Association for Computational Linguistics</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>inproceedings</td>\n",
       "      <td>Improving Readability of Swedish Electronic He...</td>\n",
       "      <td>2014</td>\n",
       "      <td>Association for Computational Linguistics</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65909</th>\n",
       "      <td>inproceedings</td>\n",
       "      <td>Optimizing the weighted sequence alignment alg...</td>\n",
       "      <td>2022</td>\n",
       "      <td>Association for Computational Linguistics</td>\n",
       "      <td>We present an optimized implementation of the ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65910</th>\n",
       "      <td>proceedings</td>\n",
       "      <td>Proceedings of the 25th Conference on Computat...</td>\n",
       "      <td>2021</td>\n",
       "      <td>Association for Computational Linguistics</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65911</th>\n",
       "      <td>article</td>\n",
       "      <td>A Large-Scale Pseudoword-Based Evaluation Fram...</td>\n",
       "      <td>2014</td>\n",
       "      <td>MIT Press</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65912</th>\n",
       "      <td>inproceedings</td>\n",
       "      <td>CIST System for CL-SciSumm 2016 Shared Task</td>\n",
       "      <td>2016</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65913</th>\n",
       "      <td>inproceedings</td>\n",
       "      <td>Ontology Engineering and Knowledge Extraction ...</td>\n",
       "      <td>2009</td>\n",
       "      <td>Association for Computational Linguistics</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>65914 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           ENTRYTYPE                                              title  year  \\\n",
       "0      inproceedings  Philippine Language Resources: Trends and Dire...  2009   \n",
       "1      inproceedings  A System for Translating Locative Prepositions...  1991   \n",
       "2      inproceedings  Introduction to the Shared Task on Comparing S...  2008   \n",
       "3      inproceedings  Pynini: A Python library for weighted finite-s...  2016   \n",
       "4      inproceedings  Improving Readability of Swedish Electronic He...  2014   \n",
       "...              ...                                                ...   ...   \n",
       "65909  inproceedings  Optimizing the weighted sequence alignment alg...  2022   \n",
       "65910    proceedings  Proceedings of the 25th Conference on Computat...  2021   \n",
       "65911        article  A Large-Scale Pseudoword-Based Evaluation Fram...  2014   \n",
       "65912  inproceedings        CIST System for CL-SciSumm 2016 Shared Task  2016   \n",
       "65913  inproceedings  Ontology Engineering and Knowledge Extraction ...  2009   \n",
       "\n",
       "                                       publisher  \\\n",
       "0      Association for Computational Linguistics   \n",
       "1      Association for Computational Linguistics   \n",
       "2                           College Publications   \n",
       "3      Association for Computational Linguistics   \n",
       "4      Association for Computational Linguistics   \n",
       "...                                          ...   \n",
       "65909  Association for Computational Linguistics   \n",
       "65910  Association for Computational Linguistics   \n",
       "65911                                  MIT Press   \n",
       "65912                                       None   \n",
       "65913  Association for Computational Linguistics   \n",
       "\n",
       "                                                abstract  \\\n",
       "0                                                   None   \n",
       "1                                                   None   \n",
       "2                                                   None   \n",
       "3                                                   None   \n",
       "4                                                   None   \n",
       "...                                                  ...   \n",
       "65909  We present an optimized implementation of the ...   \n",
       "65910                                               None   \n",
       "65911                                               None   \n",
       "65912                                               None   \n",
       "65913                                               None   \n",
       "\n",
       "       bhattacharyya, pushpak  gurevych, iryna  \n",
       "0                           0                0  \n",
       "1                           0                0  \n",
       "2                           0                0  \n",
       "3                           0                0  \n",
       "4                           0                0  \n",
       "...                       ...              ...  \n",
       "65909                       0                0  \n",
       "65910                       0                0  \n",
       "65911                       0                0  \n",
       "65912                       0                0  \n",
       "65913                       0                0  \n",
       "\n",
       "[65914 rows x 7 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Drop all redundant columns\n",
    "train_filtered_buffer = train_filtered.drop(['author', 'author_str', 'author_str_filtered'], axis=1)\n",
    "\n",
    "# Concatenate the original with dropped redundants and the extracted features for author\n",
    "train_2 = pd.concat([train_filtered_buffer, author_count_df], axis=1).reindex(train_filtered_buffer.index)\n",
    "train_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.3 Vectorize 'ENTRYTYPE' column (3-categorical variable one-hot encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category_article</th>\n",
       "      <th>category_inproceedings</th>\n",
       "      <th>category_proceedings</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>63840</td>\n",
       "      <td>3948</td>\n",
       "      <td>64040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2074</td>\n",
       "      <td>61966</td>\n",
       "      <td>1874</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   category_article  category_inproceedings  category_proceedings\n",
       "0             63840                    3948                 64040\n",
       "1              2074                   61966                  1874"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Perform one-hot encoding\n",
    "train_encoded_entrytype = pd.get_dummies(train_2['ENTRYTYPE'], columns=['category'], prefix='category')\n",
    "\n",
    "# Show count-values for each of the columns\n",
    "train_encoded_entrytype.apply(lambda x: x.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merge with train_filtered, meaning we drop the ENTRYTYPE column and then add train_encoded_entrytype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>year</th>\n",
       "      <th>publisher</th>\n",
       "      <th>abstract</th>\n",
       "      <th>bhattacharyya, pushpak</th>\n",
       "      <th>gurevych, iryna</th>\n",
       "      <th>category_article</th>\n",
       "      <th>category_inproceedings</th>\n",
       "      <th>category_proceedings</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Philippine Language Resources: Trends and Dire...</td>\n",
       "      <td>2009</td>\n",
       "      <td>Association for Computational Linguistics</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A System for Translating Locative Prepositions...</td>\n",
       "      <td>1991</td>\n",
       "      <td>Association for Computational Linguistics</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Introduction to the Shared Task on Comparing S...</td>\n",
       "      <td>2008</td>\n",
       "      <td>College Publications</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Pynini: A Python library for weighted finite-s...</td>\n",
       "      <td>2016</td>\n",
       "      <td>Association for Computational Linguistics</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Improving Readability of Swedish Electronic He...</td>\n",
       "      <td>2014</td>\n",
       "      <td>Association for Computational Linguistics</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65909</th>\n",
       "      <td>Optimizing the weighted sequence alignment alg...</td>\n",
       "      <td>2022</td>\n",
       "      <td>Association for Computational Linguistics</td>\n",
       "      <td>We present an optimized implementation of the ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65910</th>\n",
       "      <td>Proceedings of the 25th Conference on Computat...</td>\n",
       "      <td>2021</td>\n",
       "      <td>Association for Computational Linguistics</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65911</th>\n",
       "      <td>A Large-Scale Pseudoword-Based Evaluation Fram...</td>\n",
       "      <td>2014</td>\n",
       "      <td>MIT Press</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65912</th>\n",
       "      <td>CIST System for CL-SciSumm 2016 Shared Task</td>\n",
       "      <td>2016</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65913</th>\n",
       "      <td>Ontology Engineering and Knowledge Extraction ...</td>\n",
       "      <td>2009</td>\n",
       "      <td>Association for Computational Linguistics</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>65914 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   title  year  \\\n",
       "0      Philippine Language Resources: Trends and Dire...  2009   \n",
       "1      A System for Translating Locative Prepositions...  1991   \n",
       "2      Introduction to the Shared Task on Comparing S...  2008   \n",
       "3      Pynini: A Python library for weighted finite-s...  2016   \n",
       "4      Improving Readability of Swedish Electronic He...  2014   \n",
       "...                                                  ...   ...   \n",
       "65909  Optimizing the weighted sequence alignment alg...  2022   \n",
       "65910  Proceedings of the 25th Conference on Computat...  2021   \n",
       "65911  A Large-Scale Pseudoword-Based Evaluation Fram...  2014   \n",
       "65912        CIST System for CL-SciSumm 2016 Shared Task  2016   \n",
       "65913  Ontology Engineering and Knowledge Extraction ...  2009   \n",
       "\n",
       "                                       publisher  \\\n",
       "0      Association for Computational Linguistics   \n",
       "1      Association for Computational Linguistics   \n",
       "2                           College Publications   \n",
       "3      Association for Computational Linguistics   \n",
       "4      Association for Computational Linguistics   \n",
       "...                                          ...   \n",
       "65909  Association for Computational Linguistics   \n",
       "65910  Association for Computational Linguistics   \n",
       "65911                                  MIT Press   \n",
       "65912                                       None   \n",
       "65913  Association for Computational Linguistics   \n",
       "\n",
       "                                                abstract  \\\n",
       "0                                                   None   \n",
       "1                                                   None   \n",
       "2                                                   None   \n",
       "3                                                   None   \n",
       "4                                                   None   \n",
       "...                                                  ...   \n",
       "65909  We present an optimized implementation of the ...   \n",
       "65910                                               None   \n",
       "65911                                               None   \n",
       "65912                                               None   \n",
       "65913                                               None   \n",
       "\n",
       "       bhattacharyya, pushpak  gurevych, iryna  category_article  \\\n",
       "0                           0                0                 0   \n",
       "1                           0                0                 0   \n",
       "2                           0                0                 0   \n",
       "3                           0                0                 0   \n",
       "4                           0                0                 0   \n",
       "...                       ...              ...               ...   \n",
       "65909                       0                0                 0   \n",
       "65910                       0                0                 0   \n",
       "65911                       0                0                 1   \n",
       "65912                       0                0                 0   \n",
       "65913                       0                0                 0   \n",
       "\n",
       "       category_inproceedings  category_proceedings  \n",
       "0                           1                     0  \n",
       "1                           1                     0  \n",
       "2                           1                     0  \n",
       "3                           1                     0  \n",
       "4                           1                     0  \n",
       "...                       ...                   ...  \n",
       "65909                       1                     0  \n",
       "65910                       0                     1  \n",
       "65911                       0                     0  \n",
       "65912                       1                     0  \n",
       "65913                       1                     0  \n",
       "\n",
       "[65914 rows x 9 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Drop all redundant columns\n",
    "train_2 = train_2.drop(['ENTRYTYPE'], axis=1)\n",
    "\n",
    "# Concatenate the original with dropped redundants and the extracted features for ENTRYTYPE\n",
    "train_3 = pd.concat([train_2, train_encoded_entrytype], axis=1).reindex(train_2.index)\n",
    "train_3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.4 Vectorize 'Publisher' column (116-categorical variable one-hot encoded, and reduced to X most frequent publishers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>publisher_Association for Computational Linguistics</th>\n",
       "      <th>publisher_European Language Resources Association (ELRA)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4456</td>\n",
       "      <td>37526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>37526</td>\n",
       "      <td>4456</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   publisher_Association for Computational Linguistics  \\\n",
       "0                                               4456     \n",
       "1                                              37526     \n",
       "\n",
       "   publisher_European Language Resources Association (ELRA)  \n",
       "0                                              37526         \n",
       "1                                               4456         "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set the number of most frequent publishers to include\n",
    "n_mostfreq_publishers = 2  # Adjust this value as needed\n",
    "\n",
    "# Get the X most frequent publishers\n",
    "top_publishers = train_3['publisher'].value_counts().head(n_mostfreq_publishers).index.tolist()\n",
    "\n",
    "# Create a new DataFrame with one-hot encoding for the X most frequent publishers\n",
    "train_encoded_publisher = pd.get_dummies(train_3['publisher'][train_3['publisher'].isin(top_publishers)], prefix='publisher')\n",
    "\n",
    "# Show count-values for each of the columns\n",
    "train_encoded_publisher.apply(lambda x: x.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>year</th>\n",
       "      <th>abstract</th>\n",
       "      <th>bhattacharyya, pushpak</th>\n",
       "      <th>gurevych, iryna</th>\n",
       "      <th>category_article</th>\n",
       "      <th>category_inproceedings</th>\n",
       "      <th>category_proceedings</th>\n",
       "      <th>publisher_Association for Computational Linguistics</th>\n",
       "      <th>publisher_European Language Resources Association (ELRA)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Philippine Language Resources: Trends and Dire...</td>\n",
       "      <td>2009</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A System for Translating Locative Prepositions...</td>\n",
       "      <td>1991</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Introduction to the Shared Task on Comparing S...</td>\n",
       "      <td>2008</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Pynini: A Python library for weighted finite-s...</td>\n",
       "      <td>2016</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Improving Readability of Swedish Electronic He...</td>\n",
       "      <td>2014</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65909</th>\n",
       "      <td>Optimizing the weighted sequence alignment alg...</td>\n",
       "      <td>2022</td>\n",
       "      <td>We present an optimized implementation of the ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65910</th>\n",
       "      <td>Proceedings of the 25th Conference on Computat...</td>\n",
       "      <td>2021</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65911</th>\n",
       "      <td>A Large-Scale Pseudoword-Based Evaluation Fram...</td>\n",
       "      <td>2014</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65912</th>\n",
       "      <td>CIST System for CL-SciSumm 2016 Shared Task</td>\n",
       "      <td>2016</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65913</th>\n",
       "      <td>Ontology Engineering and Knowledge Extraction ...</td>\n",
       "      <td>2009</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>65914 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   title  year  \\\n",
       "0      Philippine Language Resources: Trends and Dire...  2009   \n",
       "1      A System for Translating Locative Prepositions...  1991   \n",
       "2      Introduction to the Shared Task on Comparing S...  2008   \n",
       "3      Pynini: A Python library for weighted finite-s...  2016   \n",
       "4      Improving Readability of Swedish Electronic He...  2014   \n",
       "...                                                  ...   ...   \n",
       "65909  Optimizing the weighted sequence alignment alg...  2022   \n",
       "65910  Proceedings of the 25th Conference on Computat...  2021   \n",
       "65911  A Large-Scale Pseudoword-Based Evaluation Fram...  2014   \n",
       "65912        CIST System for CL-SciSumm 2016 Shared Task  2016   \n",
       "65913  Ontology Engineering and Knowledge Extraction ...  2009   \n",
       "\n",
       "                                                abstract  \\\n",
       "0                                                      0   \n",
       "1                                                      0   \n",
       "2                                                      0   \n",
       "3                                                      0   \n",
       "4                                                      0   \n",
       "...                                                  ...   \n",
       "65909  We present an optimized implementation of the ...   \n",
       "65910                                                  0   \n",
       "65911                                                  0   \n",
       "65912                                                  0   \n",
       "65913                                                  0   \n",
       "\n",
       "       bhattacharyya, pushpak  gurevych, iryna  category_article  \\\n",
       "0                           0                0                 0   \n",
       "1                           0                0                 0   \n",
       "2                           0                0                 0   \n",
       "3                           0                0                 0   \n",
       "4                           0                0                 0   \n",
       "...                       ...              ...               ...   \n",
       "65909                       0                0                 0   \n",
       "65910                       0                0                 0   \n",
       "65911                       0                0                 1   \n",
       "65912                       0                0                 0   \n",
       "65913                       0                0                 0   \n",
       "\n",
       "       category_inproceedings  category_proceedings  \\\n",
       "0                           1                     0   \n",
       "1                           1                     0   \n",
       "2                           1                     0   \n",
       "3                           1                     0   \n",
       "4                           1                     0   \n",
       "...                       ...                   ...   \n",
       "65909                       1                     0   \n",
       "65910                       0                     1   \n",
       "65911                       0                     0   \n",
       "65912                       1                     0   \n",
       "65913                       1                     0   \n",
       "\n",
       "       publisher_Association for Computational Linguistics  \\\n",
       "0                                                    1.0     \n",
       "1                                                    1.0     \n",
       "2                                                    0.0     \n",
       "3                                                    1.0     \n",
       "4                                                    1.0     \n",
       "...                                                  ...     \n",
       "65909                                                1.0     \n",
       "65910                                                1.0     \n",
       "65911                                                0.0     \n",
       "65912                                                0.0     \n",
       "65913                                                1.0     \n",
       "\n",
       "       publisher_European Language Resources Association (ELRA)  \n",
       "0                                                    0.0         \n",
       "1                                                    0.0         \n",
       "2                                                    0.0         \n",
       "3                                                    0.0         \n",
       "4                                                    0.0         \n",
       "...                                                  ...         \n",
       "65909                                                0.0         \n",
       "65910                                                0.0         \n",
       "65911                                                0.0         \n",
       "65912                                                0.0         \n",
       "65913                                                0.0         \n",
       "\n",
       "[65914 rows x 10 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Drop all redundant columns\n",
    "train_3 = train_3.drop(['publisher'], axis=1)\n",
    "\n",
    "# Concatenate the original with dropped redundants and the extracted features for ENTRYTYPE\n",
    "train_4 = pd.concat([train_3, train_encoded_publisher], axis=1).reindex(train_3.index).fillna(0)\n",
    "train_4\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.5.1 Vectorize 'title' and 'abstract' column (English-translated with stop-words removal and/or synonym replacement)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from googletrans import Translator\n",
    "from langdetect import detect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def det(x):\n",
    "    try:\n",
    "        lang = detect(x)\n",
    "    except:\n",
    "        lang = 'Other'\n",
    "    return lang"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Assign DataFrame to title_tfidf_df\n",
    "train_sample_translated = train_sample.copy()\n",
    "\n",
    "train_sample_translated['language_title'] = train_sample_translated['title'].apply(det)\n",
    "train_sample_translated['language_abstract'] = train_sample_translated['abstract'].apply(det)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "en    6304\n",
       "fr     166\n",
       "it      26\n",
       "de      17\n",
       "ca      15\n",
       "nl      14\n",
       "ro      11\n",
       "da       9\n",
       "tl       9\n",
       "pt       4\n",
       "af       4\n",
       "no       3\n",
       "id       3\n",
       "es       3\n",
       "cy       1\n",
       "hr       1\n",
       "et       1\n",
       "Name: language_title, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_sample_translated['language_title'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Other    3421\n",
       "en       3040\n",
       "fr        117\n",
       "zh-cn      12\n",
       "no          1\n",
       "Name: language_abstract, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_sample_translated['language_abstract'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "translator = Translator(service_urls=['translate.googleapis.com'])\n",
    "\n",
    "# Function to translate non-English titles to English based on 'translated_title' column\n",
    "def translate_to_english(dataframe, column, translated_column):\n",
    "    for i in dataframe[column].index:\n",
    "        # Check if the value in 'translated_title' is not 'en' or 'Other' before translation\n",
    "        if dataframe[translated_column][i] not in ['en', 'Other']:\n",
    "            dataframe[column][i] = translator.translate(dataframe[column][i], dest='en').text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the translation function\n",
    "translate_to_english(train_sample_translated, 'title', 'language_title')\n",
    "translate_to_english(train_sample_translated, 'abstract', 'language_abstract')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sample_translated['new_language_title'] = train_sample_translated['title'].apply(det)\n",
    "train_sample_translated['new_language_abstract'] = train_sample_translated['abstract'].apply(det)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "en    6465\n",
       "it      20\n",
       "ca      18\n",
       "fr      17\n",
       "de      15\n",
       "nl      12\n",
       "tl      11\n",
       "ro      11\n",
       "da       7\n",
       "id       6\n",
       "af       3\n",
       "es       2\n",
       "cy       1\n",
       "sv       1\n",
       "pt       1\n",
       "et       1\n",
       "Name: new_language_title, dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_sample_translated['new_language_title'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Other    3421\n",
       "en       3170\n",
       "Name: new_language_abstract, dtype: int64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_sample_translated['new_language_abstract'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sample_translated = train_sample_translated.fillna(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We've transformed the 'title' column to a dataframe of 100 columns.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>alignment</th>\n",
       "      <th>analysis</th>\n",
       "      <th>annotation</th>\n",
       "      <th>answering</th>\n",
       "      <th>approach</th>\n",
       "      <th>arabic</th>\n",
       "      <th>automatic</th>\n",
       "      <th>based</th>\n",
       "      <th>case</th>\n",
       "      <th>chinese</th>\n",
       "      <th>...</th>\n",
       "      <th>task</th>\n",
       "      <th>text</th>\n",
       "      <th>texts</th>\n",
       "      <th>training</th>\n",
       "      <th>translation</th>\n",
       "      <th>understanding</th>\n",
       "      <th>unsupervised</th>\n",
       "      <th>using</th>\n",
       "      <th>word</th>\n",
       "      <th>workshop</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.495148</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.333044</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.533204</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.650351</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 100 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   alignment  analysis  annotation  answering  approach  arabic  automatic  \\\n",
       "0        0.0  0.000000         0.0   0.495148       0.0     0.0        0.0   \n",
       "1        0.0  0.533204         0.0   0.000000       0.0     0.0        0.0   \n",
       "2        0.0  0.000000         0.0   0.000000       0.0     0.0        0.0   \n",
       "3        0.0  0.000000         0.0   0.000000       0.0     0.0        0.0   \n",
       "4        0.0  0.000000         0.0   0.000000       0.0     0.0        0.0   \n",
       "\n",
       "      based  case  chinese  ...  task      text  texts  training  translation  \\\n",
       "0  0.333044   0.0      0.0  ...   0.0  0.000000    0.0       0.0          0.0   \n",
       "1  0.000000   0.0      0.0  ...   0.0  0.000000    0.0       0.0          0.0   \n",
       "2  0.000000   0.0      0.0  ...   0.0  0.000000    0.0       0.0          0.0   \n",
       "3  0.000000   0.0      0.0  ...   0.0  0.650351    0.0       0.0          0.0   \n",
       "4  0.000000   0.0      0.0  ...   0.0  0.000000    0.0       0.0          0.0   \n",
       "\n",
       "   understanding  unsupervised  using  word  workshop  \n",
       "0            0.0           0.0    0.0   0.0       0.0  \n",
       "1            0.0           0.0    0.0   0.0       0.0  \n",
       "2            0.0           0.0    0.0   0.0       0.0  \n",
       "3            0.0           0.0    0.0   0.0       0.0  \n",
       "4            0.0           0.0    0.0   0.0       0.0  \n",
       "\n",
       "[5 rows x 100 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Specify the number of most frequent tokens you want to keep (replace X with the desired value)\n",
    "max_features_title = 100\n",
    "\n",
    "# Create a list of English stopwords\n",
    "stop_words = 'english'\n",
    "\n",
    "# Apply the TF-IDF vectorizer to column 'title' with max_features parameter\n",
    "tfidf_vectorizer = TfidfVectorizer(stop_words=stop_words, max_features=max_features_title)\n",
    "\n",
    "# Apply the TF-IDF vectorizer to column 'title'\n",
    "tfidf_matrix_title = tfidf_vectorizer.fit_transform(train_sample_translated['title'])\n",
    "\n",
    "# Extract and create columns\n",
    "feature_names = tfidf_vectorizer.get_feature_names_out()\n",
    "tfidf_df_title = pd.DataFrame(tfidf_matrix_title.toarray(), columns=feature_names)\n",
    "\n",
    "print(f\"We've transformed the 'title' column to a dataframe of {len(tfidf_df_title.columns)} columns.\")\n",
    "tfidf_df_title.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We've transformed the 'abstract' column to a dataframe of 100 columns.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>analysis</th>\n",
       "      <th>annotated</th>\n",
       "      <th>annotation</th>\n",
       "      <th>approach</th>\n",
       "      <th>approaches</th>\n",
       "      <th>art</th>\n",
       "      <th>attention</th>\n",
       "      <th>automatic</th>\n",
       "      <th>available</th>\n",
       "      <th>based</th>\n",
       "      <th>...</th>\n",
       "      <th>time</th>\n",
       "      <th>trained</th>\n",
       "      <th>training</th>\n",
       "      <th>translation</th>\n",
       "      <th>use</th>\n",
       "      <th>used</th>\n",
       "      <th>using</th>\n",
       "      <th>word</th>\n",
       "      <th>words</th>\n",
       "      <th>work</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.363133</td>\n",
       "      <td>0.21556</td>\n",
       "      <td>0.186109</td>\n",
       "      <td>0.233257</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.147688</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.18771</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.176033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 100 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   analysis  annotated  annotation  approach  approaches       art  attention  \\\n",
       "0       0.0        0.0         0.0  0.000000     0.00000  0.000000   0.000000   \n",
       "1       0.0        0.0         0.0  0.000000     0.00000  0.000000   0.000000   \n",
       "2       0.0        0.0         0.0  0.000000     0.00000  0.000000   0.000000   \n",
       "3       0.0        0.0         0.0  0.363133     0.21556  0.186109   0.233257   \n",
       "4       0.0        0.0         0.0  0.000000     0.00000  0.000000   0.000000   \n",
       "\n",
       "   automatic  available     based  ...  time  trained  training  translation  \\\n",
       "0        0.0        0.0  0.000000  ...   0.0      0.0       0.0          0.0   \n",
       "1        0.0        0.0  0.000000  ...   0.0      0.0       0.0          0.0   \n",
       "2        0.0        0.0  0.000000  ...   0.0      0.0       0.0          0.0   \n",
       "3        0.0        0.0  0.147688  ...   0.0      0.0       0.0          0.0   \n",
       "4        0.0        0.0  0.000000  ...   0.0      0.0       0.0          0.0   \n",
       "\n",
       "       use  used  using  word  words      work  \n",
       "0  0.00000   0.0    0.0   0.0    0.0  0.000000  \n",
       "1  0.00000   0.0    0.0   0.0    0.0  0.000000  \n",
       "2  0.00000   0.0    0.0   0.0    0.0  0.000000  \n",
       "3  0.18771   0.0    0.0   0.0    0.0  0.176033  \n",
       "4  0.00000   0.0    0.0   0.0    0.0  0.000000  \n",
       "\n",
       "[5 rows x 100 columns]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Specify the number of most frequent tokens you want to keep (replace X with the desired value)\n",
    "max_features_abstract = 100\n",
    "\n",
    "# Create a list of English stopwords\n",
    "stop_words = 'english'\n",
    "\n",
    "# Apply the TF-IDF vectorizer to column 'title' with max_features parameter\n",
    "tfidf_vectorizer = TfidfVectorizer(stop_words=stop_words, max_features=max_features_abstract)\n",
    "\n",
    "# Apply the TF-IDF vectorizer to column 'title'\n",
    "tfidf_matrix_abstract = tfidf_vectorizer.fit_transform(train_sample_translated['abstract'])\n",
    "\n",
    "# Extract and create columns\n",
    "feature_names = tfidf_vectorizer.get_feature_names_out()\n",
    "tfidf_df_abstract = pd.DataFrame(tfidf_matrix_abstract.toarray(), columns=feature_names)\n",
    "\n",
    "print(f\"We've transformed the 'abstract' column to a dataframe of {len(tfidf_df_abstract.columns)} columns.\")\n",
    "tfidf_df_abstract.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.5.2 Extract topics from 'title' and 'abstract' column (English-translated with stop-words removal and/or synonym replacement)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Incorporate into baseline code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Loading training/test data\n",
      "INFO:root:Splitting validation\n",
      "INFO:root:Fitting model with featurizer 1\n",
      "INFO:root:Evaluating on validation data\n",
      "INFO:root:Ridge regress MAE with featurizer 1 (5-fold cross-validated): 5.773010450586702\n",
      "INFO:root:Fitting model with featurizer 2\n",
      "INFO:root:Evaluating on validation data\n",
      "INFO:root:Ridge regress MAE with featurizer 2 (5-fold cross-validated): 5.384430333156983\n",
      "INFO:root:Fitting model with featurizer 3\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'int' object has no attribute 'lower'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Gebruiker\\MLChallenge\\notebooks\\Main experiment\\Experiments.ipynb Cell 39\u001b[0m line \u001b[0;36m3\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Gebruiker/MLChallenge/notebooks/Main%20experiment/Experiments.ipynb#X53sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m \u001b[39m# Drop target variable column and fit both models\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Gebruiker/MLChallenge/notebooks/Main%20experiment/Experiments.ipynb#X53sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m logging\u001b[39m.\u001b[39minfo(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mFitting model with featurizer \u001b[39m\u001b[39m{\u001b[39;00mi\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Gebruiker/MLChallenge/notebooks/Main%20experiment/Experiments.ipynb#X53sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m ridge_cv\u001b[39m.\u001b[39;49mfit(train_4\u001b[39m.\u001b[39;49mdrop(\u001b[39m'\u001b[39;49m\u001b[39myear\u001b[39;49m\u001b[39m'\u001b[39;49m, axis\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m), train_4[\u001b[39m'\u001b[39;49m\u001b[39myear\u001b[39;49m\u001b[39m'\u001b[39;49m]\u001b[39m.\u001b[39;49mvalues)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Gebruiker/MLChallenge/notebooks/Main%20experiment/Experiments.ipynb#X53sZmlsZQ%3D%3D?line=32'>33</a>\u001b[0m \u001b[39m# Calculate and report both MAE's\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Gebruiker/MLChallenge/notebooks/Main%20experiment/Experiments.ipynb#X53sZmlsZQ%3D%3D?line=33'>34</a>\u001b[0m logging\u001b[39m.\u001b[39minfo(\u001b[39m\"\u001b[39m\u001b[39mEvaluating on validation data\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Gebruiker\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\base.py:1151\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1144\u001b[0m     estimator\u001b[39m.\u001b[39m_validate_params()\n\u001b[0;32m   1146\u001b[0m \u001b[39mwith\u001b[39;00m config_context(\n\u001b[0;32m   1147\u001b[0m     skip_parameter_validation\u001b[39m=\u001b[39m(\n\u001b[0;32m   1148\u001b[0m         prefer_skip_nested_validation \u001b[39mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1149\u001b[0m     )\n\u001b[0;32m   1150\u001b[0m ):\n\u001b[1;32m-> 1151\u001b[0m     \u001b[39mreturn\u001b[39;00m fit_method(estimator, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Gebruiker\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\pipeline.py:416\u001b[0m, in \u001b[0;36mPipeline.fit\u001b[1;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[0;32m    390\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Fit the model.\u001b[39;00m\n\u001b[0;32m    391\u001b[0m \n\u001b[0;32m    392\u001b[0m \u001b[39mFit all the transformers one after the other and transform the\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    413\u001b[0m \u001b[39m    Pipeline with fitted steps.\u001b[39;00m\n\u001b[0;32m    414\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    415\u001b[0m fit_params_steps \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_fit_params(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfit_params)\n\u001b[1;32m--> 416\u001b[0m Xt \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fit(X, y, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mfit_params_steps)\n\u001b[0;32m    417\u001b[0m \u001b[39mwith\u001b[39;00m _print_elapsed_time(\u001b[39m\"\u001b[39m\u001b[39mPipeline\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_log_message(\u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msteps) \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m)):\n\u001b[0;32m    418\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_final_estimator \u001b[39m!=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mpassthrough\u001b[39m\u001b[39m\"\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\Gebruiker\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\pipeline.py:370\u001b[0m, in \u001b[0;36mPipeline._fit\u001b[1;34m(self, X, y, **fit_params_steps)\u001b[0m\n\u001b[0;32m    368\u001b[0m     cloned_transformer \u001b[39m=\u001b[39m clone(transformer)\n\u001b[0;32m    369\u001b[0m \u001b[39m# Fit or load from cache the current transformer\u001b[39;00m\n\u001b[1;32m--> 370\u001b[0m X, fitted_transformer \u001b[39m=\u001b[39m fit_transform_one_cached(\n\u001b[0;32m    371\u001b[0m     cloned_transformer,\n\u001b[0;32m    372\u001b[0m     X,\n\u001b[0;32m    373\u001b[0m     y,\n\u001b[0;32m    374\u001b[0m     \u001b[39mNone\u001b[39;49;00m,\n\u001b[0;32m    375\u001b[0m     message_clsname\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mPipeline\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m    376\u001b[0m     message\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_log_message(step_idx),\n\u001b[0;32m    377\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mfit_params_steps[name],\n\u001b[0;32m    378\u001b[0m )\n\u001b[0;32m    379\u001b[0m \u001b[39m# Replace the transformer of the step with the fitted\u001b[39;00m\n\u001b[0;32m    380\u001b[0m \u001b[39m# transformer. This is necessary when loading the transformer\u001b[39;00m\n\u001b[0;32m    381\u001b[0m \u001b[39m# from the cache.\u001b[39;00m\n\u001b[0;32m    382\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msteps[step_idx] \u001b[39m=\u001b[39m (name, fitted_transformer)\n",
      "File \u001b[1;32mc:\\Users\\Gebruiker\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\joblib\\memory.py:353\u001b[0m, in \u001b[0;36mNotMemorizedFunc.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    352\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m--> 353\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfunc(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Gebruiker\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\pipeline.py:950\u001b[0m, in \u001b[0;36m_fit_transform_one\u001b[1;34m(transformer, X, y, weight, message_clsname, message, **fit_params)\u001b[0m\n\u001b[0;32m    948\u001b[0m \u001b[39mwith\u001b[39;00m _print_elapsed_time(message_clsname, message):\n\u001b[0;32m    949\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(transformer, \u001b[39m\"\u001b[39m\u001b[39mfit_transform\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m--> 950\u001b[0m         res \u001b[39m=\u001b[39m transformer\u001b[39m.\u001b[39;49mfit_transform(X, y, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mfit_params)\n\u001b[0;32m    951\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    952\u001b[0m         res \u001b[39m=\u001b[39m transformer\u001b[39m.\u001b[39mfit(X, y, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfit_params)\u001b[39m.\u001b[39mtransform(X)\n",
      "File \u001b[1;32mc:\\Users\\Gebruiker\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\_set_output.py:140\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[1;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[0;32m    138\u001b[0m \u001b[39m@wraps\u001b[39m(f)\n\u001b[0;32m    139\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrapped\u001b[39m(\u001b[39mself\u001b[39m, X, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m--> 140\u001b[0m     data_to_wrap \u001b[39m=\u001b[39m f(\u001b[39mself\u001b[39;49m, X, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m    141\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(data_to_wrap, \u001b[39mtuple\u001b[39m):\n\u001b[0;32m    142\u001b[0m         \u001b[39m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[0;32m    143\u001b[0m         return_tuple \u001b[39m=\u001b[39m (\n\u001b[0;32m    144\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[39m0\u001b[39m], X, \u001b[39mself\u001b[39m),\n\u001b[0;32m    145\u001b[0m             \u001b[39m*\u001b[39mdata_to_wrap[\u001b[39m1\u001b[39m:],\n\u001b[0;32m    146\u001b[0m         )\n",
      "File \u001b[1;32mc:\\Users\\Gebruiker\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\base.py:1151\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1144\u001b[0m     estimator\u001b[39m.\u001b[39m_validate_params()\n\u001b[0;32m   1146\u001b[0m \u001b[39mwith\u001b[39;00m config_context(\n\u001b[0;32m   1147\u001b[0m     skip_parameter_validation\u001b[39m=\u001b[39m(\n\u001b[0;32m   1148\u001b[0m         prefer_skip_nested_validation \u001b[39mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1149\u001b[0m     )\n\u001b[0;32m   1150\u001b[0m ):\n\u001b[1;32m-> 1151\u001b[0m     \u001b[39mreturn\u001b[39;00m fit_method(estimator, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Gebruiker\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\compose\\_column_transformer.py:743\u001b[0m, in \u001b[0;36mColumnTransformer.fit_transform\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    740\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_validate_column_callables(X)\n\u001b[0;32m    741\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_validate_remainder(X)\n\u001b[1;32m--> 743\u001b[0m result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fit_transform(X, y, _fit_transform_one)\n\u001b[0;32m    745\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m result:\n\u001b[0;32m    746\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_update_fitted_transformers([])\n",
      "File \u001b[1;32mc:\\Users\\Gebruiker\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\compose\\_column_transformer.py:670\u001b[0m, in \u001b[0;36mColumnTransformer._fit_transform\u001b[1;34m(self, X, y, func, fitted, column_as_strings)\u001b[0m\n\u001b[0;32m    664\u001b[0m transformers \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(\n\u001b[0;32m    665\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_iter(\n\u001b[0;32m    666\u001b[0m         fitted\u001b[39m=\u001b[39mfitted, replace_strings\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, column_as_strings\u001b[39m=\u001b[39mcolumn_as_strings\n\u001b[0;32m    667\u001b[0m     )\n\u001b[0;32m    668\u001b[0m )\n\u001b[0;32m    669\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 670\u001b[0m     \u001b[39mreturn\u001b[39;00m Parallel(n_jobs\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mn_jobs)(\n\u001b[0;32m    671\u001b[0m         delayed(func)(\n\u001b[0;32m    672\u001b[0m             transformer\u001b[39m=\u001b[39;49mclone(trans) \u001b[39mif\u001b[39;49;00m \u001b[39mnot\u001b[39;49;00m fitted \u001b[39melse\u001b[39;49;00m trans,\n\u001b[0;32m    673\u001b[0m             X\u001b[39m=\u001b[39;49m_safe_indexing(X, column, axis\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m),\n\u001b[0;32m    674\u001b[0m             y\u001b[39m=\u001b[39;49my,\n\u001b[0;32m    675\u001b[0m             weight\u001b[39m=\u001b[39;49mweight,\n\u001b[0;32m    676\u001b[0m             message_clsname\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mColumnTransformer\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m    677\u001b[0m             message\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_log_message(name, idx, \u001b[39mlen\u001b[39;49m(transformers)),\n\u001b[0;32m    678\u001b[0m         )\n\u001b[0;32m    679\u001b[0m         \u001b[39mfor\u001b[39;49;00m idx, (name, trans, column, weight) \u001b[39min\u001b[39;49;00m \u001b[39menumerate\u001b[39;49m(transformers, \u001b[39m1\u001b[39;49m)\n\u001b[0;32m    680\u001b[0m     )\n\u001b[0;32m    681\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mValueError\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    682\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mExpected 2D array, got 1D array instead\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m \u001b[39mstr\u001b[39m(e):\n",
      "File \u001b[1;32mc:\\Users\\Gebruiker\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\parallel.py:65\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m     60\u001b[0m config \u001b[39m=\u001b[39m get_config()\n\u001b[0;32m     61\u001b[0m iterable_with_config \u001b[39m=\u001b[39m (\n\u001b[0;32m     62\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[0;32m     63\u001b[0m     \u001b[39mfor\u001b[39;00m delayed_func, args, kwargs \u001b[39min\u001b[39;00m iterable\n\u001b[0;32m     64\u001b[0m )\n\u001b[1;32m---> 65\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__call__\u001b[39;49m(iterable_with_config)\n",
      "File \u001b[1;32mc:\\Users\\Gebruiker\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\joblib\\parallel.py:1863\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1861\u001b[0m     output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_sequential_output(iterable)\n\u001b[0;32m   1862\u001b[0m     \u001b[39mnext\u001b[39m(output)\n\u001b[1;32m-> 1863\u001b[0m     \u001b[39mreturn\u001b[39;00m output \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreturn_generator \u001b[39melse\u001b[39;00m \u001b[39mlist\u001b[39m(output)\n\u001b[0;32m   1865\u001b[0m \u001b[39m# Let's create an ID that uniquely identifies the current call. If the\u001b[39;00m\n\u001b[0;32m   1866\u001b[0m \u001b[39m# call is interrupted early and that the same instance is immediately\u001b[39;00m\n\u001b[0;32m   1867\u001b[0m \u001b[39m# re-used, this id will be used to prevent workers that were\u001b[39;00m\n\u001b[0;32m   1868\u001b[0m \u001b[39m# concurrently finalizing a task from the previous call to run the\u001b[39;00m\n\u001b[0;32m   1869\u001b[0m \u001b[39m# callback.\u001b[39;00m\n\u001b[0;32m   1870\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n",
      "File \u001b[1;32mc:\\Users\\Gebruiker\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\joblib\\parallel.py:1792\u001b[0m, in \u001b[0;36mParallel._get_sequential_output\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1790\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_dispatched_batches \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m   1791\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_dispatched_tasks \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m-> 1792\u001b[0m res \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1793\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_completed_tasks \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m   1794\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprint_progress()\n",
      "File \u001b[1;32mc:\\Users\\Gebruiker\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\parallel.py:127\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    125\u001b[0m     config \u001b[39m=\u001b[39m {}\n\u001b[0;32m    126\u001b[0m \u001b[39mwith\u001b[39;00m config_context(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mconfig):\n\u001b[1;32m--> 127\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfunction(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Gebruiker\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\pipeline.py:950\u001b[0m, in \u001b[0;36m_fit_transform_one\u001b[1;34m(transformer, X, y, weight, message_clsname, message, **fit_params)\u001b[0m\n\u001b[0;32m    948\u001b[0m \u001b[39mwith\u001b[39;00m _print_elapsed_time(message_clsname, message):\n\u001b[0;32m    949\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(transformer, \u001b[39m\"\u001b[39m\u001b[39mfit_transform\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m--> 950\u001b[0m         res \u001b[39m=\u001b[39m transformer\u001b[39m.\u001b[39;49mfit_transform(X, y, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mfit_params)\n\u001b[0;32m    951\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    952\u001b[0m         res \u001b[39m=\u001b[39m transformer\u001b[39m.\u001b[39mfit(X, y, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfit_params)\u001b[39m.\u001b[39mtransform(X)\n",
      "File \u001b[1;32mc:\\Users\\Gebruiker\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\base.py:1151\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1144\u001b[0m     estimator\u001b[39m.\u001b[39m_validate_params()\n\u001b[0;32m   1146\u001b[0m \u001b[39mwith\u001b[39;00m config_context(\n\u001b[0;32m   1147\u001b[0m     skip_parameter_validation\u001b[39m=\u001b[39m(\n\u001b[0;32m   1148\u001b[0m         prefer_skip_nested_validation \u001b[39mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1149\u001b[0m     )\n\u001b[0;32m   1150\u001b[0m ):\n\u001b[1;32m-> 1151\u001b[0m     \u001b[39mreturn\u001b[39;00m fit_method(estimator, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Gebruiker\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:1383\u001b[0m, in \u001b[0;36mCountVectorizer.fit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   1375\u001b[0m             warnings\u001b[39m.\u001b[39mwarn(\n\u001b[0;32m   1376\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39mUpper case characters found in\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1377\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39m vocabulary while \u001b[39m\u001b[39m'\u001b[39m\u001b[39mlowercase\u001b[39m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1378\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39m is True. These entries will not\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1379\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39m be matched with any documents\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1380\u001b[0m             )\n\u001b[0;32m   1381\u001b[0m             \u001b[39mbreak\u001b[39;00m\n\u001b[1;32m-> 1383\u001b[0m vocabulary, X \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_count_vocab(raw_documents, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfixed_vocabulary_)\n\u001b[0;32m   1385\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbinary:\n\u001b[0;32m   1386\u001b[0m     X\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mfill(\u001b[39m1\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Gebruiker\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:1270\u001b[0m, in \u001b[0;36mCountVectorizer._count_vocab\u001b[1;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[0;32m   1268\u001b[0m \u001b[39mfor\u001b[39;00m doc \u001b[39min\u001b[39;00m raw_documents:\n\u001b[0;32m   1269\u001b[0m     feature_counter \u001b[39m=\u001b[39m {}\n\u001b[1;32m-> 1270\u001b[0m     \u001b[39mfor\u001b[39;00m feature \u001b[39min\u001b[39;00m analyze(doc):\n\u001b[0;32m   1271\u001b[0m         \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1272\u001b[0m             feature_idx \u001b[39m=\u001b[39m vocabulary[feature]\n",
      "File \u001b[1;32mc:\\Users\\Gebruiker\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:110\u001b[0m, in \u001b[0;36m_analyze\u001b[1;34m(doc, analyzer, tokenizer, ngrams, preprocessor, decoder, stop_words)\u001b[0m\n\u001b[0;32m    108\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    109\u001b[0m     \u001b[39mif\u001b[39;00m preprocessor \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 110\u001b[0m         doc \u001b[39m=\u001b[39m preprocessor(doc)\n\u001b[0;32m    111\u001b[0m     \u001b[39mif\u001b[39;00m tokenizer \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    112\u001b[0m         doc \u001b[39m=\u001b[39m tokenizer(doc)\n",
      "File \u001b[1;32mc:\\Users\\Gebruiker\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:68\u001b[0m, in \u001b[0;36m_preprocess\u001b[1;34m(doc, accent_function, lower)\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Chain together an optional series of text preprocessing steps to\u001b[39;00m\n\u001b[0;32m     50\u001b[0m \u001b[39mapply to a document.\u001b[39;00m\n\u001b[0;32m     51\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[39m    preprocessed string\u001b[39;00m\n\u001b[0;32m     66\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m     67\u001b[0m \u001b[39mif\u001b[39;00m lower:\n\u001b[1;32m---> 68\u001b[0m     doc \u001b[39m=\u001b[39m doc\u001b[39m.\u001b[39;49mlower()\n\u001b[0;32m     69\u001b[0m \u001b[39mif\u001b[39;00m accent_function \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m     70\u001b[0m     doc \u001b[39m=\u001b[39m accent_function(doc)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'int' object has no attribute 'lower'"
     ]
    }
   ],
   "source": [
    "# Set the logging level to INFO and set loading message\n",
    "logging.getLogger().setLevel(logging.INFO)\n",
    "    \n",
    "# Load train and test sets and change all NA values to empty values\n",
    "logging.info(\"Loading training/test data\")\n",
    "#train = pd.DataFrame.from_records(json.load(open('../data/train.json'))).fillna(\"\")\n",
    "#test = pd.DataFrame.from_records(json.load(open('../data/test.json'))).fillna(\"\")\n",
    "    \n",
    "# Split the train set into train (80%) and validation (20%) sets, 5-folds\n",
    "logging.info(\"Splitting validation\")\n",
    "num_folds = 5\n",
    "k_fold = KFold(n_splits=num_folds, shuffle=True, random_state=123)\n",
    "    \n",
    "# Store a featurizer to transform the 'title' column into a bag-of-words format\n",
    "featurizer_1 = ColumnTransformer(\n",
    "    transformers=[(\"title\", CountVectorizer(), \"title\")], remainder='drop')\n",
    "featurizer_2 = ColumnTransformer(\n",
    "    transformers=[(\"title\", TfidfVectorizer(), \"title\")], remainder='drop')\n",
    "featurizer_3 = ColumnTransformer(\n",
    "    transformers=[(\"abstract\", CountVectorizer(), \"abstract\")], remainder='drop')\n",
    "featurizer_4 = ColumnTransformer(\n",
    "    transformers=[(\"abstract\", TfidfVectorizer(), \"abstract\")], remainder='drop')\n",
    "featurizers = [featurizer_1, featurizer_2, featurizer_3, featurizer_4]\n",
    "\n",
    "for i, featurizer in enumerate(featurizers):\n",
    "    # Make a pipeline for the featurizer and a ridge model, that aims to minimize the sum of squares\n",
    "    ridge_cv = make_pipeline(featurizer, Ridge())\n",
    "    \n",
    "    # Drop target variable column and fit both models\n",
    "    logging.info(f\"Fitting model with featurizer {i+1}\")\n",
    "    ridge_cv.fit(train_4.drop('year', axis=1), train_4['year'].values)\n",
    "    \n",
    "    # Calculate and report both MAE's\n",
    "    logging.info(\"Evaluating on validation data\")\n",
    "    ridge_cv_scores = cross_val_score(ridge_cv, train_4.drop('year', axis=1), train_4['year'].values, cv=k_fold, scoring='neg_mean_absolute_error')\n",
    "    logging.info(f\"Ridge regress MAE with featurizer {i+1} ({num_folds}-fold cross-validated): {-ridge_cv_scores.mean()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This paragraph build upon the previous baseline code. It entails the following adjustments/additions chronologically:\n",
    "\n",
    "- [x] Removal of dummy regressor, since ridge works better from the very start;\n",
    "- [x] 5-fold cross validation to reduce variability (Ridge regress MAE (5.773));\n",
    "- [x] Try sklearn's other feature vectorizers (tf-idf (5.384), ...);\n",
    "- [ ] Perform custom preprocessing, tokenizations within sklearn;\n",
    "- [ ] Tune hyperparameters of feature vectorizers (n-gram size);\n",
    "- [ ] Try tasks other than regression, like lazy learning (kNN)(?);\n",
    "- [ ] Try BERTopic modelling;"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
