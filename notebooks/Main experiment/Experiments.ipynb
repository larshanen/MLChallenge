{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Baseline code provided by uni"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import logging\n",
    "import json\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.dummy import DummyRegressor\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.metrics import mean_absolute_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Baseline function to create [predictions](https://github.com/larshanen/MLChallenge/tree/main/notebooks/predicted.json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    # Set the logging level to INFO and set loading message\n",
    "    logging.getLogger().setLevel(logging.INFO)\n",
    "    \n",
    "    # Load train and test sets and change all NA values to empty values\n",
    "    logging.info(\"Loading training/test data\")\n",
    "    train = pd.DataFrame.from_records(json.load(open('../data/train.json'))).fillna(\"\")\n",
    "    test = pd.DataFrame.from_records(json.load(open('../data/test.json'))).fillna(\"\")\n",
    "    \n",
    "    # Split the train set into train (75%) and validation (25%) sets\n",
    "    logging.info(\"Splitting validation\")\n",
    "    train, val = train_test_split(train, stratify=train['year'], random_state=123)\n",
    "    \n",
    "    # Store a featurizer to transform the 'title' column into a bag-of-words format\n",
    "    featurizer = ColumnTransformer(\n",
    "        transformers=[(\"title\", CountVectorizer(), \"title\")], remainder='drop')\n",
    "    \n",
    "    # Make a pipeline for the featurizer combined with a dummy regressor, that simply predicts the overall trained mean of the target variable\n",
    "    dummy = make_pipeline(featurizer, DummyRegressor(strategy='mean'))\n",
    "\n",
    "    # Make a pipeline for the featurizer and a ridge model, that aims to minimize the sum of squares\n",
    "    ridge = make_pipeline(featurizer, Ridge())\n",
    "    \n",
    "    # Drop target variable column and fit both models\n",
    "    logging.info(\"Fitting models\")\n",
    "    dummy.fit(train.drop('year', axis=1), train['year'].values)\n",
    "    ridge.fit(train.drop('year', axis=1), train['year'].values)\n",
    "    \n",
    "    # Calculate and report both MAE's\n",
    "    logging.info(\"Evaluating on validation data\")\n",
    "    err = mean_absolute_error(val['year'].values, dummy.predict(val.drop('year', axis=1)))\n",
    "    logging.info(f\"Mean baseline MAE: {err}\")\n",
    "    err = mean_absolute_error(val['year'].values, ridge.predict(val.drop('year', axis=1)))\n",
    "    logging.info(f\"Ridge regress MAE: {err}\")\n",
    "    \n",
    "    # Let the ridge model predict on test set\n",
    "    logging.info(f\"Predicting on test\")\n",
    "    pred = ridge.predict(test)\n",
    "    test['year'] = pred\n",
    "    \n",
    "    # Write JSON prediction file\n",
    "    logging.info(\"Writing prediction file\")\n",
    "    test.to_json(\"predicted.json\", orient='records', indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Loading training/test data\n",
      "INFO:root:Splitting validation\n",
      "INFO:root:Fitting models\n",
      "INFO:root:Evaluating on validation data\n",
      "INFO:root:Mean baseline MAE: 7.8054390754858805\n",
      "INFO:root:Ridge regress MAE: 5.812345349001838\n",
      "INFO:root:Predicting on test\n",
      "INFO:root:Writing prediction file\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Team code\n",
    "\n",
    "Please follow the instructions beneath when writing or adjusting code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Describe every piece of code with comments\n",
    "# Include your name in every header so we can report our individual contributions (this is mandatory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Explore baseline performance (Lars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Loading training/test data\n"
     ]
    }
   ],
   "source": [
    "# Set the logging level to INFO and set loading message\n",
    "logging.getLogger().setLevel(logging.INFO)\n",
    "    \n",
    "# Load train and test sets and change all NA values to empty values\n",
    "logging.info(\"Loading training/test data\")\n",
    "train = pd.DataFrame.from_records(json.load(open('../../data/train.json')))\n",
    "test = pd.DataFrame.from_records(json.load(open('../../data/test.json')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Splitting validation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Fitting models\n",
      "INFO:root:Evaluating on validation data\n",
      "INFO:root:Mean baseline MAE: 7.8054390754858805\n",
      "INFO:root:Ridge regress MAE: 5.812345349001838\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "# Split the train set into train (75%) and validation (25%) sets\n",
    "logging.info(\"Splitting validation\")\n",
    "train, val = train_test_split(train, stratify=train['year'], random_state=123)\n",
    "    \n",
    "# Store a featurizer to transform the 'title' column into a bag-of-words format\n",
    "featurizer = ColumnTransformer(\n",
    "transformers=[(\"title\", CountVectorizer(), \"title\")], remainder='drop')\n",
    "    \n",
    "# Make a pipeline for the featurizer combined with a dummy regressor, that simply predicts the overall trained mean of the target variable\n",
    "dummy = make_pipeline(featurizer, DummyRegressor(strategy='mean'))\n",
    "\n",
    "# Make a pipeline for the featurizer and a ridge model, that aims to minimize the sum of squares\n",
    "ridge = make_pipeline(featurizer, Ridge())\n",
    "    \n",
    "# Drop target variable column and fit both models\n",
    "logging.info(\"Fitting models\")\n",
    "dummy.fit(train.drop('year', axis=1), train['year'].values)\n",
    "ridge.fit(train.drop('year', axis=1), train['year'].values)\n",
    "    \n",
    "# Calculate and report both MAE's\n",
    "logging.info(\"Evaluating on validation data\")\n",
    "err = mean_absolute_error(val['year'].values, dummy.predict(val.drop('year', axis=1)))\n",
    "logging.info(f\"Mean baseline MAE: {err}\")\n",
    "err = mean_absolute_error(val['year'].values, ridge.predict(val.drop('year', axis=1)))\n",
    "logging.info(f\"Ridge regress MAE: {err}\")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Preprocessing (Lars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import extra modules\n",
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For experimental purposes we start working with a 10% subset of the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ENTRYTYPE</th>\n",
       "      <th>title</th>\n",
       "      <th>editor</th>\n",
       "      <th>year</th>\n",
       "      <th>publisher</th>\n",
       "      <th>author</th>\n",
       "      <th>abstract</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>12680</th>\n",
       "      <td>inproceedings</td>\n",
       "      <td>Question-Answering Based on Virtually Integrat...</td>\n",
       "      <td>None</td>\n",
       "      <td>2003</td>\n",
       "      <td>Association for Computational Linguistics</td>\n",
       "      <td>[Choi, Key-Sun, Kim, Jae-Ho, Miyazaki, Masaru,...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17292</th>\n",
       "      <td>inproceedings</td>\n",
       "      <td>AMI&amp;ERIC: How to Learn with Naive Bayes and Pr...</td>\n",
       "      <td>None</td>\n",
       "      <td>2013</td>\n",
       "      <td>Association for Computational Linguistics</td>\n",
       "      <td>[Dermouche, Mohamed, Khouas, Leila, Velcin, Ju...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33265</th>\n",
       "      <td>inproceedings</td>\n",
       "      <td>Inducing Gazetteers for Named Entity Recogniti...</td>\n",
       "      <td>None</td>\n",
       "      <td>2008</td>\n",
       "      <td>Association for Computational Linguistics</td>\n",
       "      <td>[Kazama, Jun'ichi, Torisawa, Kentaro]</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52850</th>\n",
       "      <td>inproceedings</td>\n",
       "      <td>Leveraging Explicit Lexico-logical Alignments ...</td>\n",
       "      <td>None</td>\n",
       "      <td>2022</td>\n",
       "      <td>Association for Computational Linguistics</td>\n",
       "      <td>[Sun, Runxin, He, Shizhu, Zhu, Chong, He, Yaoh...</td>\n",
       "      <td>Text-to-SQL aims to parse natural language que...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2298</th>\n",
       "      <td>inproceedings</td>\n",
       "      <td>CLAM: Quickly deploy NLP command-line tools on...</td>\n",
       "      <td>None</td>\n",
       "      <td>2014</td>\n",
       "      <td>Dublin City University and Association for Com...</td>\n",
       "      <td>[van Gompel, Maarten, Reynaert, Martin]</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>505</th>\n",
       "      <td>inproceedings</td>\n",
       "      <td>XUXEN: A Spelling Checker/Corrector for Basque...</td>\n",
       "      <td>None</td>\n",
       "      <td>1992</td>\n",
       "      <td>Association for Computational Linguistics</td>\n",
       "      <td>[Agirre, E., Alegria, I, Arregi, X, Artola, X,...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39223</th>\n",
       "      <td>inproceedings</td>\n",
       "      <td>Towards Building a Spoken Dialogue System for ...</td>\n",
       "      <td>None</td>\n",
       "      <td>2022</td>\n",
       "      <td>European Language Resources Association</td>\n",
       "      <td>[Aicher, Annalena, Gerstenlauer, Nadine, Feust...</td>\n",
       "      <td>Speech interfaces for argumentative dialogue s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14117</th>\n",
       "      <td>inproceedings</td>\n",
       "      <td>Combining Multiple, Large-Scale Resources in a...</td>\n",
       "      <td>None</td>\n",
       "      <td>1998</td>\n",
       "      <td>None</td>\n",
       "      <td>[Jing, Hongyan, McKeown, Kathleen]</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29826</th>\n",
       "      <td>inproceedings</td>\n",
       "      <td>Lying Through One's Teeth: A Study on Verbal L...</td>\n",
       "      <td>None</td>\n",
       "      <td>2021</td>\n",
       "      <td>Association for Computational Linguistics</td>\n",
       "      <td>[Yeh, Min-Hsuan, Ku, Lun-Wei]</td>\n",
       "      <td>Although many studies use the LIWC lexicon to ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12496</th>\n",
       "      <td>inproceedings</td>\n",
       "      <td>Toward an Interagency Language Roundtable Base...</td>\n",
       "      <td>None</td>\n",
       "      <td>2006</td>\n",
       "      <td>Association for Machine Translation in the Ame...</td>\n",
       "      <td>[Jones, Douglas, Anderson, Timothy, Atwell, Sa...</td>\n",
       "      <td>We present observations from three exercises d...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6591 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           ENTRYTYPE                                              title  \\\n",
       "12680  inproceedings  Question-Answering Based on Virtually Integrat...   \n",
       "17292  inproceedings  AMI&ERIC: How to Learn with Naive Bayes and Pr...   \n",
       "33265  inproceedings  Inducing Gazetteers for Named Entity Recogniti...   \n",
       "52850  inproceedings  Leveraging Explicit Lexico-logical Alignments ...   \n",
       "2298   inproceedings  CLAM: Quickly deploy NLP command-line tools on...   \n",
       "...              ...                                                ...   \n",
       "505    inproceedings  XUXEN: A Spelling Checker/Corrector for Basque...   \n",
       "39223  inproceedings  Towards Building a Spoken Dialogue System for ...   \n",
       "14117  inproceedings  Combining Multiple, Large-Scale Resources in a...   \n",
       "29826  inproceedings  Lying Through One's Teeth: A Study on Verbal L...   \n",
       "12496  inproceedings  Toward an Interagency Language Roundtable Base...   \n",
       "\n",
       "      editor  year                                          publisher  \\\n",
       "12680   None  2003          Association for Computational Linguistics   \n",
       "17292   None  2013          Association for Computational Linguistics   \n",
       "33265   None  2008          Association for Computational Linguistics   \n",
       "52850   None  2022          Association for Computational Linguistics   \n",
       "2298    None  2014  Dublin City University and Association for Com...   \n",
       "...      ...   ...                                                ...   \n",
       "505     None  1992          Association for Computational Linguistics   \n",
       "39223   None  2022            European Language Resources Association   \n",
       "14117   None  1998                                               None   \n",
       "29826   None  2021          Association for Computational Linguistics   \n",
       "12496   None  2006  Association for Machine Translation in the Ame...   \n",
       "\n",
       "                                                  author  \\\n",
       "12680  [Choi, Key-Sun, Kim, Jae-Ho, Miyazaki, Masaru,...   \n",
       "17292  [Dermouche, Mohamed, Khouas, Leila, Velcin, Ju...   \n",
       "33265              [Kazama, Jun'ichi, Torisawa, Kentaro]   \n",
       "52850  [Sun, Runxin, He, Shizhu, Zhu, Chong, He, Yaoh...   \n",
       "2298             [van Gompel, Maarten, Reynaert, Martin]   \n",
       "...                                                  ...   \n",
       "505    [Agirre, E., Alegria, I, Arregi, X, Artola, X,...   \n",
       "39223  [Aicher, Annalena, Gerstenlauer, Nadine, Feust...   \n",
       "14117                 [Jing, Hongyan, McKeown, Kathleen]   \n",
       "29826                      [Yeh, Min-Hsuan, Ku, Lun-Wei]   \n",
       "12496  [Jones, Douglas, Anderson, Timothy, Atwell, Sa...   \n",
       "\n",
       "                                                abstract  \n",
       "12680                                               None  \n",
       "17292                                               None  \n",
       "33265                                               None  \n",
       "52850  Text-to-SQL aims to parse natural language que...  \n",
       "2298                                                None  \n",
       "...                                                  ...  \n",
       "505                                                 None  \n",
       "39223  Speech interfaces for argumentative dialogue s...  \n",
       "14117                                               None  \n",
       "29826  Although many studies use the LIWC lexicon to ...  \n",
       "12496  We present observations from three exercises d...  \n",
       "\n",
       "[6591 rows x 7 columns]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Randomly save 10% of the train set for velocity purposes\n",
    "percentage_to_save = 10\n",
    "\n",
    "# Calculate the number of rows to save\n",
    "num_rows_to_save = int(len(train) * (percentage_to_save / 100))\n",
    "\n",
    "# Use the sample method to randomly select rows\n",
    "train_sample = train.sample(n=num_rows_to_save, random_state=42)  # Set a random_state for reproducibility\n",
    "\n",
    "train_sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.1 Drop all columns with over 75% of missing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ENTRYTYPE</th>\n",
       "      <th>title</th>\n",
       "      <th>year</th>\n",
       "      <th>publisher</th>\n",
       "      <th>author</th>\n",
       "      <th>abstract</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>inproceedings</td>\n",
       "      <td>Philippine Language Resources: Trends and Dire...</td>\n",
       "      <td>2009</td>\n",
       "      <td>Association for Computational Linguistics</td>\n",
       "      <td>[Roxas, Rachel Edita, Cheng, Charibeth, Lim, N...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>inproceedings</td>\n",
       "      <td>A System for Translating Locative Prepositions...</td>\n",
       "      <td>1991</td>\n",
       "      <td>Association for Computational Linguistics</td>\n",
       "      <td>[Japkowicz, Nathalie, Wiebe, Janyce M.]</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>inproceedings</td>\n",
       "      <td>Introduction to the Shared Task on Comparing S...</td>\n",
       "      <td>2008</td>\n",
       "      <td>College Publications</td>\n",
       "      <td>[Bos, Johan]</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>inproceedings</td>\n",
       "      <td>Pynini: A Python library for weighted finite-s...</td>\n",
       "      <td>2016</td>\n",
       "      <td>Association for Computational Linguistics</td>\n",
       "      <td>[Gorman, Kyle]</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>inproceedings</td>\n",
       "      <td>Improving Readability of Swedish Electronic He...</td>\n",
       "      <td>2014</td>\n",
       "      <td>Association for Computational Linguistics</td>\n",
       "      <td>[Grigonyte, Gintarė, Kvist, Maria, Velupillai,...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65909</th>\n",
       "      <td>inproceedings</td>\n",
       "      <td>Optimizing the weighted sequence alignment alg...</td>\n",
       "      <td>2022</td>\n",
       "      <td>Association for Computational Linguistics</td>\n",
       "      <td>[Janicki, Maciej]</td>\n",
       "      <td>We present an optimized implementation of the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65910</th>\n",
       "      <td>proceedings</td>\n",
       "      <td>Proceedings of the 25th Conference on Computat...</td>\n",
       "      <td>2021</td>\n",
       "      <td>Association for Computational Linguistics</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65911</th>\n",
       "      <td>article</td>\n",
       "      <td>A Large-Scale Pseudoword-Based Evaluation Fram...</td>\n",
       "      <td>2014</td>\n",
       "      <td>MIT Press</td>\n",
       "      <td>[Pilehvar, Mohammad Taher, Navigli, Roberto]</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65912</th>\n",
       "      <td>inproceedings</td>\n",
       "      <td>CIST System for CL-SciSumm 2016 Shared Task</td>\n",
       "      <td>2016</td>\n",
       "      <td>None</td>\n",
       "      <td>[Li, Lei, Mao, Liyuan, Zhang, Yazhao, Chi, Jun...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65913</th>\n",
       "      <td>inproceedings</td>\n",
       "      <td>Ontology Engineering and Knowledge Extraction ...</td>\n",
       "      <td>2009</td>\n",
       "      <td>Association for Computational Linguistics</td>\n",
       "      <td>[Trapman, Jantine, Monachesi, Paola]</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>65914 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           ENTRYTYPE                                              title  year  \\\n",
       "0      inproceedings  Philippine Language Resources: Trends and Dire...  2009   \n",
       "1      inproceedings  A System for Translating Locative Prepositions...  1991   \n",
       "2      inproceedings  Introduction to the Shared Task on Comparing S...  2008   \n",
       "3      inproceedings  Pynini: A Python library for weighted finite-s...  2016   \n",
       "4      inproceedings  Improving Readability of Swedish Electronic He...  2014   \n",
       "...              ...                                                ...   ...   \n",
       "65909  inproceedings  Optimizing the weighted sequence alignment alg...  2022   \n",
       "65910    proceedings  Proceedings of the 25th Conference on Computat...  2021   \n",
       "65911        article  A Large-Scale Pseudoword-Based Evaluation Fram...  2014   \n",
       "65912  inproceedings        CIST System for CL-SciSumm 2016 Shared Task  2016   \n",
       "65913  inproceedings  Ontology Engineering and Knowledge Extraction ...  2009   \n",
       "\n",
       "                                       publisher  \\\n",
       "0      Association for Computational Linguistics   \n",
       "1      Association for Computational Linguistics   \n",
       "2                           College Publications   \n",
       "3      Association for Computational Linguistics   \n",
       "4      Association for Computational Linguistics   \n",
       "...                                          ...   \n",
       "65909  Association for Computational Linguistics   \n",
       "65910  Association for Computational Linguistics   \n",
       "65911                                  MIT Press   \n",
       "65912                                       None   \n",
       "65913  Association for Computational Linguistics   \n",
       "\n",
       "                                                  author  \\\n",
       "0      [Roxas, Rachel Edita, Cheng, Charibeth, Lim, N...   \n",
       "1                [Japkowicz, Nathalie, Wiebe, Janyce M.]   \n",
       "2                                           [Bos, Johan]   \n",
       "3                                         [Gorman, Kyle]   \n",
       "4      [Grigonyte, Gintarė, Kvist, Maria, Velupillai,...   \n",
       "...                                                  ...   \n",
       "65909                                  [Janicki, Maciej]   \n",
       "65910                                               None   \n",
       "65911       [Pilehvar, Mohammad Taher, Navigli, Roberto]   \n",
       "65912  [Li, Lei, Mao, Liyuan, Zhang, Yazhao, Chi, Jun...   \n",
       "65913               [Trapman, Jantine, Monachesi, Paola]   \n",
       "\n",
       "                                                abstract  \n",
       "0                                                   None  \n",
       "1                                                   None  \n",
       "2                                                   None  \n",
       "3                                                   None  \n",
       "4                                                   None  \n",
       "...                                                  ...  \n",
       "65909  We present an optimized implementation of the ...  \n",
       "65910                                               None  \n",
       "65911                                               None  \n",
       "65912                                               None  \n",
       "65913                                               None  \n",
       "\n",
       "[65914 rows x 6 columns]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set threshold on 75%\n",
    "threshold = 0.25\n",
    "\n",
    "# Calculate the threshold for each column\n",
    "missing_threshold = int(threshold * len(train))\n",
    "\n",
    "# Drop columns with more than the specified percentage of missing data\n",
    "train_filtered = train.dropna(axis=1, thresh=missing_threshold)\n",
    "\n",
    "train_filtered"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.2 Featurize 'author' column (count-vectors, reduced to top X most frequent authors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Gebruiker\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bhattacharyya, pushpak</th>\n",
       "      <th>gurevych, iryna</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65909</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65910</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65911</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65912</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65913</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>65914 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       bhattacharyya, pushpak  gurevych, iryna\n",
       "0                           0                0\n",
       "1                           0                0\n",
       "2                           0                0\n",
       "3                           0                0\n",
       "4                           0                0\n",
       "...                       ...              ...\n",
       "65909                       0                0\n",
       "65910                       0                0\n",
       "65911                       0                0\n",
       "65912                       0                0\n",
       "65913                       0                0\n",
       "\n",
       "[65914 rows x 2 columns]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert lists of strings, accounting for None values\n",
    "train_filtered['author_str'] = train_filtered['author'].apply(lambda x: ';'.join(map(str, x)) if x is not None else 'unknown')\n",
    "\n",
    "# Add a column to store the original row numbers\n",
    "# train_filtered['original_index'] = train_filtered.index\n",
    "\n",
    "# Count the number of papers for each author\n",
    "author_paper_counts = train_filtered['author_str'].str.split(';').explode().value_counts()\n",
    "\n",
    "# Set the number of most frequent authors you want to include\n",
    "n_mostfreq_authors = 3  # Adjust this value to the desired number of most frequent authors\n",
    "\n",
    "# Filter authors based on the X most frequent authors\n",
    "top_authors = author_paper_counts.head(n_mostfreq_authors).index.tolist()\n",
    "\n",
    "# Filter only the top authors in 'author_str'\n",
    "train_filtered['author_str_filtered'] = train_filtered['author_str'].apply(lambda x: ';'.join([author for author in x.split(';') if author in top_authors]))\n",
    "\n",
    "# Count-vectorize 'author_str_filtered'\n",
    "count_vectorizer = CountVectorizer(tokenizer=lambda x: x.split(';'))\n",
    "count_matrix = count_vectorizer.fit_transform(train_filtered['author_str_filtered'])\n",
    "\n",
    "# Extract and create columns\n",
    "feature_names = count_vectorizer.get_feature_names_out()\n",
    "author_count_df = pd.DataFrame(count_matrix.toarray(), columns=feature_names)\n",
    "\n",
    "# Set the original_index column as the index\n",
    "# author_count_df.set_index(train_filtered['original_index'], inplace=True)\n",
    "\n",
    "author_count_df = author_count_df.drop(['unknown', ''], axis=1) # See if this approach always works out\n",
    "author_count_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merge with train_filtered, meaning we drop the author column and then add author_count_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ENTRYTYPE</th>\n",
       "      <th>title</th>\n",
       "      <th>year</th>\n",
       "      <th>publisher</th>\n",
       "      <th>abstract</th>\n",
       "      <th>bhattacharyya, pushpak</th>\n",
       "      <th>gurevych, iryna</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>inproceedings</td>\n",
       "      <td>Philippine Language Resources: Trends and Dire...</td>\n",
       "      <td>2009</td>\n",
       "      <td>Association for Computational Linguistics</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>inproceedings</td>\n",
       "      <td>A System for Translating Locative Prepositions...</td>\n",
       "      <td>1991</td>\n",
       "      <td>Association for Computational Linguistics</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>inproceedings</td>\n",
       "      <td>Introduction to the Shared Task on Comparing S...</td>\n",
       "      <td>2008</td>\n",
       "      <td>College Publications</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>inproceedings</td>\n",
       "      <td>Pynini: A Python library for weighted finite-s...</td>\n",
       "      <td>2016</td>\n",
       "      <td>Association for Computational Linguistics</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>inproceedings</td>\n",
       "      <td>Improving Readability of Swedish Electronic He...</td>\n",
       "      <td>2014</td>\n",
       "      <td>Association for Computational Linguistics</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65909</th>\n",
       "      <td>inproceedings</td>\n",
       "      <td>Optimizing the weighted sequence alignment alg...</td>\n",
       "      <td>2022</td>\n",
       "      <td>Association for Computational Linguistics</td>\n",
       "      <td>We present an optimized implementation of the ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65910</th>\n",
       "      <td>proceedings</td>\n",
       "      <td>Proceedings of the 25th Conference on Computat...</td>\n",
       "      <td>2021</td>\n",
       "      <td>Association for Computational Linguistics</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65911</th>\n",
       "      <td>article</td>\n",
       "      <td>A Large-Scale Pseudoword-Based Evaluation Fram...</td>\n",
       "      <td>2014</td>\n",
       "      <td>MIT Press</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65912</th>\n",
       "      <td>inproceedings</td>\n",
       "      <td>CIST System for CL-SciSumm 2016 Shared Task</td>\n",
       "      <td>2016</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65913</th>\n",
       "      <td>inproceedings</td>\n",
       "      <td>Ontology Engineering and Knowledge Extraction ...</td>\n",
       "      <td>2009</td>\n",
       "      <td>Association for Computational Linguistics</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>65914 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           ENTRYTYPE                                              title  year  \\\n",
       "0      inproceedings  Philippine Language Resources: Trends and Dire...  2009   \n",
       "1      inproceedings  A System for Translating Locative Prepositions...  1991   \n",
       "2      inproceedings  Introduction to the Shared Task on Comparing S...  2008   \n",
       "3      inproceedings  Pynini: A Python library for weighted finite-s...  2016   \n",
       "4      inproceedings  Improving Readability of Swedish Electronic He...  2014   \n",
       "...              ...                                                ...   ...   \n",
       "65909  inproceedings  Optimizing the weighted sequence alignment alg...  2022   \n",
       "65910    proceedings  Proceedings of the 25th Conference on Computat...  2021   \n",
       "65911        article  A Large-Scale Pseudoword-Based Evaluation Fram...  2014   \n",
       "65912  inproceedings        CIST System for CL-SciSumm 2016 Shared Task  2016   \n",
       "65913  inproceedings  Ontology Engineering and Knowledge Extraction ...  2009   \n",
       "\n",
       "                                       publisher  \\\n",
       "0      Association for Computational Linguistics   \n",
       "1      Association for Computational Linguistics   \n",
       "2                           College Publications   \n",
       "3      Association for Computational Linguistics   \n",
       "4      Association for Computational Linguistics   \n",
       "...                                          ...   \n",
       "65909  Association for Computational Linguistics   \n",
       "65910  Association for Computational Linguistics   \n",
       "65911                                  MIT Press   \n",
       "65912                                       None   \n",
       "65913  Association for Computational Linguistics   \n",
       "\n",
       "                                                abstract  \\\n",
       "0                                                   None   \n",
       "1                                                   None   \n",
       "2                                                   None   \n",
       "3                                                   None   \n",
       "4                                                   None   \n",
       "...                                                  ...   \n",
       "65909  We present an optimized implementation of the ...   \n",
       "65910                                               None   \n",
       "65911                                               None   \n",
       "65912                                               None   \n",
       "65913                                               None   \n",
       "\n",
       "       bhattacharyya, pushpak  gurevych, iryna  \n",
       "0                           0                0  \n",
       "1                           0                0  \n",
       "2                           0                0  \n",
       "3                           0                0  \n",
       "4                           0                0  \n",
       "...                       ...              ...  \n",
       "65909                       0                0  \n",
       "65910                       0                0  \n",
       "65911                       0                0  \n",
       "65912                       0                0  \n",
       "65913                       0                0  \n",
       "\n",
       "[65914 rows x 7 columns]"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Drop all redundant columns\n",
    "train_filtered_buffer = train_filtered.drop(['author', 'author_str', 'author_str_filtered'], axis=1)\n",
    "\n",
    "# Concatenate the original with dropped redundants and the extracted features for author\n",
    "train_2 = pd.concat([train_filtered_buffer, author_count_df], axis=1).reindex(train_filtered_buffer.index)\n",
    "train_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.3 Vectorize 'ENTRYTYPE' column (3-categorical variable one-hot encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category_article</th>\n",
       "      <th>category_inproceedings</th>\n",
       "      <th>category_proceedings</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>63840</td>\n",
       "      <td>3948</td>\n",
       "      <td>64040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2074</td>\n",
       "      <td>61966</td>\n",
       "      <td>1874</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   category_article  category_inproceedings  category_proceedings\n",
       "0             63840                    3948                 64040\n",
       "1              2074                   61966                  1874"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Perform one-hot encoding\n",
    "train_encoded_entrytype = pd.get_dummies(train_2['ENTRYTYPE'], columns=['category'], prefix='category')\n",
    "\n",
    "# Show count-values for each of the columns\n",
    "train_encoded_entrytype.apply(lambda x: x.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merge with train_filtered, meaning we drop the ENTRYTYPE column and then add train_encoded_entrytype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>year</th>\n",
       "      <th>publisher</th>\n",
       "      <th>abstract</th>\n",
       "      <th>bhattacharyya, pushpak</th>\n",
       "      <th>gurevych, iryna</th>\n",
       "      <th>category_article</th>\n",
       "      <th>category_inproceedings</th>\n",
       "      <th>category_proceedings</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Philippine Language Resources: Trends and Dire...</td>\n",
       "      <td>2009</td>\n",
       "      <td>Association for Computational Linguistics</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A System for Translating Locative Prepositions...</td>\n",
       "      <td>1991</td>\n",
       "      <td>Association for Computational Linguistics</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Introduction to the Shared Task on Comparing S...</td>\n",
       "      <td>2008</td>\n",
       "      <td>College Publications</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Pynini: A Python library for weighted finite-s...</td>\n",
       "      <td>2016</td>\n",
       "      <td>Association for Computational Linguistics</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Improving Readability of Swedish Electronic He...</td>\n",
       "      <td>2014</td>\n",
       "      <td>Association for Computational Linguistics</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65909</th>\n",
       "      <td>Optimizing the weighted sequence alignment alg...</td>\n",
       "      <td>2022</td>\n",
       "      <td>Association for Computational Linguistics</td>\n",
       "      <td>We present an optimized implementation of the ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65910</th>\n",
       "      <td>Proceedings of the 25th Conference on Computat...</td>\n",
       "      <td>2021</td>\n",
       "      <td>Association for Computational Linguistics</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65911</th>\n",
       "      <td>A Large-Scale Pseudoword-Based Evaluation Fram...</td>\n",
       "      <td>2014</td>\n",
       "      <td>MIT Press</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65912</th>\n",
       "      <td>CIST System for CL-SciSumm 2016 Shared Task</td>\n",
       "      <td>2016</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65913</th>\n",
       "      <td>Ontology Engineering and Knowledge Extraction ...</td>\n",
       "      <td>2009</td>\n",
       "      <td>Association for Computational Linguistics</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>65914 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   title  year  \\\n",
       "0      Philippine Language Resources: Trends and Dire...  2009   \n",
       "1      A System for Translating Locative Prepositions...  1991   \n",
       "2      Introduction to the Shared Task on Comparing S...  2008   \n",
       "3      Pynini: A Python library for weighted finite-s...  2016   \n",
       "4      Improving Readability of Swedish Electronic He...  2014   \n",
       "...                                                  ...   ...   \n",
       "65909  Optimizing the weighted sequence alignment alg...  2022   \n",
       "65910  Proceedings of the 25th Conference on Computat...  2021   \n",
       "65911  A Large-Scale Pseudoword-Based Evaluation Fram...  2014   \n",
       "65912        CIST System for CL-SciSumm 2016 Shared Task  2016   \n",
       "65913  Ontology Engineering and Knowledge Extraction ...  2009   \n",
       "\n",
       "                                       publisher  \\\n",
       "0      Association for Computational Linguistics   \n",
       "1      Association for Computational Linguistics   \n",
       "2                           College Publications   \n",
       "3      Association for Computational Linguistics   \n",
       "4      Association for Computational Linguistics   \n",
       "...                                          ...   \n",
       "65909  Association for Computational Linguistics   \n",
       "65910  Association for Computational Linguistics   \n",
       "65911                                  MIT Press   \n",
       "65912                                       None   \n",
       "65913  Association for Computational Linguistics   \n",
       "\n",
       "                                                abstract  \\\n",
       "0                                                   None   \n",
       "1                                                   None   \n",
       "2                                                   None   \n",
       "3                                                   None   \n",
       "4                                                   None   \n",
       "...                                                  ...   \n",
       "65909  We present an optimized implementation of the ...   \n",
       "65910                                               None   \n",
       "65911                                               None   \n",
       "65912                                               None   \n",
       "65913                                               None   \n",
       "\n",
       "       bhattacharyya, pushpak  gurevych, iryna  category_article  \\\n",
       "0                           0                0                 0   \n",
       "1                           0                0                 0   \n",
       "2                           0                0                 0   \n",
       "3                           0                0                 0   \n",
       "4                           0                0                 0   \n",
       "...                       ...              ...               ...   \n",
       "65909                       0                0                 0   \n",
       "65910                       0                0                 0   \n",
       "65911                       0                0                 1   \n",
       "65912                       0                0                 0   \n",
       "65913                       0                0                 0   \n",
       "\n",
       "       category_inproceedings  category_proceedings  \n",
       "0                           1                     0  \n",
       "1                           1                     0  \n",
       "2                           1                     0  \n",
       "3                           1                     0  \n",
       "4                           1                     0  \n",
       "...                       ...                   ...  \n",
       "65909                       1                     0  \n",
       "65910                       0                     1  \n",
       "65911                       0                     0  \n",
       "65912                       1                     0  \n",
       "65913                       1                     0  \n",
       "\n",
       "[65914 rows x 9 columns]"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Drop all redundant columns\n",
    "train_2 = train_2.drop(['ENTRYTYPE'], axis=1)\n",
    "\n",
    "# Concatenate the original with dropped redundants and the extracted features for ENTRYTYPE\n",
    "train_3 = pd.concat([train_2, train_encoded_entrytype], axis=1).reindex(train_2.index)\n",
    "train_3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.4 Vectorize 'Publisher' column (116-categorical variable one-hot encoded, and reduced to X most frequent publishers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>publisher_Association for Computational Linguistics</th>\n",
       "      <th>publisher_European Language Resources Association (ELRA)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4456</td>\n",
       "      <td>37526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>37526</td>\n",
       "      <td>4456</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   publisher_Association for Computational Linguistics  \\\n",
       "0                                               4456     \n",
       "1                                              37526     \n",
       "\n",
       "   publisher_European Language Resources Association (ELRA)  \n",
       "0                                              37526         \n",
       "1                                               4456         "
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set the number of most frequent publishers to include\n",
    "n_mostfreq_publishers = 2  # Adjust this value as needed\n",
    "\n",
    "# Get the X most frequent publishers\n",
    "top_publishers = train_3['publisher'].value_counts().head(n_mostfreq_publishers).index.tolist()\n",
    "\n",
    "# Create a new DataFrame with one-hot encoding for the X most frequent publishers\n",
    "train_encoded_publisher = pd.get_dummies(train_3['publisher'][train_3['publisher'].isin(top_publishers)], prefix='publisher')\n",
    "\n",
    "# Show count-values for each of the columns\n",
    "train_encoded_publisher.apply(lambda x: x.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>year</th>\n",
       "      <th>abstract</th>\n",
       "      <th>bhattacharyya, pushpak</th>\n",
       "      <th>gurevych, iryna</th>\n",
       "      <th>category_article</th>\n",
       "      <th>category_inproceedings</th>\n",
       "      <th>category_proceedings</th>\n",
       "      <th>publisher_Association for Computational Linguistics</th>\n",
       "      <th>publisher_European Language Resources Association (ELRA)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Philippine Language Resources: Trends and Dire...</td>\n",
       "      <td>2009</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A System for Translating Locative Prepositions...</td>\n",
       "      <td>1991</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Introduction to the Shared Task on Comparing S...</td>\n",
       "      <td>2008</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Pynini: A Python library for weighted finite-s...</td>\n",
       "      <td>2016</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Improving Readability of Swedish Electronic He...</td>\n",
       "      <td>2014</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65909</th>\n",
       "      <td>Optimizing the weighted sequence alignment alg...</td>\n",
       "      <td>2022</td>\n",
       "      <td>We present an optimized implementation of the ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65910</th>\n",
       "      <td>Proceedings of the 25th Conference on Computat...</td>\n",
       "      <td>2021</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65911</th>\n",
       "      <td>A Large-Scale Pseudoword-Based Evaluation Fram...</td>\n",
       "      <td>2014</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65912</th>\n",
       "      <td>CIST System for CL-SciSumm 2016 Shared Task</td>\n",
       "      <td>2016</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65913</th>\n",
       "      <td>Ontology Engineering and Knowledge Extraction ...</td>\n",
       "      <td>2009</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>65914 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   title  year  \\\n",
       "0      Philippine Language Resources: Trends and Dire...  2009   \n",
       "1      A System for Translating Locative Prepositions...  1991   \n",
       "2      Introduction to the Shared Task on Comparing S...  2008   \n",
       "3      Pynini: A Python library for weighted finite-s...  2016   \n",
       "4      Improving Readability of Swedish Electronic He...  2014   \n",
       "...                                                  ...   ...   \n",
       "65909  Optimizing the weighted sequence alignment alg...  2022   \n",
       "65910  Proceedings of the 25th Conference on Computat...  2021   \n",
       "65911  A Large-Scale Pseudoword-Based Evaluation Fram...  2014   \n",
       "65912        CIST System for CL-SciSumm 2016 Shared Task  2016   \n",
       "65913  Ontology Engineering and Knowledge Extraction ...  2009   \n",
       "\n",
       "                                                abstract  \\\n",
       "0                                                      0   \n",
       "1                                                      0   \n",
       "2                                                      0   \n",
       "3                                                      0   \n",
       "4                                                      0   \n",
       "...                                                  ...   \n",
       "65909  We present an optimized implementation of the ...   \n",
       "65910                                                  0   \n",
       "65911                                                  0   \n",
       "65912                                                  0   \n",
       "65913                                                  0   \n",
       "\n",
       "       bhattacharyya, pushpak  gurevych, iryna  category_article  \\\n",
       "0                           0                0                 0   \n",
       "1                           0                0                 0   \n",
       "2                           0                0                 0   \n",
       "3                           0                0                 0   \n",
       "4                           0                0                 0   \n",
       "...                       ...              ...               ...   \n",
       "65909                       0                0                 0   \n",
       "65910                       0                0                 0   \n",
       "65911                       0                0                 1   \n",
       "65912                       0                0                 0   \n",
       "65913                       0                0                 0   \n",
       "\n",
       "       category_inproceedings  category_proceedings  \\\n",
       "0                           1                     0   \n",
       "1                           1                     0   \n",
       "2                           1                     0   \n",
       "3                           1                     0   \n",
       "4                           1                     0   \n",
       "...                       ...                   ...   \n",
       "65909                       1                     0   \n",
       "65910                       0                     1   \n",
       "65911                       0                     0   \n",
       "65912                       1                     0   \n",
       "65913                       1                     0   \n",
       "\n",
       "       publisher_Association for Computational Linguistics  \\\n",
       "0                                                    1.0     \n",
       "1                                                    1.0     \n",
       "2                                                    0.0     \n",
       "3                                                    1.0     \n",
       "4                                                    1.0     \n",
       "...                                                  ...     \n",
       "65909                                                1.0     \n",
       "65910                                                1.0     \n",
       "65911                                                0.0     \n",
       "65912                                                0.0     \n",
       "65913                                                1.0     \n",
       "\n",
       "       publisher_European Language Resources Association (ELRA)  \n",
       "0                                                    0.0         \n",
       "1                                                    0.0         \n",
       "2                                                    0.0         \n",
       "3                                                    0.0         \n",
       "4                                                    0.0         \n",
       "...                                                  ...         \n",
       "65909                                                0.0         \n",
       "65910                                                0.0         \n",
       "65911                                                0.0         \n",
       "65912                                                0.0         \n",
       "65913                                                0.0         \n",
       "\n",
       "[65914 rows x 10 columns]"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Drop all redundant columns\n",
    "train_3 = train_3.drop(['publisher'], axis=1)\n",
    "\n",
    "# Concatenate the original with dropped redundants and the extracted features for ENTRYTYPE\n",
    "train_4 = pd.concat([train_3, train_encoded_publisher], axis=1).reindex(train_3.index).fillna(0)\n",
    "train_4\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.3 Vectorize 'title' column (English-translated (?) with stop-words removal and/or synonym replacement)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "import googletrans    \n",
    "from googletrans import Translator\n",
    "from translate import Translator\n",
    "from langdetect import detect\n",
    "\n",
    "from mtranslate import translate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Gebruiker\\MLChallenge\\notebooks\\Main experiment\\Experiments.ipynb Cell 31\u001b[0m line \u001b[0;36m1\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Gebruiker/MLChallenge/notebooks/Main%20experiment/Experiments.ipynb#X32sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m translation \u001b[39mif\u001b[39;00m translation \u001b[39m!=\u001b[39m title \u001b[39melse\u001b[39;00m title\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Gebruiker/MLChallenge/notebooks/Main%20experiment/Experiments.ipynb#X32sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39m# Apply the translation function to the 'title' column\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Gebruiker/MLChallenge/notebooks/Main%20experiment/Experiments.ipynb#X32sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m title_tfidf_df[\u001b[39m'\u001b[39m\u001b[39mtranslated_title\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m title_tfidf_df[\u001b[39m'\u001b[39;49m\u001b[39mtitle\u001b[39;49m\u001b[39m'\u001b[39;49m]\u001b[39m.\u001b[39;49mapply(translate_to_english)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Gebruiker/MLChallenge/notebooks/Main%20experiment/Experiments.ipynb#X32sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m \u001b[39m# Display the resulting DataFrame\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Gebruiker/MLChallenge/notebooks/Main%20experiment/Experiments.ipynb#X32sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m title_tfidf_df\n",
      "File \u001b[1;32mc:\\Users\\Gebruiker\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\series.py:4771\u001b[0m, in \u001b[0;36mSeries.apply\u001b[1;34m(self, func, convert_dtype, args, **kwargs)\u001b[0m\n\u001b[0;32m   4661\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mapply\u001b[39m(\n\u001b[0;32m   4662\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m   4663\u001b[0m     func: AggFuncType,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4666\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[0;32m   4667\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m DataFrame \u001b[39m|\u001b[39m Series:\n\u001b[0;32m   4668\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   4669\u001b[0m \u001b[39m    Invoke function on values of Series.\u001b[39;00m\n\u001b[0;32m   4670\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4769\u001b[0m \u001b[39m    dtype: float64\u001b[39;00m\n\u001b[0;32m   4770\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 4771\u001b[0m     \u001b[39mreturn\u001b[39;00m SeriesApply(\u001b[39mself\u001b[39;49m, func, convert_dtype, args, kwargs)\u001b[39m.\u001b[39;49mapply()\n",
      "File \u001b[1;32mc:\\Users\\Gebruiker\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\apply.py:1123\u001b[0m, in \u001b[0;36mSeriesApply.apply\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1120\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mapply_str()\n\u001b[0;32m   1122\u001b[0m \u001b[39m# self.f is Callable\u001b[39;00m\n\u001b[1;32m-> 1123\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mapply_standard()\n",
      "File \u001b[1;32mc:\\Users\\Gebruiker\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\apply.py:1174\u001b[0m, in \u001b[0;36mSeriesApply.apply_standard\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1172\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1173\u001b[0m         values \u001b[39m=\u001b[39m obj\u001b[39m.\u001b[39mastype(\u001b[39mobject\u001b[39m)\u001b[39m.\u001b[39m_values\n\u001b[1;32m-> 1174\u001b[0m         mapped \u001b[39m=\u001b[39m lib\u001b[39m.\u001b[39;49mmap_infer(\n\u001b[0;32m   1175\u001b[0m             values,\n\u001b[0;32m   1176\u001b[0m             f,\n\u001b[0;32m   1177\u001b[0m             convert\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconvert_dtype,\n\u001b[0;32m   1178\u001b[0m         )\n\u001b[0;32m   1180\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(mapped) \u001b[39mand\u001b[39;00m \u001b[39misinstance\u001b[39m(mapped[\u001b[39m0\u001b[39m], ABCSeries):\n\u001b[0;32m   1181\u001b[0m     \u001b[39m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[0;32m   1182\u001b[0m     \u001b[39m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[0;32m   1183\u001b[0m     \u001b[39mreturn\u001b[39;00m obj\u001b[39m.\u001b[39m_constructor_expanddim(\u001b[39mlist\u001b[39m(mapped), index\u001b[39m=\u001b[39mobj\u001b[39m.\u001b[39mindex)\n",
      "File \u001b[1;32mc:\\Users\\Gebruiker\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\_libs\\lib.pyx:2924\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Gebruiker\\MLChallenge\\notebooks\\Main experiment\\Experiments.ipynb Cell 31\u001b[0m line \u001b[0;36m6\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Gebruiker/MLChallenge/notebooks/Main%20experiment/Experiments.ipynb#X32sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtranslate_to_english\u001b[39m(title):\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Gebruiker/MLChallenge/notebooks/Main%20experiment/Experiments.ipynb#X32sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     translation \u001b[39m=\u001b[39m translate(title, \u001b[39m'\u001b[39;49m\u001b[39men\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Gebruiker/MLChallenge/notebooks/Main%20experiment/Experiments.ipynb#X32sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m translation \u001b[39mif\u001b[39;00m translation \u001b[39m!=\u001b[39m title \u001b[39melse\u001b[39;00m title\n",
      "File \u001b[1;32mc:\\Users\\Gebruiker\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\mtranslate\\core.py:80\u001b[0m, in \u001b[0;36mtranslate\u001b[1;34m(to_translate, to_language, from_language)\u001b[0m\n\u001b[0;32m     78\u001b[0m     link \u001b[39m=\u001b[39m base_link \u001b[39m%\u001b[39m (to_language, from_language, to_translate)\n\u001b[0;32m     79\u001b[0m     request \u001b[39m=\u001b[39m urllib\u001b[39m.\u001b[39mrequest\u001b[39m.\u001b[39mRequest(link, headers\u001b[39m=\u001b[39magent)\n\u001b[1;32m---> 80\u001b[0m     raw_data \u001b[39m=\u001b[39m urllib\u001b[39m.\u001b[39;49mrequest\u001b[39m.\u001b[39;49murlopen(request)\u001b[39m.\u001b[39mread()\n\u001b[0;32m     81\u001b[0m data \u001b[39m=\u001b[39m raw_data\u001b[39m.\u001b[39mdecode(\u001b[39m\"\u001b[39m\u001b[39mutf-8\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     82\u001b[0m expr \u001b[39m=\u001b[39m \u001b[39mr\u001b[39m\u001b[39m'\u001b[39m\u001b[39m(?s)class=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m(?:t0|result-container)\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m>(.*?)<\u001b[39m\u001b[39m'\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\Gebruiker\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\urllib\\request.py:216\u001b[0m, in \u001b[0;36murlopen\u001b[1;34m(url, data, timeout, cafile, capath, cadefault, context)\u001b[0m\n\u001b[0;32m    214\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    215\u001b[0m     opener \u001b[39m=\u001b[39m _opener\n\u001b[1;32m--> 216\u001b[0m \u001b[39mreturn\u001b[39;00m opener\u001b[39m.\u001b[39;49mopen(url, data, timeout)\n",
      "File \u001b[1;32mc:\\Users\\Gebruiker\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\urllib\\request.py:525\u001b[0m, in \u001b[0;36mOpenerDirector.open\u001b[1;34m(self, fullurl, data, timeout)\u001b[0m\n\u001b[0;32m    523\u001b[0m \u001b[39mfor\u001b[39;00m processor \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprocess_response\u001b[39m.\u001b[39mget(protocol, []):\n\u001b[0;32m    524\u001b[0m     meth \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(processor, meth_name)\n\u001b[1;32m--> 525\u001b[0m     response \u001b[39m=\u001b[39m meth(req, response)\n\u001b[0;32m    527\u001b[0m \u001b[39mreturn\u001b[39;00m response\n",
      "File \u001b[1;32mc:\\Users\\Gebruiker\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\urllib\\request.py:634\u001b[0m, in \u001b[0;36mHTTPErrorProcessor.http_response\u001b[1;34m(self, request, response)\u001b[0m\n\u001b[0;32m    631\u001b[0m \u001b[39m# According to RFC 2616, \"2xx\" code indicates that the client's\u001b[39;00m\n\u001b[0;32m    632\u001b[0m \u001b[39m# request was successfully received, understood, and accepted.\u001b[39;00m\n\u001b[0;32m    633\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39m200\u001b[39m \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m code \u001b[39m<\u001b[39m \u001b[39m300\u001b[39m):\n\u001b[1;32m--> 634\u001b[0m     response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mparent\u001b[39m.\u001b[39;49merror(\n\u001b[0;32m    635\u001b[0m         \u001b[39m'\u001b[39;49m\u001b[39mhttp\u001b[39;49m\u001b[39m'\u001b[39;49m, request, response, code, msg, hdrs)\n\u001b[0;32m    637\u001b[0m \u001b[39mreturn\u001b[39;00m response\n",
      "File \u001b[1;32mc:\\Users\\Gebruiker\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\urllib\\request.py:557\u001b[0m, in \u001b[0;36mOpenerDirector.error\u001b[1;34m(self, proto, *args)\u001b[0m\n\u001b[0;32m    555\u001b[0m     http_err \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m    556\u001b[0m args \u001b[39m=\u001b[39m (\u001b[39mdict\u001b[39m, proto, meth_name) \u001b[39m+\u001b[39m args\n\u001b[1;32m--> 557\u001b[0m result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_chain(\u001b[39m*\u001b[39;49margs)\n\u001b[0;32m    558\u001b[0m \u001b[39mif\u001b[39;00m result:\n\u001b[0;32m    559\u001b[0m     \u001b[39mreturn\u001b[39;00m result\n",
      "File \u001b[1;32mc:\\Users\\Gebruiker\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\urllib\\request.py:496\u001b[0m, in \u001b[0;36mOpenerDirector._call_chain\u001b[1;34m(self, chain, kind, meth_name, *args)\u001b[0m\n\u001b[0;32m    494\u001b[0m \u001b[39mfor\u001b[39;00m handler \u001b[39min\u001b[39;00m handlers:\n\u001b[0;32m    495\u001b[0m     func \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(handler, meth_name)\n\u001b[1;32m--> 496\u001b[0m     result \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39;49margs)\n\u001b[0;32m    497\u001b[0m     \u001b[39mif\u001b[39;00m result \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    498\u001b[0m         \u001b[39mreturn\u001b[39;00m result\n",
      "File \u001b[1;32mc:\\Users\\Gebruiker\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\urllib\\request.py:749\u001b[0m, in \u001b[0;36mHTTPRedirectHandler.http_error_302\u001b[1;34m(self, req, fp, code, msg, headers)\u001b[0m\n\u001b[0;32m    746\u001b[0m fp\u001b[39m.\u001b[39mread()\n\u001b[0;32m    747\u001b[0m fp\u001b[39m.\u001b[39mclose()\n\u001b[1;32m--> 749\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mparent\u001b[39m.\u001b[39;49mopen(new, timeout\u001b[39m=\u001b[39;49mreq\u001b[39m.\u001b[39;49mtimeout)\n",
      "File \u001b[1;32mc:\\Users\\Gebruiker\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\urllib\\request.py:519\u001b[0m, in \u001b[0;36mOpenerDirector.open\u001b[1;34m(self, fullurl, data, timeout)\u001b[0m\n\u001b[0;32m    516\u001b[0m     req \u001b[39m=\u001b[39m meth(req)\n\u001b[0;32m    518\u001b[0m sys\u001b[39m.\u001b[39maudit(\u001b[39m'\u001b[39m\u001b[39murllib.Request\u001b[39m\u001b[39m'\u001b[39m, req\u001b[39m.\u001b[39mfull_url, req\u001b[39m.\u001b[39mdata, req\u001b[39m.\u001b[39mheaders, req\u001b[39m.\u001b[39mget_method())\n\u001b[1;32m--> 519\u001b[0m response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_open(req, data)\n\u001b[0;32m    521\u001b[0m \u001b[39m# post-process response\u001b[39;00m\n\u001b[0;32m    522\u001b[0m meth_name \u001b[39m=\u001b[39m protocol\u001b[39m+\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m_response\u001b[39m\u001b[39m\"\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\Gebruiker\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\urllib\\request.py:536\u001b[0m, in \u001b[0;36mOpenerDirector._open\u001b[1;34m(self, req, data)\u001b[0m\n\u001b[0;32m    533\u001b[0m     \u001b[39mreturn\u001b[39;00m result\n\u001b[0;32m    535\u001b[0m protocol \u001b[39m=\u001b[39m req\u001b[39m.\u001b[39mtype\n\u001b[1;32m--> 536\u001b[0m result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_chain(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mhandle_open, protocol, protocol \u001b[39m+\u001b[39;49m\n\u001b[0;32m    537\u001b[0m                           \u001b[39m'\u001b[39;49m\u001b[39m_open\u001b[39;49m\u001b[39m'\u001b[39;49m, req)\n\u001b[0;32m    538\u001b[0m \u001b[39mif\u001b[39;00m result:\n\u001b[0;32m    539\u001b[0m     \u001b[39mreturn\u001b[39;00m result\n",
      "File \u001b[1;32mc:\\Users\\Gebruiker\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\urllib\\request.py:496\u001b[0m, in \u001b[0;36mOpenerDirector._call_chain\u001b[1;34m(self, chain, kind, meth_name, *args)\u001b[0m\n\u001b[0;32m    494\u001b[0m \u001b[39mfor\u001b[39;00m handler \u001b[39min\u001b[39;00m handlers:\n\u001b[0;32m    495\u001b[0m     func \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(handler, meth_name)\n\u001b[1;32m--> 496\u001b[0m     result \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39;49margs)\n\u001b[0;32m    497\u001b[0m     \u001b[39mif\u001b[39;00m result \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    498\u001b[0m         \u001b[39mreturn\u001b[39;00m result\n",
      "File \u001b[1;32mc:\\Users\\Gebruiker\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\urllib\\request.py:1391\u001b[0m, in \u001b[0;36mHTTPSHandler.https_open\u001b[1;34m(self, req)\u001b[0m\n\u001b[0;32m   1390\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mhttps_open\u001b[39m(\u001b[39mself\u001b[39m, req):\n\u001b[1;32m-> 1391\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdo_open(http\u001b[39m.\u001b[39;49mclient\u001b[39m.\u001b[39;49mHTTPSConnection, req,\n\u001b[0;32m   1392\u001b[0m         context\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_context, check_hostname\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_check_hostname)\n",
      "File \u001b[1;32mc:\\Users\\Gebruiker\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\urllib\\request.py:1352\u001b[0m, in \u001b[0;36mAbstractHTTPHandler.do_open\u001b[1;34m(self, http_class, req, **http_conn_args)\u001b[0m\n\u001b[0;32m   1350\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mOSError\u001b[39;00m \u001b[39mas\u001b[39;00m err: \u001b[39m# timeout error\u001b[39;00m\n\u001b[0;32m   1351\u001b[0m         \u001b[39mraise\u001b[39;00m URLError(err)\n\u001b[1;32m-> 1352\u001b[0m     r \u001b[39m=\u001b[39m h\u001b[39m.\u001b[39;49mgetresponse()\n\u001b[0;32m   1353\u001b[0m \u001b[39mexcept\u001b[39;00m:\n\u001b[0;32m   1354\u001b[0m     h\u001b[39m.\u001b[39mclose()\n",
      "File \u001b[1;32mc:\\Users\\Gebruiker\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\http\\client.py:1374\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1372\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1373\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 1374\u001b[0m         response\u001b[39m.\u001b[39;49mbegin()\n\u001b[0;32m   1375\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mConnectionError\u001b[39;00m:\n\u001b[0;32m   1376\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclose()\n",
      "File \u001b[1;32mc:\\Users\\Gebruiker\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\http\\client.py:318\u001b[0m, in \u001b[0;36mHTTPResponse.begin\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    316\u001b[0m \u001b[39m# read until we get a non-100 response\u001b[39;00m\n\u001b[0;32m    317\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m--> 318\u001b[0m     version, status, reason \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_read_status()\n\u001b[0;32m    319\u001b[0m     \u001b[39mif\u001b[39;00m status \u001b[39m!=\u001b[39m CONTINUE:\n\u001b[0;32m    320\u001b[0m         \u001b[39mbreak\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Gebruiker\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\http\\client.py:279\u001b[0m, in \u001b[0;36mHTTPResponse._read_status\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    278\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_read_status\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m--> 279\u001b[0m     line \u001b[39m=\u001b[39m \u001b[39mstr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfp\u001b[39m.\u001b[39mreadline(_MAXLINE \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m), \u001b[39m\"\u001b[39m\u001b[39miso-8859-1\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    280\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(line) \u001b[39m>\u001b[39m _MAXLINE:\n\u001b[0;32m    281\u001b[0m         \u001b[39mraise\u001b[39;00m LineTooLong(\u001b[39m\"\u001b[39m\u001b[39mstatus line\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Gebruiker\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\socket.py:706\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[1;34m(self, b)\u001b[0m\n\u001b[0;32m    704\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m    705\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 706\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sock\u001b[39m.\u001b[39;49mrecv_into(b)\n\u001b[0;32m    707\u001b[0m     \u001b[39mexcept\u001b[39;00m timeout:\n\u001b[0;32m    708\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_timeout_occurred \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Gebruiker\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\ssl.py:1278\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[1;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[0;32m   1274\u001b[0m     \u001b[39mif\u001b[39;00m flags \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m   1275\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m   1276\u001b[0m           \u001b[39m\"\u001b[39m\u001b[39mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m\n\u001b[0;32m   1277\u001b[0m           \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m)\n\u001b[1;32m-> 1278\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mread(nbytes, buffer)\n\u001b[0;32m   1279\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1280\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39mrecv_into(buffer, nbytes, flags)\n",
      "File \u001b[1;32mc:\\Users\\Gebruiker\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\ssl.py:1134\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[1;34m(self, len, buffer)\u001b[0m\n\u001b[0;32m   1132\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1133\u001b[0m     \u001b[39mif\u001b[39;00m buffer \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m-> 1134\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sslobj\u001b[39m.\u001b[39;49mread(\u001b[39mlen\u001b[39;49m, buffer)\n\u001b[0;32m   1135\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1136\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sslobj\u001b[39m.\u001b[39mread(\u001b[39mlen\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Assign DataFrame to title_tfidf_df\n",
    "title_tfidf_df = train_sample.copy()\n",
    "\n",
    "# Function to translate non-English titles to English\n",
    "def translate_to_english(title):\n",
    "    translation = translate(title, 'en')\n",
    "    return translation if translation != title else title\n",
    "\n",
    "# Apply the translation function to the 'title' column\n",
    "title_tfidf_df['translated_title'] = title_tfidf_df['title'].apply(translate_to_english)\n",
    "\n",
    "# Display the resulting DataFrame\n",
    "title_tfidf_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ENTRYTYPE</th>\n",
       "      <th>title</th>\n",
       "      <th>editor</th>\n",
       "      <th>year</th>\n",
       "      <th>publisher</th>\n",
       "      <th>author</th>\n",
       "      <th>abstract</th>\n",
       "      <th>translated_title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>12680</th>\n",
       "      <td>inproceedings</td>\n",
       "      <td>Question-Answering Based on Virtually Integrat...</td>\n",
       "      <td>None</td>\n",
       "      <td>2003</td>\n",
       "      <td>Association for Computational Linguistics</td>\n",
       "      <td>[Choi, Key-Sun, Kim, Jae-Ho, Miyazaki, Masaru,...</td>\n",
       "      <td>None</td>\n",
       "      <td>Question-Answering Based on Virtually Integrat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17292</th>\n",
       "      <td>inproceedings</td>\n",
       "      <td>AMI&amp;ERIC: How to Learn with Naive Bayes and Pr...</td>\n",
       "      <td>None</td>\n",
       "      <td>2013</td>\n",
       "      <td>Association for Computational Linguistics</td>\n",
       "      <td>[Dermouche, Mohamed, Khouas, Leila, Velcin, Ju...</td>\n",
       "      <td>None</td>\n",
       "      <td>AMI&amp;ERIC: How to Learn with Naive Bayes and Pr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33265</th>\n",
       "      <td>inproceedings</td>\n",
       "      <td>Inducing Gazetteers for Named Entity Recogniti...</td>\n",
       "      <td>None</td>\n",
       "      <td>2008</td>\n",
       "      <td>Association for Computational Linguistics</td>\n",
       "      <td>[Kazama, Jun'ichi, Torisawa, Kentaro]</td>\n",
       "      <td>None</td>\n",
       "      <td>Inducing Gazetteers for Named Entity Recogniti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52850</th>\n",
       "      <td>inproceedings</td>\n",
       "      <td>Leveraging Explicit Lexico-logical Alignments ...</td>\n",
       "      <td>None</td>\n",
       "      <td>2022</td>\n",
       "      <td>Association for Computational Linguistics</td>\n",
       "      <td>[Sun, Runxin, He, Shizhu, Zhu, Chong, He, Yaoh...</td>\n",
       "      <td>Text-to-SQL aims to parse natural language que...</td>\n",
       "      <td>Leveraging Explicit Lexico-logical Alignments ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2298</th>\n",
       "      <td>inproceedings</td>\n",
       "      <td>CLAM: Quickly deploy NLP command-line tools on...</td>\n",
       "      <td>None</td>\n",
       "      <td>2014</td>\n",
       "      <td>Dublin City University and Association for Com...</td>\n",
       "      <td>[van Gompel, Maarten, Reynaert, Martin]</td>\n",
       "      <td>None</td>\n",
       "      <td>CLAM: Quickly deploy NLP command-line tools on...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27576</th>\n",
       "      <td>inproceedings</td>\n",
       "      <td>Analyzing Culture-Specific Argument Structures...</td>\n",
       "      <td>None</td>\n",
       "      <td>2022</td>\n",
       "      <td>International Conference on Computational Ling...</td>\n",
       "      <td>[Chen, Wei-Fan, Chen, Mei-Hua, Mudgal, Garima,...</td>\n",
       "      <td>Language education has been shown to benefit f...</td>\n",
       "      <td>Analyzing Culture-Specific Argument Structures...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33657</th>\n",
       "      <td>inproceedings</td>\n",
       "      <td>Acquistion of the Morphological Structure of t...</td>\n",
       "      <td>None</td>\n",
       "      <td>2008</td>\n",
       "      <td>Coling 2008 Organizing Committee</td>\n",
       "      <td>[Hathout, Nabil]</td>\n",
       "      <td>None</td>\n",
       "      <td>Acquistion of the Morphological Structure of t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20315</th>\n",
       "      <td>inproceedings</td>\n",
       "      <td>A Deep Linguistic Analysis for Cross-language ...</td>\n",
       "      <td>None</td>\n",
       "      <td>2006</td>\n",
       "      <td>European Language Resources Association (ELRA)</td>\n",
       "      <td>[Semmar, Nasredine, Laib, Meriama, Fluhr, Chri...</td>\n",
       "      <td>Cross-language information retrieval consists ...</td>\n",
       "      <td>A Deep Linguistic Analysis for Cross-language ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23899</th>\n",
       "      <td>inproceedings</td>\n",
       "      <td>A Theory of Unsupervised Speech Recognition</td>\n",
       "      <td>None</td>\n",
       "      <td>2023</td>\n",
       "      <td>Association for Computational Linguistics</td>\n",
       "      <td>[Wang, Liming, Hasegawa-Johnson, Mark, Yoo, Ch...</td>\n",
       "      <td>Unsupervised speech recognition (pasted macro ...</td>\n",
       "      <td>A Theory of Unsupervised Speech Recognition</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56546</th>\n",
       "      <td>inproceedings</td>\n",
       "      <td>Simple Queries as Distant Labels for Predictin...</td>\n",
       "      <td>None</td>\n",
       "      <td>2017</td>\n",
       "      <td>Association for Computational Linguistics</td>\n",
       "      <td>[Emmery, Chris, Chrupała, Grzegorz, Daelemans,...</td>\n",
       "      <td>The majority of research on extracting missing...</td>\n",
       "      <td>Simple Queries as Distant Labels for Predictin...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>13182 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           ENTRYTYPE                                              title  \\\n",
       "12680  inproceedings  Question-Answering Based on Virtually Integrat...   \n",
       "17292  inproceedings  AMI&ERIC: How to Learn with Naive Bayes and Pr...   \n",
       "33265  inproceedings  Inducing Gazetteers for Named Entity Recogniti...   \n",
       "52850  inproceedings  Leveraging Explicit Lexico-logical Alignments ...   \n",
       "2298   inproceedings  CLAM: Quickly deploy NLP command-line tools on...   \n",
       "...              ...                                                ...   \n",
       "27576  inproceedings  Analyzing Culture-Specific Argument Structures...   \n",
       "33657  inproceedings  Acquistion of the Morphological Structure of t...   \n",
       "20315  inproceedings  A Deep Linguistic Analysis for Cross-language ...   \n",
       "23899  inproceedings        A Theory of Unsupervised Speech Recognition   \n",
       "56546  inproceedings  Simple Queries as Distant Labels for Predictin...   \n",
       "\n",
       "      editor  year                                          publisher  \\\n",
       "12680   None  2003          Association for Computational Linguistics   \n",
       "17292   None  2013          Association for Computational Linguistics   \n",
       "33265   None  2008          Association for Computational Linguistics   \n",
       "52850   None  2022          Association for Computational Linguistics   \n",
       "2298    None  2014  Dublin City University and Association for Com...   \n",
       "...      ...   ...                                                ...   \n",
       "27576   None  2022  International Conference on Computational Ling...   \n",
       "33657   None  2008                   Coling 2008 Organizing Committee   \n",
       "20315   None  2006     European Language Resources Association (ELRA)   \n",
       "23899   None  2023          Association for Computational Linguistics   \n",
       "56546   None  2017          Association for Computational Linguistics   \n",
       "\n",
       "                                                  author  \\\n",
       "12680  [Choi, Key-Sun, Kim, Jae-Ho, Miyazaki, Masaru,...   \n",
       "17292  [Dermouche, Mohamed, Khouas, Leila, Velcin, Ju...   \n",
       "33265              [Kazama, Jun'ichi, Torisawa, Kentaro]   \n",
       "52850  [Sun, Runxin, He, Shizhu, Zhu, Chong, He, Yaoh...   \n",
       "2298             [van Gompel, Maarten, Reynaert, Martin]   \n",
       "...                                                  ...   \n",
       "27576  [Chen, Wei-Fan, Chen, Mei-Hua, Mudgal, Garima,...   \n",
       "33657                                   [Hathout, Nabil]   \n",
       "20315  [Semmar, Nasredine, Laib, Meriama, Fluhr, Chri...   \n",
       "23899  [Wang, Liming, Hasegawa-Johnson, Mark, Yoo, Ch...   \n",
       "56546  [Emmery, Chris, Chrupała, Grzegorz, Daelemans,...   \n",
       "\n",
       "                                                abstract  \\\n",
       "12680                                               None   \n",
       "17292                                               None   \n",
       "33265                                               None   \n",
       "52850  Text-to-SQL aims to parse natural language que...   \n",
       "2298                                                None   \n",
       "...                                                  ...   \n",
       "27576  Language education has been shown to benefit f...   \n",
       "33657                                               None   \n",
       "20315  Cross-language information retrieval consists ...   \n",
       "23899  Unsupervised speech recognition (pasted macro ...   \n",
       "56546  The majority of research on extracting missing...   \n",
       "\n",
       "                                        translated_title  \n",
       "12680  Question-Answering Based on Virtually Integrat...  \n",
       "17292  AMI&ERIC: How to Learn with Naive Bayes and Pr...  \n",
       "33265  Inducing Gazetteers for Named Entity Recogniti...  \n",
       "52850  Leveraging Explicit Lexico-logical Alignments ...  \n",
       "2298   CLAM: Quickly deploy NLP command-line tools on...  \n",
       "...                                                  ...  \n",
       "27576  Analyzing Culture-Specific Argument Structures...  \n",
       "33657  Acquistion of the Morphological Structure of t...  \n",
       "20315  A Deep Linguistic Analysis for Cross-language ...  \n",
       "23899        A Theory of Unsupervised Speech Recognition  \n",
       "56546  Simple Queries as Distant Labels for Predictin...  \n",
       "\n",
       "[13182 rows x 8 columns]"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Assign DataFrame to title_tfidf_df\n",
    "title_tfidf_df = train_sample\n",
    "\n",
    "# Function to translate non-English titles to English\n",
    "def translate_to_english(title):\n",
    "    translator= Translator(to_lang=\"en\")\n",
    "    translation = translator.translate(title)\n",
    "    return translation if translator.from_lang != 'en' else title\n",
    "\n",
    "# Apply the translation function to the 'title' column\n",
    "title_tfidf_df['translated_title'] = title_tfidf_df['title'].apply(lambda x: translate_to_english(x))\n",
    "\n",
    "# Display the resulting DataFrame\n",
    "title_tfidf_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def det(x):\n",
    "    try:\n",
    "        lang = detect(x)\n",
    "    except:\n",
    "        lang = 'Other'\n",
    "    return lang\n",
    "\n",
    "title_tfidf_df['translated_language'] = title_tfidf_df['translated_title'].apply(det)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "en       12590\n",
       "fr         352\n",
       "it          51\n",
       "de          34\n",
       "da          23\n",
       "nl          22\n",
       "ca          21\n",
       "ro          18\n",
       "tl          16\n",
       "af          11\n",
       "es          10\n",
       "id          10\n",
       "pt           9\n",
       "no           6\n",
       "et           4\n",
       "sv           2\n",
       "cy           1\n",
       "sw           1\n",
       "Other        1\n",
       "Name: translated_language, dtype: int64"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "title_tfidf_df['translated_language'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We've transformed the 'author' column to a dataframe of 4820 columns.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>05</th>\n",
       "      <th>08</th>\n",
       "      <th>10</th>\n",
       "      <th>10th</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>14</th>\n",
       "      <th>14th</th>\n",
       "      <th>14ème</th>\n",
       "      <th>15</th>\n",
       "      <th>...</th>\n",
       "      <th>基于bilstm</th>\n",
       "      <th>基于语料库的形容词性别偏度历时研究</th>\n",
       "      <th>多模态表述视域下的小学数学课堂语言计量初探</th>\n",
       "      <th>多語語碼轉換之未知詞擷取</th>\n",
       "      <th>大規模詞彙語意關係自動標示之初步研究</th>\n",
       "      <th>字里行间的道德</th>\n",
       "      <th>為例</th>\n",
       "      <th>融合多层语义特征图的缅甸语图像文本识别方法</th>\n",
       "      <th>調變頻譜正規化法使用於強健語音辨識之研究</th>\n",
       "      <th>非監督式學習於中文電視新聞自動轉寫之初步應用</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2466</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2467</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2468</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2469</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2470</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2471 rows × 4820 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       05   08   10  10th   11   12   14  14th  14ème   15  ...  基于bilstm  \\\n",
       "0     0.0  0.0  0.0   0.0  0.0  0.0  0.0   0.0    0.0  0.0  ...       0.0   \n",
       "1     0.0  0.0  0.0   0.0  0.0  0.0  0.0   0.0    0.0  0.0  ...       0.0   \n",
       "2     0.0  0.0  0.0   0.0  0.0  0.0  0.0   0.0    0.0  0.0  ...       0.0   \n",
       "3     0.0  0.0  0.0   0.0  0.0  0.0  0.0   0.0    0.0  0.0  ...       0.0   \n",
       "4     0.0  0.0  0.0   0.0  0.0  0.0  0.0   0.0    0.0  0.0  ...       0.0   \n",
       "...   ...  ...  ...   ...  ...  ...  ...   ...    ...  ...  ...       ...   \n",
       "2466  0.0  0.0  0.0   0.0  0.0  0.0  0.0   0.0    0.0  0.0  ...       0.0   \n",
       "2467  0.0  0.0  0.0   0.0  0.0  0.0  0.0   0.0    0.0  0.0  ...       0.0   \n",
       "2468  0.0  0.0  0.0   0.0  0.0  0.0  0.0   0.0    0.0  0.0  ...       0.0   \n",
       "2469  0.0  0.0  0.0   0.0  0.0  0.0  0.0   0.0    0.0  0.0  ...       0.0   \n",
       "2470  0.0  0.0  0.0   0.0  0.0  0.0  0.0   0.0    0.0  0.0  ...       0.0   \n",
       "\n",
       "      基于语料库的形容词性别偏度历时研究  多模态表述视域下的小学数学课堂语言计量初探  多語語碼轉換之未知詞擷取  \\\n",
       "0                   0.0                    0.0           0.0   \n",
       "1                   0.0                    0.0           0.0   \n",
       "2                   0.0                    0.0           0.0   \n",
       "3                   0.0                    0.0           0.0   \n",
       "4                   0.0                    0.0           0.0   \n",
       "...                 ...                    ...           ...   \n",
       "2466                0.0                    0.0           0.0   \n",
       "2467                0.0                    0.0           0.0   \n",
       "2468                0.0                    0.0           0.0   \n",
       "2469                0.0                    0.0           0.0   \n",
       "2470                0.0                    0.0           0.0   \n",
       "\n",
       "      大規模詞彙語意關係自動標示之初步研究  字里行间的道德   為例  融合多层语义特征图的缅甸语图像文本识别方法  \\\n",
       "0                    0.0      0.0  0.0                    0.0   \n",
       "1                    0.0      0.0  0.0                    0.0   \n",
       "2                    0.0      0.0  0.0                    0.0   \n",
       "3                    0.0      0.0  0.0                    0.0   \n",
       "4                    0.0      0.0  0.0                    0.0   \n",
       "...                  ...      ...  ...                    ...   \n",
       "2466                 0.0      0.0  0.0                    0.0   \n",
       "2467                 0.0      0.0  0.0                    0.0   \n",
       "2468                 0.0      0.0  0.0                    0.0   \n",
       "2469                 0.0      0.0  0.0                    0.0   \n",
       "2470                 0.0      0.0  0.0                    0.0   \n",
       "\n",
       "      調變頻譜正規化法使用於強健語音辨識之研究  非監督式學習於中文電視新聞自動轉寫之初步應用  \n",
       "0                      0.0                     0.0  \n",
       "1                      0.0                     0.0  \n",
       "2                      0.0                     0.0  \n",
       "3                      0.0                     0.0  \n",
       "4                      0.0                     0.0  \n",
       "...                    ...                     ...  \n",
       "2466                   0.0                     0.0  \n",
       "2467                   0.0                     0.0  \n",
       "2468                   0.0                     0.0  \n",
       "2469                   0.0                     0.0  \n",
       "2470                   0.0                     0.0  \n",
       "\n",
       "[2471 rows x 4820 columns]"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Apply the TF-IDF vectorizer to column 'title'\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(train_sample_filtered['title'])\n",
    "\n",
    "# Extract and create columns\n",
    "feature_names = tfidf_vectorizer.get_feature_names_out()\n",
    "tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=feature_names)\n",
    "\n",
    "print(f\"We've transformed the 'author' column to a dataframe of {len(tfidf_df.columns)} columns.\")\n",
    "tfidf_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.5 Vectorize 'Abstract' column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Incorporate into baseline code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Loading training/test data\n",
      "INFO:root:Splitting validation\n",
      "INFO:root:Fitting model with featurizer 1\n",
      "INFO:root:Evaluating on validation data\n",
      "INFO:root:Ridge regress MAE with featurizer 1 (5-fold cross-validated): 5.773010450586702\n",
      "INFO:root:Fitting model with featurizer 2\n",
      "INFO:root:Evaluating on validation data\n",
      "INFO:root:Ridge regress MAE with featurizer 2 (5-fold cross-validated): 5.384430333156983\n",
      "INFO:root:Fitting model with featurizer 3\n",
      "INFO:root:Evaluating on validation data\n",
      "INFO:root:Ridge regress MAE with featurizer 3 (5-fold cross-validated): 6.340921782179531\n",
      "INFO:root:Fitting model with featurizer 4\n",
      "INFO:root:Evaluating on validation data\n",
      "INFO:root:Ridge regress MAE with featurizer 4 (5-fold cross-validated): 5.480748043346883\n",
      "INFO:root:Fitting model with featurizer 5\n"
     ]
    },
    {
     "ename": "NotFittedError",
     "evalue": "This MultiLabelBinarizer instance is not fitted yet. Call 'fit' with appropriate arguments before using this estimator.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNotFittedError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Gebruiker\\MLChallenge\\notebooks\\Experiments.ipynb Cell 16\u001b[0m line \u001b[0;36m4\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Gebruiker/MLChallenge/notebooks/Experiments.ipynb#X21sZmlsZQ%3D%3D?line=41'>42</a>\u001b[0m \u001b[39m# Drop target variable column and fit both models\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Gebruiker/MLChallenge/notebooks/Experiments.ipynb#X21sZmlsZQ%3D%3D?line=42'>43</a>\u001b[0m logging\u001b[39m.\u001b[39minfo(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mFitting model with featurizer \u001b[39m\u001b[39m{\u001b[39;00mi\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Gebruiker/MLChallenge/notebooks/Experiments.ipynb#X21sZmlsZQ%3D%3D?line=43'>44</a>\u001b[0m ridge_cv\u001b[39m.\u001b[39;49mfit(train\u001b[39m.\u001b[39;49mdrop(\u001b[39m'\u001b[39;49m\u001b[39myear\u001b[39;49m\u001b[39m'\u001b[39;49m, axis\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m), train[\u001b[39m'\u001b[39;49m\u001b[39myear\u001b[39;49m\u001b[39m'\u001b[39;49m]\u001b[39m.\u001b[39;49mvalues)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Gebruiker/MLChallenge/notebooks/Experiments.ipynb#X21sZmlsZQ%3D%3D?line=45'>46</a>\u001b[0m \u001b[39m# Calculate and report both MAE's\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Gebruiker/MLChallenge/notebooks/Experiments.ipynb#X21sZmlsZQ%3D%3D?line=46'>47</a>\u001b[0m logging\u001b[39m.\u001b[39minfo(\u001b[39m\"\u001b[39m\u001b[39mEvaluating on validation data\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Gebruiker\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\base.py:1151\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1144\u001b[0m     estimator\u001b[39m.\u001b[39m_validate_params()\n\u001b[0;32m   1146\u001b[0m \u001b[39mwith\u001b[39;00m config_context(\n\u001b[0;32m   1147\u001b[0m     skip_parameter_validation\u001b[39m=\u001b[39m(\n\u001b[0;32m   1148\u001b[0m         prefer_skip_nested_validation \u001b[39mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1149\u001b[0m     )\n\u001b[0;32m   1150\u001b[0m ):\n\u001b[1;32m-> 1151\u001b[0m     \u001b[39mreturn\u001b[39;00m fit_method(estimator, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Gebruiker\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\pipeline.py:416\u001b[0m, in \u001b[0;36mPipeline.fit\u001b[1;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[0;32m    390\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Fit the model.\u001b[39;00m\n\u001b[0;32m    391\u001b[0m \n\u001b[0;32m    392\u001b[0m \u001b[39mFit all the transformers one after the other and transform the\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    413\u001b[0m \u001b[39m    Pipeline with fitted steps.\u001b[39;00m\n\u001b[0;32m    414\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    415\u001b[0m fit_params_steps \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_fit_params(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfit_params)\n\u001b[1;32m--> 416\u001b[0m Xt \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fit(X, y, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mfit_params_steps)\n\u001b[0;32m    417\u001b[0m \u001b[39mwith\u001b[39;00m _print_elapsed_time(\u001b[39m\"\u001b[39m\u001b[39mPipeline\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_log_message(\u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msteps) \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m)):\n\u001b[0;32m    418\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_final_estimator \u001b[39m!=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mpassthrough\u001b[39m\u001b[39m\"\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\Gebruiker\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\pipeline.py:370\u001b[0m, in \u001b[0;36mPipeline._fit\u001b[1;34m(self, X, y, **fit_params_steps)\u001b[0m\n\u001b[0;32m    368\u001b[0m     cloned_transformer \u001b[39m=\u001b[39m clone(transformer)\n\u001b[0;32m    369\u001b[0m \u001b[39m# Fit or load from cache the current transformer\u001b[39;00m\n\u001b[1;32m--> 370\u001b[0m X, fitted_transformer \u001b[39m=\u001b[39m fit_transform_one_cached(\n\u001b[0;32m    371\u001b[0m     cloned_transformer,\n\u001b[0;32m    372\u001b[0m     X,\n\u001b[0;32m    373\u001b[0m     y,\n\u001b[0;32m    374\u001b[0m     \u001b[39mNone\u001b[39;49;00m,\n\u001b[0;32m    375\u001b[0m     message_clsname\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mPipeline\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m    376\u001b[0m     message\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_log_message(step_idx),\n\u001b[0;32m    377\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mfit_params_steps[name],\n\u001b[0;32m    378\u001b[0m )\n\u001b[0;32m    379\u001b[0m \u001b[39m# Replace the transformer of the step with the fitted\u001b[39;00m\n\u001b[0;32m    380\u001b[0m \u001b[39m# transformer. This is necessary when loading the transformer\u001b[39;00m\n\u001b[0;32m    381\u001b[0m \u001b[39m# from the cache.\u001b[39;00m\n\u001b[0;32m    382\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msteps[step_idx] \u001b[39m=\u001b[39m (name, fitted_transformer)\n",
      "File \u001b[1;32mc:\\Users\\Gebruiker\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\joblib\\memory.py:353\u001b[0m, in \u001b[0;36mNotMemorizedFunc.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    352\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m--> 353\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfunc(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Gebruiker\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\pipeline.py:950\u001b[0m, in \u001b[0;36m_fit_transform_one\u001b[1;34m(transformer, X, y, weight, message_clsname, message, **fit_params)\u001b[0m\n\u001b[0;32m    948\u001b[0m \u001b[39mwith\u001b[39;00m _print_elapsed_time(message_clsname, message):\n\u001b[0;32m    949\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(transformer, \u001b[39m\"\u001b[39m\u001b[39mfit_transform\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m--> 950\u001b[0m         res \u001b[39m=\u001b[39m transformer\u001b[39m.\u001b[39;49mfit_transform(X, y, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mfit_params)\n\u001b[0;32m    951\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    952\u001b[0m         res \u001b[39m=\u001b[39m transformer\u001b[39m.\u001b[39mfit(X, y, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfit_params)\u001b[39m.\u001b[39mtransform(X)\n",
      "File \u001b[1;32mc:\\Users\\Gebruiker\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\_set_output.py:140\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[1;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[0;32m    138\u001b[0m \u001b[39m@wraps\u001b[39m(f)\n\u001b[0;32m    139\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrapped\u001b[39m(\u001b[39mself\u001b[39m, X, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m--> 140\u001b[0m     data_to_wrap \u001b[39m=\u001b[39m f(\u001b[39mself\u001b[39;49m, X, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m    141\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(data_to_wrap, \u001b[39mtuple\u001b[39m):\n\u001b[0;32m    142\u001b[0m         \u001b[39m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[0;32m    143\u001b[0m         return_tuple \u001b[39m=\u001b[39m (\n\u001b[0;32m    144\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[39m0\u001b[39m], X, \u001b[39mself\u001b[39m),\n\u001b[0;32m    145\u001b[0m             \u001b[39m*\u001b[39mdata_to_wrap[\u001b[39m1\u001b[39m:],\n\u001b[0;32m    146\u001b[0m         )\n",
      "File \u001b[1;32mc:\\Users\\Gebruiker\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\base.py:1151\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1144\u001b[0m     estimator\u001b[39m.\u001b[39m_validate_params()\n\u001b[0;32m   1146\u001b[0m \u001b[39mwith\u001b[39;00m config_context(\n\u001b[0;32m   1147\u001b[0m     skip_parameter_validation\u001b[39m=\u001b[39m(\n\u001b[0;32m   1148\u001b[0m         prefer_skip_nested_validation \u001b[39mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1149\u001b[0m     )\n\u001b[0;32m   1150\u001b[0m ):\n\u001b[1;32m-> 1151\u001b[0m     \u001b[39mreturn\u001b[39;00m fit_method(estimator, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Gebruiker\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\compose\\_column_transformer.py:743\u001b[0m, in \u001b[0;36mColumnTransformer.fit_transform\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    740\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_validate_column_callables(X)\n\u001b[0;32m    741\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_validate_remainder(X)\n\u001b[1;32m--> 743\u001b[0m result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fit_transform(X, y, _fit_transform_one)\n\u001b[0;32m    745\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m result:\n\u001b[0;32m    746\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_update_fitted_transformers([])\n",
      "File \u001b[1;32mc:\\Users\\Gebruiker\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\compose\\_column_transformer.py:670\u001b[0m, in \u001b[0;36mColumnTransformer._fit_transform\u001b[1;34m(self, X, y, func, fitted, column_as_strings)\u001b[0m\n\u001b[0;32m    664\u001b[0m transformers \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(\n\u001b[0;32m    665\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_iter(\n\u001b[0;32m    666\u001b[0m         fitted\u001b[39m=\u001b[39mfitted, replace_strings\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, column_as_strings\u001b[39m=\u001b[39mcolumn_as_strings\n\u001b[0;32m    667\u001b[0m     )\n\u001b[0;32m    668\u001b[0m )\n\u001b[0;32m    669\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 670\u001b[0m     \u001b[39mreturn\u001b[39;00m Parallel(n_jobs\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mn_jobs)(\n\u001b[0;32m    671\u001b[0m         delayed(func)(\n\u001b[0;32m    672\u001b[0m             transformer\u001b[39m=\u001b[39;49mclone(trans) \u001b[39mif\u001b[39;49;00m \u001b[39mnot\u001b[39;49;00m fitted \u001b[39melse\u001b[39;49;00m trans,\n\u001b[0;32m    673\u001b[0m             X\u001b[39m=\u001b[39;49m_safe_indexing(X, column, axis\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m),\n\u001b[0;32m    674\u001b[0m             y\u001b[39m=\u001b[39;49my,\n\u001b[0;32m    675\u001b[0m             weight\u001b[39m=\u001b[39;49mweight,\n\u001b[0;32m    676\u001b[0m             message_clsname\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mColumnTransformer\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m    677\u001b[0m             message\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_log_message(name, idx, \u001b[39mlen\u001b[39;49m(transformers)),\n\u001b[0;32m    678\u001b[0m         )\n\u001b[0;32m    679\u001b[0m         \u001b[39mfor\u001b[39;49;00m idx, (name, trans, column, weight) \u001b[39min\u001b[39;49;00m \u001b[39menumerate\u001b[39;49m(transformers, \u001b[39m1\u001b[39;49m)\n\u001b[0;32m    680\u001b[0m     )\n\u001b[0;32m    681\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mValueError\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    682\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mExpected 2D array, got 1D array instead\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m \u001b[39mstr\u001b[39m(e):\n",
      "File \u001b[1;32mc:\\Users\\Gebruiker\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\parallel.py:65\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m     60\u001b[0m config \u001b[39m=\u001b[39m get_config()\n\u001b[0;32m     61\u001b[0m iterable_with_config \u001b[39m=\u001b[39m (\n\u001b[0;32m     62\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[0;32m     63\u001b[0m     \u001b[39mfor\u001b[39;00m delayed_func, args, kwargs \u001b[39min\u001b[39;00m iterable\n\u001b[0;32m     64\u001b[0m )\n\u001b[1;32m---> 65\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__call__\u001b[39;49m(iterable_with_config)\n",
      "File \u001b[1;32mc:\\Users\\Gebruiker\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\joblib\\parallel.py:1863\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1861\u001b[0m     output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_sequential_output(iterable)\n\u001b[0;32m   1862\u001b[0m     \u001b[39mnext\u001b[39m(output)\n\u001b[1;32m-> 1863\u001b[0m     \u001b[39mreturn\u001b[39;00m output \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreturn_generator \u001b[39melse\u001b[39;00m \u001b[39mlist\u001b[39m(output)\n\u001b[0;32m   1865\u001b[0m \u001b[39m# Let's create an ID that uniquely identifies the current call. If the\u001b[39;00m\n\u001b[0;32m   1866\u001b[0m \u001b[39m# call is interrupted early and that the same instance is immediately\u001b[39;00m\n\u001b[0;32m   1867\u001b[0m \u001b[39m# re-used, this id will be used to prevent workers that were\u001b[39;00m\n\u001b[0;32m   1868\u001b[0m \u001b[39m# concurrently finalizing a task from the previous call to run the\u001b[39;00m\n\u001b[0;32m   1869\u001b[0m \u001b[39m# callback.\u001b[39;00m\n\u001b[0;32m   1870\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n",
      "File \u001b[1;32mc:\\Users\\Gebruiker\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\joblib\\parallel.py:1792\u001b[0m, in \u001b[0;36mParallel._get_sequential_output\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1790\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_dispatched_batches \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m   1791\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_dispatched_tasks \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m-> 1792\u001b[0m res \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1793\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_completed_tasks \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m   1794\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprint_progress()\n",
      "File \u001b[1;32mc:\\Users\\Gebruiker\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\parallel.py:127\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    125\u001b[0m     config \u001b[39m=\u001b[39m {}\n\u001b[0;32m    126\u001b[0m \u001b[39mwith\u001b[39;00m config_context(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mconfig):\n\u001b[1;32m--> 127\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfunction(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Gebruiker\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\pipeline.py:950\u001b[0m, in \u001b[0;36m_fit_transform_one\u001b[1;34m(transformer, X, y, weight, message_clsname, message, **fit_params)\u001b[0m\n\u001b[0;32m    948\u001b[0m \u001b[39mwith\u001b[39;00m _print_elapsed_time(message_clsname, message):\n\u001b[0;32m    949\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(transformer, \u001b[39m\"\u001b[39m\u001b[39mfit_transform\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m--> 950\u001b[0m         res \u001b[39m=\u001b[39m transformer\u001b[39m.\u001b[39;49mfit_transform(X, y, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mfit_params)\n\u001b[0;32m    951\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    952\u001b[0m         res \u001b[39m=\u001b[39m transformer\u001b[39m.\u001b[39mfit(X, y, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfit_params)\u001b[39m.\u001b[39mtransform(X)\n",
      "File \u001b[1;32mc:\\Users\\Gebruiker\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\_set_output.py:140\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[1;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[0;32m    138\u001b[0m \u001b[39m@wraps\u001b[39m(f)\n\u001b[0;32m    139\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrapped\u001b[39m(\u001b[39mself\u001b[39m, X, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m--> 140\u001b[0m     data_to_wrap \u001b[39m=\u001b[39m f(\u001b[39mself\u001b[39;49m, X, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m    141\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(data_to_wrap, \u001b[39mtuple\u001b[39m):\n\u001b[0;32m    142\u001b[0m         \u001b[39m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[0;32m    143\u001b[0m         return_tuple \u001b[39m=\u001b[39m (\n\u001b[0;32m    144\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[39m0\u001b[39m], X, \u001b[39mself\u001b[39m),\n\u001b[0;32m    145\u001b[0m             \u001b[39m*\u001b[39mdata_to_wrap[\u001b[39m1\u001b[39m:],\n\u001b[0;32m    146\u001b[0m         )\n",
      "File \u001b[1;32mc:\\Users\\Gebruiker\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\base.py:918\u001b[0m, in \u001b[0;36mTransformerMixin.fit_transform\u001b[1;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[0;32m    915\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfit(X, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfit_params)\u001b[39m.\u001b[39mtransform(X)\n\u001b[0;32m    916\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    917\u001b[0m     \u001b[39m# fit method of arity 2 (supervised transformation)\u001b[39;00m\n\u001b[1;32m--> 918\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfit(X, y, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mfit_params)\u001b[39m.\u001b[39;49mtransform(X)\n",
      "File \u001b[1;32mc:\\Users\\Gebruiker\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\_set_output.py:140\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[1;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[0;32m    138\u001b[0m \u001b[39m@wraps\u001b[39m(f)\n\u001b[0;32m    139\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrapped\u001b[39m(\u001b[39mself\u001b[39m, X, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m--> 140\u001b[0m     data_to_wrap \u001b[39m=\u001b[39m f(\u001b[39mself\u001b[39;49m, X, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m    141\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(data_to_wrap, \u001b[39mtuple\u001b[39m):\n\u001b[0;32m    142\u001b[0m         \u001b[39m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[0;32m    143\u001b[0m         return_tuple \u001b[39m=\u001b[39m (\n\u001b[0;32m    144\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[39m0\u001b[39m], X, \u001b[39mself\u001b[39m),\n\u001b[0;32m    145\u001b[0m             \u001b[39m*\u001b[39mdata_to_wrap[\u001b[39m1\u001b[39m:],\n\u001b[0;32m    146\u001b[0m         )\n",
      "\u001b[1;32mc:\\Users\\Gebruiker\\MLChallenge\\notebooks\\Experiments.ipynb Cell 16\u001b[0m line \u001b[0;36m2\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Gebruiker/MLChallenge/notebooks/Experiments.ipynb#X21sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtransform\u001b[39m(\u001b[39mself\u001b[39m, X):\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Gebruiker/MLChallenge/notebooks/Experiments.ipynb#X21sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmlb\u001b[39m.\u001b[39;49mtransform(X)\n",
      "File \u001b[1;32mc:\\Users\\Gebruiker\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\_set_output.py:140\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[1;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[0;32m    138\u001b[0m \u001b[39m@wraps\u001b[39m(f)\n\u001b[0;32m    139\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrapped\u001b[39m(\u001b[39mself\u001b[39m, X, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m--> 140\u001b[0m     data_to_wrap \u001b[39m=\u001b[39m f(\u001b[39mself\u001b[39;49m, X, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m    141\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(data_to_wrap, \u001b[39mtuple\u001b[39m):\n\u001b[0;32m    142\u001b[0m         \u001b[39m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[0;32m    143\u001b[0m         return_tuple \u001b[39m=\u001b[39m (\n\u001b[0;32m    144\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[39m0\u001b[39m], X, \u001b[39mself\u001b[39m),\n\u001b[0;32m    145\u001b[0m             \u001b[39m*\u001b[39mdata_to_wrap[\u001b[39m1\u001b[39m:],\n\u001b[0;32m    146\u001b[0m         )\n",
      "File \u001b[1;32mc:\\Users\\Gebruiker\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\preprocessing\\_label.py:853\u001b[0m, in \u001b[0;36mMultiLabelBinarizer.transform\u001b[1;34m(self, y)\u001b[0m\n\u001b[0;32m    837\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtransform\u001b[39m(\u001b[39mself\u001b[39m, y):\n\u001b[0;32m    838\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Transform the given label sets.\u001b[39;00m\n\u001b[0;32m    839\u001b[0m \n\u001b[0;32m    840\u001b[0m \u001b[39m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    851\u001b[0m \u001b[39m        `y[i]`, and 0 otherwise.\u001b[39;00m\n\u001b[0;32m    852\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 853\u001b[0m     check_is_fitted(\u001b[39mself\u001b[39;49m)\n\u001b[0;32m    855\u001b[0m     class_to_index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_build_cache()\n\u001b[0;32m    856\u001b[0m     yt \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_transform(y, class_to_index)\n",
      "File \u001b[1;32mc:\\Users\\Gebruiker\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\validation.py:1462\u001b[0m, in \u001b[0;36mcheck_is_fitted\u001b[1;34m(estimator, attributes, msg, all_or_any)\u001b[0m\n\u001b[0;32m   1459\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m is not an estimator instance.\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m (estimator))\n\u001b[0;32m   1461\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m _is_fitted(estimator, attributes, all_or_any):\n\u001b[1;32m-> 1462\u001b[0m     \u001b[39mraise\u001b[39;00m NotFittedError(msg \u001b[39m%\u001b[39m {\u001b[39m\"\u001b[39m\u001b[39mname\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mtype\u001b[39m(estimator)\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m})\n",
      "\u001b[1;31mNotFittedError\u001b[0m: This MultiLabelBinarizer instance is not fitted yet. Call 'fit' with appropriate arguments before using this estimator."
     ]
    }
   ],
   "source": [
    "# Set the logging level to INFO and set loading message\n",
    "logging.getLogger().setLevel(logging.INFO)\n",
    "    \n",
    "# Load train and test sets and change all NA values to empty values\n",
    "logging.info(\"Loading training/test data\")\n",
    "train = pd.DataFrame.from_records(json.load(open('../data/train.json'))).fillna(\"\")\n",
    "test = pd.DataFrame.from_records(json.load(open('../data/test.json'))).fillna(\"\")\n",
    "    \n",
    "# Split the train set into train (80%) and validation (20%) sets, 5-folds\n",
    "logging.info(\"Splitting validation\")\n",
    "num_folds = 5\n",
    "k_fold = KFold(n_splits=num_folds, shuffle=True, random_state=123)\n",
    "    \n",
    "# Store a featurizer to transform the 'title' column into a bag-of-words format\n",
    "featurizer_1 = ColumnTransformer(\n",
    "    transformers=[(\"title\", CountVectorizer(), \"title\")], remainder='drop')\n",
    "featurizer_2 = ColumnTransformer(\n",
    "    transformers=[(\"title\", TfidfVectorizer(), \"title\")], remainder='drop')\n",
    "featurizer_3 = ColumnTransformer(\n",
    "    transformers=[(\"abstract\", CountVectorizer(), \"abstract\")], remainder='drop')\n",
    "featurizer_4 = ColumnTransformer(\n",
    "    transformers=[(\"abstract\", TfidfVectorizer(), \"abstract\")], remainder='drop')\n",
    "featurizers = [featurizer_1, featurizer_2, featurizer_3, featurizer_4]\n",
    "\n",
    "for i, featurizer in enumerate(featurizers):\n",
    "    # Make a pipeline for the featurizer and a ridge model, that aims to minimize the sum of squares\n",
    "    ridge_cv = make_pipeline(featurizer, Ridge())\n",
    "    \n",
    "    # Drop target variable column and fit both models\n",
    "    logging.info(f\"Fitting model with featurizer {i+1}\")\n",
    "    ridge_cv.fit(train.drop('year', axis=1), train['year'].values)\n",
    "    \n",
    "    # Calculate and report both MAE's\n",
    "    logging.info(\"Evaluating on validation data\")\n",
    "    ridge_cv_scores = cross_val_score(ridge_cv, train.drop('year', axis=1), train['year'].values, cv=k_fold, scoring='neg_mean_absolute_error')\n",
    "    logging.info(f\"Ridge regress MAE with featurizer {i+1} ({num_folds}-fold cross-validated): {-ridge_cv_scores.mean()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>00</th>\n",
       "      <th>000</th>\n",
       "      <th>001</th>\n",
       "      <th>0099</th>\n",
       "      <th>01</th>\n",
       "      <th>02</th>\n",
       "      <th>03</th>\n",
       "      <th>04</th>\n",
       "      <th>07</th>\n",
       "      <th>08</th>\n",
       "      <th>...</th>\n",
       "      <th>雜訊環境下應用線性估測編碼於特徵時序列之強健性語音辨識</th>\n",
       "      <th>雜訊環境與說話內容因素分析之強健性語音辨認</th>\n",
       "      <th>電腦輔助句子重組試題編製</th>\n",
       "      <th>電話查詢口語對話系統中語音辨識不確定性之處理</th>\n",
       "      <th>電話轉接對話模式與表達轉接要求句型的分析</th>\n",
       "      <th>非負矩陣分解法於語音調變頻譜強化之研究</th>\n",
       "      <th>面向中文口语理解的基于依赖引导的字特征槽填充模型</th>\n",
       "      <th>面向对话文本的实体关系抽取</th>\n",
       "      <th>面向机器阅读理解的高质量藏语数据集构建</th>\n",
       "      <th>領域相關詞彙極性分析及文件情緒分類之研究</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 13978 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    00  000  001  0099   01   02   03   04   07   08  ...  \\\n",
       "0  0.0  0.0  0.0   0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   \n",
       "1  0.0  0.0  0.0   0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   \n",
       "2  0.0  0.0  0.0   0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   \n",
       "3  0.0  0.0  0.0   0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   \n",
       "4  0.0  0.0  0.0   0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   \n",
       "\n",
       "   雜訊環境下應用線性估測編碼於特徵時序列之強健性語音辨識  雜訊環境與說話內容因素分析之強健性語音辨認  電腦輔助句子重組試題編製  \\\n",
       "0                          0.0                    0.0           0.0   \n",
       "1                          0.0                    0.0           0.0   \n",
       "2                          0.0                    0.0           0.0   \n",
       "3                          0.0                    0.0           0.0   \n",
       "4                          0.0                    0.0           0.0   \n",
       "\n",
       "   電話查詢口語對話系統中語音辨識不確定性之處理  電話轉接對話模式與表達轉接要求句型的分析  非負矩陣分解法於語音調變頻譜強化之研究  \\\n",
       "0                     0.0                   0.0                  0.0   \n",
       "1                     0.0                   0.0                  0.0   \n",
       "2                     0.0                   0.0                  0.0   \n",
       "3                     0.0                   0.0                  0.0   \n",
       "4                     0.0                   0.0                  0.0   \n",
       "\n",
       "   面向中文口语理解的基于依赖引导的字特征槽填充模型  面向对话文本的实体关系抽取  面向机器阅读理解的高质量藏语数据集构建  \\\n",
       "0                       0.0            0.0                  0.0   \n",
       "1                       0.0            0.0                  0.0   \n",
       "2                       0.0            0.0                  0.0   \n",
       "3                       0.0            0.0                  0.0   \n",
       "4                       0.0            0.0                  0.0   \n",
       "\n",
       "   領域相關詞彙極性分析及文件情緒分類之研究  \n",
       "0                   0.0  \n",
       "1                   0.0  \n",
       "2                   0.0  \n",
       "3                   0.0  \n",
       "4                   0.0  \n",
       "\n",
       "[5 rows x 13978 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(val['title'])\n",
    "feature_names1 = tfidf_vectorizer.get_feature_names_out()\n",
    "tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=feature_names1)\n",
    "tfidf_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>00</th>\n",
       "      <th>000</th>\n",
       "      <th>001</th>\n",
       "      <th>0099</th>\n",
       "      <th>01</th>\n",
       "      <th>02</th>\n",
       "      <th>03</th>\n",
       "      <th>04</th>\n",
       "      <th>07</th>\n",
       "      <th>08</th>\n",
       "      <th>...</th>\n",
       "      <th>雜訊環境下應用線性估測編碼於特徵時序列之強健性語音辨識</th>\n",
       "      <th>雜訊環境與說話內容因素分析之強健性語音辨認</th>\n",
       "      <th>電腦輔助句子重組試題編製</th>\n",
       "      <th>電話查詢口語對話系統中語音辨識不確定性之處理</th>\n",
       "      <th>電話轉接對話模式與表達轉接要求句型的分析</th>\n",
       "      <th>非負矩陣分解法於語音調變頻譜強化之研究</th>\n",
       "      <th>面向中文口语理解的基于依赖引导的字特征槽填充模型</th>\n",
       "      <th>面向对话文本的实体关系抽取</th>\n",
       "      <th>面向机器阅读理解的高质量藏语数据集构建</th>\n",
       "      <th>領域相關詞彙極性分析及文件情緒分類之研究</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 13978 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   00  000  001  0099  01  02  03  04  07  08  ...  \\\n",
       "0   0    0    0     0   0   0   0   0   0   0  ...   \n",
       "1   0    0    0     0   0   0   0   0   0   0  ...   \n",
       "2   0    0    0     0   0   0   0   0   0   0  ...   \n",
       "3   0    0    0     0   0   0   0   0   0   0  ...   \n",
       "4   0    0    0     0   0   0   0   0   0   0  ...   \n",
       "\n",
       "   雜訊環境下應用線性估測編碼於特徵時序列之強健性語音辨識  雜訊環境與說話內容因素分析之強健性語音辨認  電腦輔助句子重組試題編製  \\\n",
       "0                            0                      0             0   \n",
       "1                            0                      0             0   \n",
       "2                            0                      0             0   \n",
       "3                            0                      0             0   \n",
       "4                            0                      0             0   \n",
       "\n",
       "   電話查詢口語對話系統中語音辨識不確定性之處理  電話轉接對話模式與表達轉接要求句型的分析  非負矩陣分解法於語音調變頻譜強化之研究  \\\n",
       "0                       0                     0                    0   \n",
       "1                       0                     0                    0   \n",
       "2                       0                     0                    0   \n",
       "3                       0                     0                    0   \n",
       "4                       0                     0                    0   \n",
       "\n",
       "   面向中文口语理解的基于依赖引导的字特征槽填充模型  面向对话文本的实体关系抽取  面向机器阅读理解的高质量藏语数据集构建  \\\n",
       "0                         0              0                    0   \n",
       "1                         0              0                    0   \n",
       "2                         0              0                    0   \n",
       "3                         0              0                    0   \n",
       "4                         0              0                    0   \n",
       "\n",
       "   領域相關詞彙極性分析及文件情緒分類之研究  \n",
       "0                     0  \n",
       "1                     0  \n",
       "2                     0  \n",
       "3                     0  \n",
       "4                     0  \n",
       "\n",
       "[5 rows x 13978 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_vectorizer = CountVectorizer()\n",
    "count_matrix = count_vectorizer.fit_transform(val['title'])\n",
    "feature_names2 = count_vectorizer.get_feature_names_out()\n",
    "count_df = pd.DataFrame(count_matrix.toarray(), columns=feature_names2)\n",
    "count_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This paragraph build upon the previous baseline code. It entails the following adjustments/additions chronologically:\n",
    "\n",
    "- [x] Removal of dummy regressor, since ridge works better from the very start;\n",
    "- [x] 5-fold cross validation to reduce variability (Ridge regress MAE (5.773));\n",
    "- [x] Try sklearn's other feature vectorizers (tf-idf (5.384), ...);\n",
    "- [ ] Perform custom preprocessing, tokenizations within sklearn;\n",
    "- [ ] Tune hyperparameters of feature vectorizers (n-gram size);\n",
    "- [ ] Try tasks other than regression, like lazy learning (kNN)(?);\n",
    "- [ ] Try BERTopic modelling;\n",
    "- [ ] "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
