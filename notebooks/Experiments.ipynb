{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Baseline code provided by uni"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import logging\n",
    "import json\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.dummy import DummyRegressor\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.metrics import mean_absolute_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Baseline function to create [predictions](https://github.com/larshanen/MLChallenge/tree/main/notebooks/predicted.json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    # Set the logging level to INFO and set loading message\n",
    "    logging.getLogger().setLevel(logging.INFO)\n",
    "    \n",
    "    # Load train and test sets and change all NA values to empty values\n",
    "    logging.info(\"Loading training/test data\")\n",
    "    train = pd.DataFrame.from_records(json.load(open('../data/train.json'))).fillna(\"\")\n",
    "    test = pd.DataFrame.from_records(json.load(open('../data/test.json'))).fillna(\"\")\n",
    "    \n",
    "    # Split the train set into train (75%) and validation (25%) sets\n",
    "    logging.info(\"Splitting validation\")\n",
    "    train, val = train_test_split(train, stratify=train['year'], random_state=123)\n",
    "    \n",
    "    # Store a featurizer to transform the 'title' column into a bag-of-words format\n",
    "    featurizer = ColumnTransformer(\n",
    "        transformers=[(\"title\", CountVectorizer(), \"title\")], remainder='drop')\n",
    "    \n",
    "    # Make a pipeline for the featurizer combined with a dummy regressor, that simply predicts the overall trained mean of the target variable\n",
    "    dummy = make_pipeline(featurizer, DummyRegressor(strategy='mean'))\n",
    "\n",
    "    # Make a pipeline for the featurizer and a ridge model, that aims to minimize the sum of squares\n",
    "    ridge = make_pipeline(featurizer, Ridge())\n",
    "    \n",
    "    # Drop target variable column and fit both models\n",
    "    logging.info(\"Fitting models\")\n",
    "    dummy.fit(train.drop('year', axis=1), train['year'].values)\n",
    "    ridge.fit(train.drop('year', axis=1), train['year'].values)\n",
    "    \n",
    "    # Calculate and report both MAE's\n",
    "    logging.info(\"Evaluating on validation data\")\n",
    "    err = mean_absolute_error(val['year'].values, dummy.predict(val.drop('year', axis=1)))\n",
    "    logging.info(f\"Mean baseline MAE: {err}\")\n",
    "    err = mean_absolute_error(val['year'].values, ridge.predict(val.drop('year', axis=1)))\n",
    "    logging.info(f\"Ridge regress MAE: {err}\")\n",
    "    \n",
    "    # Let the ridge model predict on test set\n",
    "    logging.info(f\"Predicting on test\")\n",
    "    pred = ridge.predict(test)\n",
    "    test['year'] = pred\n",
    "    \n",
    "    # Write JSON prediction file\n",
    "    logging.info(\"Writing prediction file\")\n",
    "    test.to_json(\"predicted.json\", orient='records', indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Loading training/test data\n",
      "INFO:root:Splitting validation\n",
      "INFO:root:Fitting models\n",
      "INFO:root:Evaluating on validation data\n",
      "INFO:root:Mean baseline MAE: 7.8054390754858805\n",
      "INFO:root:Ridge regress MAE: 5.812345349001838\n",
      "INFO:root:Predicting on test\n",
      "INFO:root:Writing prediction file\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Team code\n",
    "\n",
    "Please follow the instructions beneath when writing or adjusting code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Describe every piece of code with comments\n",
    "# Include your name in every header so we can report our individual contributions (this is mandatory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Explore baseline performance (Lars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Loading training/test data\n",
      "INFO:root:Splitting validation\n",
      "INFO:root:Fitting models\n",
      "INFO:root:Evaluating on validation data\n",
      "INFO:root:Mean baseline MAE: 7.8054390754858805\n",
      "INFO:root:Ridge regress MAE: 5.812345349001838\n"
     ]
    }
   ],
   "source": [
    "# Set the logging level to INFO and set loading message\n",
    "logging.getLogger().setLevel(logging.INFO)\n",
    "    \n",
    "# Load train and test sets and change all NA values to empty values\n",
    "logging.info(\"Loading training/test data\")\n",
    "train = pd.DataFrame.from_records(json.load(open('../data/train.json'))).fillna(\"\")\n",
    "test = pd.DataFrame.from_records(json.load(open('../data/test.json'))).fillna(\"\")\n",
    "    \n",
    "# Split the train set into train (75%) and validation (25%) sets\n",
    "logging.info(\"Splitting validation\")\n",
    "train, val = train_test_split(train, stratify=train['year'], random_state=123)\n",
    "    \n",
    "# Store a featurizer to transform the 'title' column into a bag-of-words format\n",
    "featurizer = ColumnTransformer(\n",
    "transformers=[(\"title\", CountVectorizer(), \"title\")], remainder='drop')\n",
    "    \n",
    "# Make a pipeline for the featurizer combined with a dummy regressor, that simply predicts the overall trained mean of the target variable\n",
    "dummy = make_pipeline(featurizer, DummyRegressor(strategy='mean'))\n",
    "\n",
    "# Make a pipeline for the featurizer and a ridge model, that aims to minimize the sum of squares\n",
    "ridge = make_pipeline(featurizer, Ridge())\n",
    "    \n",
    "# Drop target variable column and fit both models\n",
    "logging.info(\"Fitting models\")\n",
    "dummy.fit(train.drop('year', axis=1), train['year'].values)\n",
    "ridge.fit(train.drop('year', axis=1), train['year'].values)\n",
    "    \n",
    "# Calculate and report both MAE's\n",
    "logging.info(\"Evaluating on validation data\")\n",
    "err = mean_absolute_error(val['year'].values, dummy.predict(val.drop('year', axis=1)))\n",
    "logging.info(f\"Mean baseline MAE: {err}\")\n",
    "err = mean_absolute_error(val['year'].values, ridge.predict(val.drop('year', axis=1)))\n",
    "logging.info(f\"Ridge regress MAE: {err}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ENTRYTYPE</th>\n",
       "      <th>title</th>\n",
       "      <th>editor</th>\n",
       "      <th>year</th>\n",
       "      <th>publisher</th>\n",
       "      <th>author</th>\n",
       "      <th>abstract</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2603</th>\n",
       "      <td>inproceedings</td>\n",
       "      <td>Use of Heuristic Knowledge in Chinese Language...</td>\n",
       "      <td></td>\n",
       "      <td>1984</td>\n",
       "      <td>Association for Computational Linguistics</td>\n",
       "      <td>[Yang, Yiming, Nishida, Toyoaki, Doshita, Shuji]</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3258</th>\n",
       "      <td>inproceedings</td>\n",
       "      <td>Deciding the Twins Property for Weighted Tree ...</td>\n",
       "      <td></td>\n",
       "      <td>2012</td>\n",
       "      <td>Association for Computational Linguistics</td>\n",
       "      <td>[Büchse, Matthias, Fischer, Anja]</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48785</th>\n",
       "      <td>inproceedings</td>\n",
       "      <td>Large Margin Neural Language Model</td>\n",
       "      <td></td>\n",
       "      <td>2018</td>\n",
       "      <td>Association for Computational Linguistics</td>\n",
       "      <td>[Huang, Jiaji, Li, Yi, Ping, Wei, Huang, Liang]</td>\n",
       "      <td>We propose a large margin criterion for traini...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8822</th>\n",
       "      <td>inproceedings</td>\n",
       "      <td>Plot-guided Adversarial Example Construction f...</td>\n",
       "      <td></td>\n",
       "      <td>2021</td>\n",
       "      <td>Association for Computational Linguistics</td>\n",
       "      <td>[Ghazarian, Sarik, Liu, Zixi, S M, Akash, Weis...</td>\n",
       "      <td>With the recent advances of open-domain story ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24495</th>\n",
       "      <td>inproceedings</td>\n",
       "      <td>Towards a terminological resource for biomedic...</td>\n",
       "      <td></td>\n",
       "      <td>2006</td>\n",
       "      <td>European Language Resources Association (ELRA)</td>\n",
       "      <td>[Nenadic, Goran, Okazaki, Naoki, Ananiadou, So...</td>\n",
       "      <td>One of the main challenges in biomedical text ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12457</th>\n",
       "      <td>inproceedings</td>\n",
       "      <td>Annotating Events in an Emotion Corpus</td>\n",
       "      <td></td>\n",
       "      <td>2014</td>\n",
       "      <td>European Language Resources Association (ELRA)</td>\n",
       "      <td>[Lee, Sophia, Li, Shoushan, Huang, Chu-Ren]</td>\n",
       "      <td>This paper presents the development of a Chine...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29939</th>\n",
       "      <td>inproceedings</td>\n",
       "      <td>POSTECH Submission on Duolingo Shared Task</td>\n",
       "      <td></td>\n",
       "      <td>2020</td>\n",
       "      <td>Association for Computational Linguistics</td>\n",
       "      <td>[Park, Junsu, Kwon, Hongseok, Lee, Jong-Hyeok]</td>\n",
       "      <td>In this paper, we propose a transfer learning ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63102</th>\n",
       "      <td>inproceedings</td>\n",
       "      <td>Annotation Trees: LDC's customizable, extensib...</td>\n",
       "      <td></td>\n",
       "      <td>2012</td>\n",
       "      <td>European Language Resources Association (ELRA)</td>\n",
       "      <td>[Wright, Jonathan, Griffitt, Kira, Ellis, Joe,...</td>\n",
       "      <td>In recent months, LDC has developed a web-base...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19430</th>\n",
       "      <td>inproceedings</td>\n",
       "      <td>Long Nights, Rainy Days, and Misspent Youth: A...</td>\n",
       "      <td></td>\n",
       "      <td>2015</td>\n",
       "      <td>Association for Computational Linguistics</td>\n",
       "      <td>[Bracewell, David]</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32248</th>\n",
       "      <td>inproceedings</td>\n",
       "      <td>Detecting late-life depression in Alzheimer's ...</td>\n",
       "      <td></td>\n",
       "      <td>2016</td>\n",
       "      <td>Association for Computational Linguistics</td>\n",
       "      <td>[Fraser, Kathleen C., Rudzicz, Frank, Hirst, G...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>16479 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           ENTRYTYPE                                              title  \\\n",
       "2603   inproceedings  Use of Heuristic Knowledge in Chinese Language...   \n",
       "3258   inproceedings  Deciding the Twins Property for Weighted Tree ...   \n",
       "48785  inproceedings                 Large Margin Neural Language Model   \n",
       "8822   inproceedings  Plot-guided Adversarial Example Construction f...   \n",
       "24495  inproceedings  Towards a terminological resource for biomedic...   \n",
       "...              ...                                                ...   \n",
       "12457  inproceedings             Annotating Events in an Emotion Corpus   \n",
       "29939  inproceedings         POSTECH Submission on Duolingo Shared Task   \n",
       "63102  inproceedings  Annotation Trees: LDC's customizable, extensib...   \n",
       "19430  inproceedings  Long Nights, Rainy Days, and Misspent Youth: A...   \n",
       "32248  inproceedings  Detecting late-life depression in Alzheimer's ...   \n",
       "\n",
       "      editor  year                                       publisher  \\\n",
       "2603          1984       Association for Computational Linguistics   \n",
       "3258          2012       Association for Computational Linguistics   \n",
       "48785         2018       Association for Computational Linguistics   \n",
       "8822          2021       Association for Computational Linguistics   \n",
       "24495         2006  European Language Resources Association (ELRA)   \n",
       "...      ...   ...                                             ...   \n",
       "12457         2014  European Language Resources Association (ELRA)   \n",
       "29939         2020       Association for Computational Linguistics   \n",
       "63102         2012  European Language Resources Association (ELRA)   \n",
       "19430         2015       Association for Computational Linguistics   \n",
       "32248         2016       Association for Computational Linguistics   \n",
       "\n",
       "                                                  author  \\\n",
       "2603    [Yang, Yiming, Nishida, Toyoaki, Doshita, Shuji]   \n",
       "3258                   [Büchse, Matthias, Fischer, Anja]   \n",
       "48785    [Huang, Jiaji, Li, Yi, Ping, Wei, Huang, Liang]   \n",
       "8822   [Ghazarian, Sarik, Liu, Zixi, S M, Akash, Weis...   \n",
       "24495  [Nenadic, Goran, Okazaki, Naoki, Ananiadou, So...   \n",
       "...                                                  ...   \n",
       "12457        [Lee, Sophia, Li, Shoushan, Huang, Chu-Ren]   \n",
       "29939     [Park, Junsu, Kwon, Hongseok, Lee, Jong-Hyeok]   \n",
       "63102  [Wright, Jonathan, Griffitt, Kira, Ellis, Joe,...   \n",
       "19430                                 [Bracewell, David]   \n",
       "32248  [Fraser, Kathleen C., Rudzicz, Frank, Hirst, G...   \n",
       "\n",
       "                                                abstract  \n",
       "2603                                                      \n",
       "3258                                                      \n",
       "48785  We propose a large margin criterion for traini...  \n",
       "8822   With the recent advances of open-domain story ...  \n",
       "24495  One of the main challenges in biomedical text ...  \n",
       "...                                                  ...  \n",
       "12457  This paper presents the development of a Chine...  \n",
       "29939  In this paper, we propose a transfer learning ...  \n",
       "63102  In recent months, LDC has developed a web-base...  \n",
       "19430                                                     \n",
       "32248                                                     \n",
       "\n",
       "[16479 rows x 7 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check what validation set looks like\n",
    "val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16479\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([2005.16848241, 2016.72621127, 2013.17436633, ..., 2014.96570928,\n",
       "       2014.87896051, 2020.60932351])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check what array with predicted values looks like\n",
    "pred_array = ridge.predict(val.drop('year', axis=1))\n",
    "print(len(pred_array))\n",
    "pred_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16479\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array(['1984', '2012', '2018', ..., '2012', '2015', '2016'], dtype=object)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check what array with true values looks like\n",
    "true_array = val['year'].values\n",
    "print(len(true_array))\n",
    "true_array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Build upon baseline code (Lars)\n",
    "\n",
    "This paragraph build upon the previous baseline code. It entails the following adjustments/additions chronologically:\n",
    "\n",
    "- [x] Removal of dummy regressor, since ridge works better from the very start;\n",
    "- [x] 5-fold cross validation to reduce variability (Ridge regress MAE (5.773));\n",
    "- [x] Try sklearn's other feature vectorizers (tf-idf (5.384), ...);\n",
    "- [ ] Perform custom preprocessing, tokenizations within sklearn;\n",
    "- [ ] Smooth sparse matrices (?);\n",
    "- [ ] Tune hyperparameters of feature vectorizers (n-gram size);\n",
    "- [ ] Test for or include other columns (abstract, authors (?));\n",
    "- [ ] Try tasks other than regression, like lazy learning (kNN)(?);\n",
    "- [ ] Try BERTopic modelling;\n",
    "- [ ] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import extra modules\n",
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Loading training/test data\n",
      "INFO:root:Splitting validation\n",
      "INFO:root:Fitting model with featurizer 1\n",
      "INFO:root:Evaluating on validation data\n",
      "INFO:root:Ridge regress MAE with featurizer 1 (5-fold cross-validated): 5.773010450586702\n",
      "INFO:root:Fitting model with featurizer 2\n",
      "INFO:root:Evaluating on validation data\n",
      "INFO:root:Ridge regress MAE with featurizer 2 (5-fold cross-validated): 5.384430333156983\n"
     ]
    }
   ],
   "source": [
    "# Set the logging level to INFO and set loading message\n",
    "logging.getLogger().setLevel(logging.INFO)\n",
    "    \n",
    "# Load train and test sets and change all NA values to empty values\n",
    "logging.info(\"Loading training/test data\")\n",
    "train = pd.DataFrame.from_records(json.load(open('../data/train.json'))).fillna(\"\")\n",
    "test = pd.DataFrame.from_records(json.load(open('../data/test.json'))).fillna(\"\")\n",
    "    \n",
    "# Split the train set into train (80%) and validation (20%) sets, 5-folds\n",
    "logging.info(\"Splitting validation\")\n",
    "num_folds = 5\n",
    "k_fold = KFold(n_splits=num_folds, shuffle=True, random_state=123)\n",
    "    \n",
    "# Store a featurizer to transform the 'title' column into a bag-of-words format\n",
    "featurizer_1 = ColumnTransformer(\n",
    "    transformers=[(\"title\", CountVectorizer(), \"title\")], remainder='drop')\n",
    "featurizer_2 = ColumnTransformer(\n",
    "    transformers=[(\"title\", TfidfVectorizer(), \"title\")], remainder='drop')\n",
    "featurizers = [featurizer_1, featurizer_2]\n",
    "\n",
    "for i, featurizer in enumerate(featurizers):\n",
    "    # Make a pipeline for the featurizer and a ridge model, that aims to minimize the sum of squares\n",
    "    ridge_cv = make_pipeline(featurizer, Ridge())\n",
    "    \n",
    "    # Drop target variable column and fit both models\n",
    "    logging.info(f\"Fitting model with featurizer {i+1}\")\n",
    "    ridge_cv.fit(train.drop('year', axis=1), train['year'].values)\n",
    "    \n",
    "    # Calculate and report both MAE's\n",
    "    logging.info(\"Evaluating on validation data\")\n",
    "    ridge_cv_scores = cross_val_score(ridge_cv, train.drop('year', axis=1), train['year'].values, cv=k_fold, scoring='neg_mean_absolute_error')\n",
    "    logging.info(f\"Ridge regress MAE with featurizer {i+1} ({num_folds}-fold cross-validated): {-ridge_cv_scores.mean()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>00</th>\n",
       "      <th>000</th>\n",
       "      <th>001</th>\n",
       "      <th>0099</th>\n",
       "      <th>01</th>\n",
       "      <th>02</th>\n",
       "      <th>03</th>\n",
       "      <th>04</th>\n",
       "      <th>07</th>\n",
       "      <th>08</th>\n",
       "      <th>...</th>\n",
       "      <th>雜訊環境下應用線性估測編碼於特徵時序列之強健性語音辨識</th>\n",
       "      <th>雜訊環境與說話內容因素分析之強健性語音辨認</th>\n",
       "      <th>電腦輔助句子重組試題編製</th>\n",
       "      <th>電話查詢口語對話系統中語音辨識不確定性之處理</th>\n",
       "      <th>電話轉接對話模式與表達轉接要求句型的分析</th>\n",
       "      <th>非負矩陣分解法於語音調變頻譜強化之研究</th>\n",
       "      <th>面向中文口语理解的基于依赖引导的字特征槽填充模型</th>\n",
       "      <th>面向对话文本的实体关系抽取</th>\n",
       "      <th>面向机器阅读理解的高质量藏语数据集构建</th>\n",
       "      <th>領域相關詞彙極性分析及文件情緒分類之研究</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 13978 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    00  000  001  0099   01   02   03   04   07   08  ...  \\\n",
       "0  0.0  0.0  0.0   0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   \n",
       "1  0.0  0.0  0.0   0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   \n",
       "2  0.0  0.0  0.0   0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   \n",
       "3  0.0  0.0  0.0   0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   \n",
       "4  0.0  0.0  0.0   0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   \n",
       "\n",
       "   雜訊環境下應用線性估測編碼於特徵時序列之強健性語音辨識  雜訊環境與說話內容因素分析之強健性語音辨認  電腦輔助句子重組試題編製  \\\n",
       "0                          0.0                    0.0           0.0   \n",
       "1                          0.0                    0.0           0.0   \n",
       "2                          0.0                    0.0           0.0   \n",
       "3                          0.0                    0.0           0.0   \n",
       "4                          0.0                    0.0           0.0   \n",
       "\n",
       "   電話查詢口語對話系統中語音辨識不確定性之處理  電話轉接對話模式與表達轉接要求句型的分析  非負矩陣分解法於語音調變頻譜強化之研究  \\\n",
       "0                     0.0                   0.0                  0.0   \n",
       "1                     0.0                   0.0                  0.0   \n",
       "2                     0.0                   0.0                  0.0   \n",
       "3                     0.0                   0.0                  0.0   \n",
       "4                     0.0                   0.0                  0.0   \n",
       "\n",
       "   面向中文口语理解的基于依赖引导的字特征槽填充模型  面向对话文本的实体关系抽取  面向机器阅读理解的高质量藏语数据集构建  \\\n",
       "0                       0.0            0.0                  0.0   \n",
       "1                       0.0            0.0                  0.0   \n",
       "2                       0.0            0.0                  0.0   \n",
       "3                       0.0            0.0                  0.0   \n",
       "4                       0.0            0.0                  0.0   \n",
       "\n",
       "   領域相關詞彙極性分析及文件情緒分類之研究  \n",
       "0                   0.0  \n",
       "1                   0.0  \n",
       "2                   0.0  \n",
       "3                   0.0  \n",
       "4                   0.0  \n",
       "\n",
       "[5 rows x 13978 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(val['title'])\n",
    "feature_names1 = tfidf_vectorizer.get_feature_names_out()\n",
    "tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=feature_names)\n",
    "tfidf_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>00</th>\n",
       "      <th>000</th>\n",
       "      <th>001</th>\n",
       "      <th>0099</th>\n",
       "      <th>01</th>\n",
       "      <th>02</th>\n",
       "      <th>03</th>\n",
       "      <th>04</th>\n",
       "      <th>07</th>\n",
       "      <th>08</th>\n",
       "      <th>...</th>\n",
       "      <th>雜訊環境下應用線性估測編碼於特徵時序列之強健性語音辨識</th>\n",
       "      <th>雜訊環境與說話內容因素分析之強健性語音辨認</th>\n",
       "      <th>電腦輔助句子重組試題編製</th>\n",
       "      <th>電話查詢口語對話系統中語音辨識不確定性之處理</th>\n",
       "      <th>電話轉接對話模式與表達轉接要求句型的分析</th>\n",
       "      <th>非負矩陣分解法於語音調變頻譜強化之研究</th>\n",
       "      <th>面向中文口语理解的基于依赖引导的字特征槽填充模型</th>\n",
       "      <th>面向对话文本的实体关系抽取</th>\n",
       "      <th>面向机器阅读理解的高质量藏语数据集构建</th>\n",
       "      <th>領域相關詞彙極性分析及文件情緒分類之研究</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 13978 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   00  000  001  0099  01  02  03  04  07  08  ...  \\\n",
       "0   0    0    0     0   0   0   0   0   0   0  ...   \n",
       "1   0    0    0     0   0   0   0   0   0   0  ...   \n",
       "2   0    0    0     0   0   0   0   0   0   0  ...   \n",
       "3   0    0    0     0   0   0   0   0   0   0  ...   \n",
       "4   0    0    0     0   0   0   0   0   0   0  ...   \n",
       "\n",
       "   雜訊環境下應用線性估測編碼於特徵時序列之強健性語音辨識  雜訊環境與說話內容因素分析之強健性語音辨認  電腦輔助句子重組試題編製  \\\n",
       "0                            0                      0             0   \n",
       "1                            0                      0             0   \n",
       "2                            0                      0             0   \n",
       "3                            0                      0             0   \n",
       "4                            0                      0             0   \n",
       "\n",
       "   電話查詢口語對話系統中語音辨識不確定性之處理  電話轉接對話模式與表達轉接要求句型的分析  非負矩陣分解法於語音調變頻譜強化之研究  \\\n",
       "0                       0                     0                    0   \n",
       "1                       0                     0                    0   \n",
       "2                       0                     0                    0   \n",
       "3                       0                     0                    0   \n",
       "4                       0                     0                    0   \n",
       "\n",
       "   面向中文口语理解的基于依赖引导的字特征槽填充模型  面向对话文本的实体关系抽取  面向机器阅读理解的高质量藏语数据集构建  \\\n",
       "0                         0              0                    0   \n",
       "1                         0              0                    0   \n",
       "2                         0              0                    0   \n",
       "3                         0              0                    0   \n",
       "4                         0              0                    0   \n",
       "\n",
       "   領域相關詞彙極性分析及文件情緒分類之研究  \n",
       "0                     0  \n",
       "1                     0  \n",
       "2                     0  \n",
       "3                     0  \n",
       "4                     0  \n",
       "\n",
       "[5 rows x 13978 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_vectorizer = CountVectorizer()\n",
    "count_matrix = count_vectorizer.fit_transform(val['title'])\n",
    "feature_names2 = count_vectorizer.get_feature_names_out()\n",
    "count_df = pd.DataFrame(count_matrix.toarray(), columns=feature_names)\n",
    "count_df.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
