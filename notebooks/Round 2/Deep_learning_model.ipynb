{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Rick - Round 1\n",
    "\n",
    "Document code using comments, so we can all understand the code easily:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Describe every piece of code with comments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Import dependencies for this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from scipy.stats import pearsonr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Missing data heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ENTRYTYPE</th>\n",
       "      <th>title</th>\n",
       "      <th>editor</th>\n",
       "      <th>year</th>\n",
       "      <th>publisher</th>\n",
       "      <th>author</th>\n",
       "      <th>abstract</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>inproceedings</td>\n",
       "      <td>Philippine Language Resources: Trends and Dire...</td>\n",
       "      <td></td>\n",
       "      <td>2009</td>\n",
       "      <td>Association for Computational Linguistics</td>\n",
       "      <td>[Roxas, Rachel Edita, Cheng, Charibeth, Lim, N...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>inproceedings</td>\n",
       "      <td>A System for Translating Locative Prepositions...</td>\n",
       "      <td></td>\n",
       "      <td>1991</td>\n",
       "      <td>Association for Computational Linguistics</td>\n",
       "      <td>[Japkowicz, Nathalie, Wiebe, Janyce M.]</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>inproceedings</td>\n",
       "      <td>Introduction to the Shared Task on Comparing S...</td>\n",
       "      <td></td>\n",
       "      <td>2008</td>\n",
       "      <td>College Publications</td>\n",
       "      <td>[Bos, Johan]</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>inproceedings</td>\n",
       "      <td>Pynini: A Python library for weighted finite-s...</td>\n",
       "      <td></td>\n",
       "      <td>2016</td>\n",
       "      <td>Association for Computational Linguistics</td>\n",
       "      <td>[Gorman, Kyle]</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>inproceedings</td>\n",
       "      <td>Improving Readability of Swedish Electronic He...</td>\n",
       "      <td></td>\n",
       "      <td>2014</td>\n",
       "      <td>Association for Computational Linguistics</td>\n",
       "      <td>[Grigonyte, Gintarė, Kvist, Maria, Velupillai,...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       ENTRYTYPE                                              title editor  \\\n",
       "0  inproceedings  Philippine Language Resources: Trends and Dire...          \n",
       "1  inproceedings  A System for Translating Locative Prepositions...          \n",
       "2  inproceedings  Introduction to the Shared Task on Comparing S...          \n",
       "3  inproceedings  Pynini: A Python library for weighted finite-s...          \n",
       "4  inproceedings  Improving Readability of Swedish Electronic He...          \n",
       "\n",
       "   year                                  publisher  \\\n",
       "0  2009  Association for Computational Linguistics   \n",
       "1  1991  Association for Computational Linguistics   \n",
       "2  2008                       College Publications   \n",
       "3  2016  Association for Computational Linguistics   \n",
       "4  2014  Association for Computational Linguistics   \n",
       "\n",
       "                                              author abstract  \n",
       "0  [Roxas, Rachel Edita, Cheng, Charibeth, Lim, N...           \n",
       "1            [Japkowicz, Nathalie, Wiebe, Janyce M.]           \n",
       "2                                       [Bos, Johan]           \n",
       "3                                     [Gorman, Kyle]           \n",
       "4  [Grigonyte, Gintarė, Kvist, Maria, Velupillai,...           "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Store train set in pandas dataframe\n",
    "myLabels = pd.DataFrame.from_records(json.load(open('../../data/train.json'))).fillna(\"\")\n",
    "\n",
    "# Print first 5 rows\n",
    "myLabels.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Code from datacamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'get_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\rick-\\OneDrive\\Documenten\\GitHub\\MLChallenge\\notebooks\\Round 2\\Deep_learning_model.ipynb Cell 8\u001b[0m line \u001b[0;36m9\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/rick-/OneDrive/Documenten/GitHub/MLChallenge/notebooks/Round%202/Deep_learning_model.ipynb#X15sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m activation_results \u001b[39m=\u001b[39m {}\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/rick-/OneDrive/Documenten/GitHub/MLChallenge/notebooks/Round%202/Deep_learning_model.ipynb#X15sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39mfor\u001b[39;00m act \u001b[39min\u001b[39;00m activations:\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/rick-/OneDrive/Documenten/GitHub/MLChallenge/notebooks/Round%202/Deep_learning_model.ipynb#X15sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m   \u001b[39m# Get a new model with the current activation\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/rick-/OneDrive/Documenten/GitHub/MLChallenge/notebooks/Round%202/Deep_learning_model.ipynb#X15sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m   model \u001b[39m=\u001b[39m get_model(act)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/rick-/OneDrive/Documenten/GitHub/MLChallenge/notebooks/Round%202/Deep_learning_model.ipynb#X15sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m   \u001b[39m# Fit the model and store the history results\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/rick-/OneDrive/Documenten/GitHub/MLChallenge/notebooks/Round%202/Deep_learning_model.ipynb#X15sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m   h_callback \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mfit(X_train, y_train, epochs\u001b[39m=\u001b[39m\u001b[39m20\u001b[39m, validation_data\u001b[39m=\u001b[39m(X_test, y_test), verbose\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'get_model' is not defined"
     ]
    }
   ],
   "source": [
    "# Activation functions to try\n",
    "activations = ['relu','leaky_relu', 'sigmoid', 'tanh']\n",
    "\n",
    "# Loop over the activation functions\n",
    "activation_results = {}\n",
    "\n",
    "for act in activations:\n",
    "  # Get a new model with the current activation\n",
    "  model = get_model(act)\n",
    "  # Fit the model and store the history results\n",
    "  h_callback = model.fit(X_train, y_train, epochs=20, validation_data=(X_test, y_test), verbose=0)\n",
    "  activation_results[act] = h_callback\n",
    "\n",
    "\n",
    "# Import the EarlyStopping and ModelCheckpoint callbacks\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "# Early stop on validation accuracy\n",
    "monitor_val_acc = EarlyStopping(monitor = 'val_accuracy', patience = 3)\n",
    "\n",
    "# Save the best model as best_banknote_model.hdf5\n",
    "model_checkpoint = ModelCheckpoint('best_banknote_model.hdf5', save_best_only = True)\n",
    "\n",
    "# Fit your model for a stupid amount of epochs\n",
    "h_callback = model.fit(X_train, y_train,\n",
    "                    epochs = 1000000000000,\n",
    "                    callbacks = [monitor_val_acc, model_checkpoint],\n",
    "                    validation_data = (X_test, y_test))\n",
    "\n",
    "# Split text into an array of words \n",
    "words = text.split()\n",
    "\n",
    "# Make sentences of 4 words each, moving one word at a time\n",
    "sentences = []\n",
    "for i in range(4, len(words)):\n",
    "  sentences.append(' '.join(words[i-4:i]))\n",
    "\n",
    "# Instantiate a Tokenizer, then fit it on the sentences\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(sentences)\n",
    "\n",
    "# Turn sentences into a sequence of numbers\n",
    "sequences = tokenizer.texts_to_sequences(sentences)\n",
    "print(\"Sentences: \\n {} \\n Sequences: \\n {}\".format(sentences[:5],sequences[:5]))\n",
    "\n",
    "\n",
    "\n",
    "# Import the Embedding, LSTM and Dense layer\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "# Add an Embedding layer with the right parameters\n",
    "model.add(Embedding(input_dim = 44, input_length = 3, output_dim = 8, ))\n",
    "\n",
    "# Add a 32 unit LSTM layer\n",
    "model.add(LSTM(32))\n",
    "\n",
    "# Add a hidden Dense layer of 32 units and an output layer of vocab_size with softmax\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dense(32, activation='softmax'))\n",
    "model.summary()\n",
    "\n",
    "\n",
    "def predict_text(test_text, model = model):\n",
    "  if len(test_text.split()) != 3:\n",
    "    print('Text input should be 3 words!')\n",
    "    return False\n",
    "  \n",
    "  # Turn the test_text into a sequence of numbers\n",
    "  test_seq = tokenizer.texts_to_sequences([test_text])\n",
    "  test_seq = np.array(test_seq)\n",
    "  \n",
    "  # Use the model passed as a parameter to predict the next word\n",
    "  pred = model.predict(test_seq).argmax(axis = 1)[0]\n",
    "  \n",
    "  # Return the word that maps to the prediction\n",
    "  return tokenizer.index_word[pred]\n",
    "\n",
    "\n",
    "\n",
    "# Create an input layer with 3 columns\n",
    "input_tensor = Input((3,))\n",
    "\n",
    "# Pass it to a Dense layer with 1 unit\n",
    "output_tensor = Dense(1)(input_tensor)\n",
    "\n",
    "# Create a model\n",
    "model = Model(input_tensor, output_tensor)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='mean_absolute_error')\n",
    "\n",
    "model.fit(games_tourney_train[['home', 'seed_diff', 'pred']],\n",
    "          games_tourney_train['score_diff'],\n",
    "          epochs=1,\n",
    "          verbose=True)\n",
    "\n",
    "\n",
    "\n",
    "print(model.evaluate(games_tourney_test[['home', 'seed_diff', 'prediction']],\n",
    "               games_tourney_test['score_diff'], verbose=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'myLabels' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\rick-\\OneDrive\\Documenten\\GitHub\\MLChallenge\\notebooks\\Round 2\\Deep_learning_model.ipynb Cell 9\u001b[0m line \u001b[0;36m7\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/rick-/OneDrive/Documenten/GitHub/MLChallenge/notebooks/Round%202/Deep_learning_model.ipynb#X16sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mkeras\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mlayers\u001b[39;00m \u001b[39mimport\u001b[39;00m Embedding, LSTM, Dense, Input\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/rick-/OneDrive/Documenten/GitHub/MLChallenge/notebooks/Round%202/Deep_learning_model.ipynb#X16sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mkeras\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpreprocessing\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtext\u001b[39;00m \u001b[39mimport\u001b[39;00m Tokenizer\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/rick-/OneDrive/Documenten/GitHub/MLChallenge/notebooks/Round%202/Deep_learning_model.ipynb#X16sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m list_words \u001b[39m=\u001b[39m myLabels[\u001b[39m'\u001b[39m\u001b[39mabstract\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mapply(\u001b[39mlambda\u001b[39;00m x: x\u001b[39m.\u001b[39msplit())\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/rick-/OneDrive/Documenten/GitHub/MLChallenge/notebooks/Round%202/Deep_learning_model.ipynb#X16sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m sequences \u001b[39m=\u001b[39m []\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/rick-/OneDrive/Documenten/GitHub/MLChallenge/notebooks/Round%202/Deep_learning_model.ipynb#X16sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m \u001b[39mfor\u001b[39;00m word_list \u001b[39min\u001b[39;00m list_words:\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/rick-/OneDrive/Documenten/GitHub/MLChallenge/notebooks/Round%202/Deep_learning_model.ipynb#X16sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m     \u001b[39m# Join the individual words in the word_list into a single string\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'myLabels' is not defined"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Input\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "\n",
    "list_words = myLabels['abstract'].apply(lambda x: x.split())\n",
    "\n",
    "sequences = []\n",
    "for word_list in list_words:\n",
    "    # Join the individual words in the word_list into a single string\n",
    "    sequence = ' '.join(word_list)\n",
    "    sequences.append(sequence)\n",
    "\n",
    "# Tokenize\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(sequences)\n",
    "numerical_sequences = tokenizer.texts_to_sequences(sequences)\n",
    "vocab_size = len(tokenizer.word_index) + 1  # +1 for the padding token\n",
    "input_length = max(len(seq) for seq in sequences)\n",
    "\n",
    "\n",
    "def get_text_model(vocab_size, input_length, activation='relu'):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(input_dim=vocab_size, output_dim=100, input_length=input_length))    \n",
    "    model.add(LSTM(32))\n",
    "    model.add(Dense(32, activation=activation))\n",
    "    model.add(Dense(1, activation='sigmoid'))  \n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "\n",
    "activations = ['relu', 'tanh', 'sigmoid']\n",
    "\n",
    "activation_results = {}\n",
    "for act in activations:\n",
    "    model = get_text_model('vocab_size', 'input_length', activation=act)\n",
    "    # Assuming X_train, y_train, X_test, y_test are preprocessed text data\n",
    "    history = model.fit(X_train, y_train, epochs=20, validation_data=(X_test, y_test), verbose=0)\n",
    "    activation_results[act] = history.history\n",
    "\n",
    "\n",
    "monitor_val_loss = EarlyStopping(monitor='val_loss', patience=3)\n",
    "model_checkpoint = ModelCheckpoint('best_model.hdf5', save_best_only=True)\n",
    "\n",
    "model.fit(X_train, y_train, epochs=1000, callbacks=[monitor_val_loss, model_checkpoint], validation_data=(X_test, y_test))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=vocab_size, output_dim=100, input_length=3))\n",
    "model.add(LSTM(32))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dense(vocab_size, activation='softmax'))\n",
    "model.compile(optimizer='adam', loss='mean_absolute_error')\n",
    "model.summary()\n",
    "\n",
    "def predict_text(test_text, model=model):\n",
    "    if len(test_text.split()) != 3:\n",
    "        print('Text input should be 3 words!')\n",
    "        return False\n",
    "    \n",
    "    test_seq = tokenizer.texts_to_sequences([test_text])\n",
    "    test_seq = np.array(test_seq)\n",
    "    pred = model.predict(test_seq).argmax(axis=1)[0]\n",
    "    return tokenizer.index_word[pred]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "input_tensor = Input(shape=(num_features,))  \n",
    "x = Dense(64, activation='relu')(input_tensor)\n",
    "output_tensor = Dense(1, activation='linear')(x)  \n",
    "\n",
    "model = Model(inputs=input_tensor, outputs=output_tensor)\n",
    "model.compile(optimizer='adam', loss='mean_absolute_error')  \n",
    "model.fit(train_features, train_target, epochs=10, verbose=1)\n",
    "\n",
    "loss = model.evaluate(test_features, test_target, verbose=0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
